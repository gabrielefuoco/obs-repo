
## Schema Riassuntivo MapReduce

### 1. Principi Fondamentali di MapReduce
    *   **1.1. Mapper:**
        *   Applicati a coppie chiave-valore di input.
        *   Generano un numero arbitrario di coppie intermedie.
    *   **1.2. Reducer:**
        *   Applicati a tutti i valori intermedi associati alla stessa chiave intermedia.
    *   **1.3. Barriera Map-Reduce:**
        *   Ordinamento e raggruppamento distribuito su larga scala tra le fasi.

### 2. Esempio "Hello World": WordCount
    *   **2.1. Problema:** Contare le occorrenze di ogni parola in una collezione di documenti.
    *   **2.2. Input:** Repository di documenti (ogni documento è un elemento).
    *   **2.3. Map:**
        *   Legge un documento.
        *   Emette coppie chiave-valore: `(w1, 1), (w2, 1), ..., (wn, 1)` (parola, 1).
        *   Codice:
            ```
            Map(String key, String value) :
                // key: nome del documento
                // value: contenuto del documento
                for each word w in value:
                    EmitIntermediate(w, "1")
            ```
    *   **2.4. Shuffle e Sort:**
        *   Raggruppa per chiave.
        *   Genera coppie: `(w1, [1, 1, ..., 1]), ..., (wp, [1, 1, ..., 1])`.
    *   **2.5. Reduce:**
        *   Somma tutti i valori per ogni parola.
        *   Emette: `(wi, k), ..., (wj, l)`.
        *   Codice:
            ```
            Reduce(String key, Iterator values):
                // key: una parola
                // values: una lista di "1" (conteggi)
                int result = 0;
                for each v in values:
                    result += ParseInt(v);
                Emit(AsString(result));
            ```
    *   **2.6. Output:**
        *   Coppie `(w, m)` (parola, numero totale di occorrenze).

### 3. Esempio: WordLengthCount
    *   **3.1. Problema:** Contare quante parole di determinate lunghezze esistono in una collezione di documenti.
    *   **3.2. Input:** Repository di documenti (ogni documento è un elemento).
    *   **3.3. Map:**
        *   Legge un documento.
        *   Emette coppie chiave-valore: `(i, w1), ..., (j, wn)` (lunghezza parola, parola).
    *   **3.4. Shuffle e Sort:**
        *   Raggruppa per chiave.
        *   Genera coppie: `(1, [w1, ..., wk]), ..., (n, [wr, ..., ws])`.
    *   **3.5. Reduce:**
        *   Conta il numero di parole in ogni lista.
        *   Emette: `(1, l), ..., (p, m)`.
    *   **3.6. Output:**
        *   Coppie `(l, n)` (lunghezza, numero totale di parole di quella lunghezza).

### 4. Ottimizzazione: Combinazione
    *   **4.1. Motivazione:** I task di reduce non possono iniziare prima del completamento della fase di map.
    *   **4.2. Soluzione:** Eseguire una mini fase di reduce (combiner) sull'output map locale.
    *   **4.3. Requisiti:** La funzione reduce deve essere associativa e commutativa.
    *   **4.4. Funzione Combiner:** `combine (k2, [v2]) → [(k3, v3)]`.
    *   **4.5. Esempio:** Addizione nel Reduce di WordCount.

---

## Schema Riassuntivo MapReduce

**1. Ottimizzazione in MapReduce**

*   **1.1 Combiner:**
    *   Riduce la quantità di dati intermedi.
    *   Riduce il traffico di rete.
*   **1.2 Partizionamento:**
    *   Divide lo spazio delle chiavi intermedie in modo personalizzato.
    *   Assegna le coppie chiave-valore intermedie ai reducer.

**2. Esempio WordCount con Combiner**

*   **2.1 Problema:** Contare le occorrenze di ogni parola in una collezione di documenti.
*   **2.2 Input:** Repository di documenti (ogni documento è un elemento).
*   **2.3 Map:**
    *   Legge un documento.
    *   Emette coppie chiave-valore: `(W1, 1), (W2, 1), ..., (Wp, 1)` (parola, 1).
*   **2.4 Combiner:**
    *   Raggruppa per chiave.
    *   Somma i valori per ogni chiave.
    *   Emette: `(W1, i), ..., (Wn, j)` (parola, somma parziale).
*   **2.5 Shuffle e Sort:**
    *   Raggruppa per chiave.
    *   Genera coppie: `(w4, [p, ..., q]), ..., (wn, [r, ..., s])` (parola, lista di somme parziali).
*   **2.6 Reduce:**
    *   Somma tutti i valori per ogni chiave.
    *   Emette: `(w, k), ..., (wj, l)` (parola, somma totale).
*   **2.7 Output:**
    *   Coppie `(w, m)` (parola, numero totale di occorrenze).
    *   `w` appare almeno una volta.
    *   `m` è il numero totale di occorrenze di `w`.

**3. Workflow MapReduce**

*   **3.1 Necessità:**
    *   Un singolo job MapReduce ha una gamma limitata di problemi risolvibili.
    *   Compiti complessi richiedono la concatenazione di più job.
*   **3.2 Esempio: Trovare gli URL più popolari**
    *   **Fase 1:** Determinazione del numero di visualizzazioni per ogni URL (conta le occorrenze).
    *   **Fase 2:** Ordinamento degli URL per popolarità (ordina in base al numero di visualizzazioni).
        *   Mapper invertono chiavi e valori (frequenza come chiave, URL come valore).
*   **3.3 Concatenazione e Performance:**
    *   L'output di un job diventa l'input del successivo.
    *   Genera file intermedi su sistemi di file distribuiti.
    *   Lettura e scrittura su disco causano un calo delle prestazioni.

**4. Esempio: k-means in MapReduce**

*   **4.1 Clustering:** Raggruppamento di "punti" in "cluster" in base a una misura di distanza.
*   **4.2 Esempi di Analisi Cluster:**
    *   Segmentazione dei clienti.
    *   Clustering del mercato azionario.
    *   Riduzione della dimensionalità.
*   **4.3 Distanza tra punti:**
    *   **4.3.1 Distanza Euclidea:**
        *   Formula: $$d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \dots + (p_n - q_n)^2}$$
        *   `n` è il numero di variabili indipendenti.
    *   **4.3.2 Distanza di Manhattan:**
        *   Somma dei valori assoluti invece dei quadrati.

---

Ecco uno schema riassuntivo del testo fornito, organizzato gerarchicamente:

**1. Distanza e Centroide**

    *   **1.1 Distanza di Manhattan:**
        *   Definizione:  $$d_{Manhattan}(p,q) = \sum_{i=1}^{n} |p_i - q_i|$$ (Basata sulla geografia stradale a griglia di Manhattan)

    *   **1.2 Distanza del Centroide:**
        *   Definizione: Distanza tra i centroidi dei cluster.
        *   Calcolo del Centroide: Posizione media di tutti i punti dati in ogni coordinata.
        *   Esempio: Punti (-1, 10, 3), (0, 5, 2), (1, 20, 10) -> Centroide (0, 35/3, 5).
        *   Nota: Il centroide raramente è un punto dati originale.

**2. Clustering K-means**

    *   **2.1 Panoramica:**
        *   Algoritmo di clustering di assegnazione di punti.
        *   Presuppone uno spazio euclideo.

    *   **2.2 Algoritmo di Lloyd (Esempio di K-means):**
        *   Obiettivo: Minimizzare la somma dei quadrati all'interno del cluster.
        *   Passi:
            *   Specificare il numero di cluster *k*.
            *   Scegliere *k* punti dati iniziali come centroidi.
            *   Ripetere:
                *   Per ogni punto dati *p*:
                    *   Trovare il centroide più vicino a *p*.
                    *   Assegnare *p* al cluster di quel centroide.
                *   Ricalcolare i centroidi dei cluster.
            *   Fermarsi quando non ci sono miglioramenti.

**3. MapReduce per un'Iterazione di K-means**

    *   **3.1 Fasi Principali:**
        *   Classifica (Assign): Assegna ogni punto al centroide più vicino.
            *   Formula: $$z_i \leftarrow \arg\min_j \left\|\mu_j - x_i\right\|_2^2$$
        *   Ricentra (Update): Aggiorna i centroidi come media dei punti assegnati.
            *   Formula: $$\mu_j = \frac{1}{n_j} \sum_{i:z_i=j} x_i$$
            *   Dove: μj = centroide per il cluster j; nj = numero di elementi nel cluster j.

    *   **3.2 Implementazione MapReduce Dettagliata:**
        *   **Classifica (Map):**
            *   Input: (`{μj}`, `xi`) (centroidi e punto dati)
            *   Output: (`zi`, `xi`) (ID del cluster e punto dati)
            *   Parallelizzazione: Sui punti dati.
        *   **Ricentra (Reduce):**
            *   Input: Punti nel cluster j (`z = j`).
            *   Output: Nuovo centroide per il cluster j.
            *   Parallelizzazione: Sui centroidi del cluster.

**4. Codice Pseudo-codice MapReduce**

    *   **4.1 Fase di Classificazione (Map):**
        *   Formula: $$z_i \leftarrow \arg\min_j \left\|\mu_j - x_i\right\|_2^2$$
        *   Pseudo-codice:
            ```
            map([μ1, μ2, …, μk], xi)
            zi ← arg min || uj – xi ||2
            emit (zi, xi)
            ```
            *   `zi` è l'ID del cluster (chiave); `xi` è il punto dati (valore).

    *   **4.2 Fase di Ricentramento (Reduce):**
        *   Formula: $$\mu_j = \frac{1}{n_j} \sum_{i:z_i=j} x_i$$
        *   Pseudo-codice:
            ```
            reduce(j, x_in_clusterj: [xi, ...])
            sum = 0
            count = 0
            for x in x_in_clusterj:
                sum += x
                count += 1
            emit (j, sum/count)
            ```
            *   Reduce sui punti dati assegnati al cluster j.
            *   Emette il nuovo centroide per il cluster j.

**5. Iterazioni Multiple e Ottimizzazioni**

    *   **5.1 Necessità di Iterazioni:** K-means richiede iterazioni multiple di MapReduce.
    *   **5.2 Problema di Scalabilità:** Ogni mapper che riceve un punto dati e tutti i centroidi genera troppi mapper.
    *   **5.3 Ottimizzazione:** Ogni mapper dovrebbe ricevere molti punti dati per migliorare l'efficienza.

---

Ecco lo schema riassuntivo del testo fornito:

**I. Iterazione e Trasmissione dei Centroidi**

   *   Ad ogni iterazione, nuovi centroidi vengono calcolati.
   *   Questi nuovi centroidi devono essere trasmessi all'intero cluster MapReduce.

**II. Ripetizione delle Fasi Map e Reduce**

   *   Le fasi di Map e Reduce vengono ripetute.

**III. Criteri di Terminazione**

   *   La ripetizione continua fino alla convergenza.
   *   Oppure, fino al raggiungimento del numero massimo di passaggi predefinito.

---
