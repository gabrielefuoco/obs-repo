
## Schema Riassuntivo: Linguaggio Naturale, IR e NLP

**I. Linguaggio Naturale, IR e NLP**

* **A. Linguaggio Naturale:** Complesso e multimodale, al centro di IR e NLP.
* **B. Recupero delle Informazioni (IR):**
    * 1. Trova dati testuali (non strutturati) rilevanti a una query in grandi collezioni.
    * 2. Utilizza modelli di recupero per valutare la corrispondenza query-documento.
* **C. Elaborazione del Linguaggio Naturale (NLP):**
    * 1. Sviluppa sistemi automatici per comprendere e generare linguaggio naturale.
    * 2. Affronta sfide come ambiguità (morfologica, sintattica, semantica, pragmatica), multilinguismo e grandi dataset.

**II. IR vs. Altre Discipline**

* **A. Obiettivo Comune:** Facilitare la ricerca di informazioni (es. sul Web).
* **B. Attività Comuni:** SEO, crawling, estrazione di documenti.
* **C. Differenze:**
    * 1. **Web scraping:** Estrazione di informazioni.
    * 2. **Ricerca di schemi:** NLP, Linguistica computazionale.
    * 3. **Scoperta di informazioni sconosciute:** NLP, Estrazione di testo.
    * 4. **Discriminazione del testo:** NLP (Machine Learning).
    * 5. **Comprensione del testo:** NLP (Machine Learning, AI generativa).

**III. Topic Detection and Tracking (TDT)**

* **A. Definizione:** Acquisizione e analisi incrementale di dati (es. news feed) per identificare topic emergenti e obsoleti.
* **B. Caratteristiche:**
    * 1. Analisi incrementale dei dati.
    * 2. Supervisionato o non supervisionato.
* **C. Difficoltà:**
    * 1. Task incrementale (riconoscimento di nuovi e obsoleti topic).
    * 2. Risorse computazionali limitate.
    * 3. Definizione di "topic" (insieme di termini).
    * 4. Tracciamento del segnale (dati in ingresso).

**IV. Modellazione di Topic Stocastica**

* **A. Descrizione:** Tecnica che usa modelli probabilistici per analizzare grandi quantità di testo e identificare i topic principali.
* **B. Principio:** Ogni documento è una combinazione di topic; ogni parola ha una probabilità di appartenere a un topic.
* **C. Applicazioni:** Scoperta di temi principali, analisi della loro distribuzione, classificazione di documenti.
* **D. Esempio:** Latent Dirichlet Allocation (LDA).

**V. Relation Extraction**

* **A. Definizione (NLP):** Identificazione di relazioni lessicali tra entità nominate.
* **B. Relazione con NER:** Richiede prima l'identificazione delle entità nominate.
* **C. Metodo:** Ricerca di pattern frequenti.


---

**I. Summarization**

* **A. Approccio Tradizionale (anni '90):** Estrazione di parole chiave.
* **B. Guida al Processo:** Utilizzo delle proprietà del documento per guidare la summarization.

**II. KDD Pipeline in NLP**

* **A. Fasi Sequenziali:**
    * 1. Indicizzazione degli elementi costitutivi.
    * 2. Rappresentazione dei contenuti informativi.
    * 3. Apprendimento a valle del *result set*.
    * 4. *Feature selection* (possibile utilizzo del machine learning).

**III. Valutazione dei Risultati**

* **A. Criteri:** Basati su statistiche (es. *accuracy*), con limitazioni.
* **B. In assenza di *gold standard*:** Utilizzo di un *silver standard* generato automaticamente (es. GPT-4).

**IV. Funzioni Principali di un Sistema NLP**

* **A. Compito Principale:** Estrazione di informazioni da testi.
* **B. Macro Categorie:**
    * 1. **Indicativa:** Rivelazione di elementi per determinare la rilevanza rispetto alle query (fondamentale per esplorazione e retrieval).
    * 2. **Informativa:** Creazione di un surrogato del testo (o porzioni) senza riferimento al testo originale (valido anche per il retrieval).

**V. Browsing**

* **A. Descrizione:** Esplorazione di collezioni di testo senza query predefinite, utile per utenti con bisogni poco chiari.
* **B. Caratteristiche:**
    * 1. Indicazione di documenti rilevanti da parte dell'utente.
    * 2. Supporto all'annotazione.

**VI. Estrazione di Informazioni**

* **A. Descrizione:** Recupero di informazioni specifiche da documenti già identificati come rilevanti.
* **B. Esempi:** *Web wrapping*, estrazione da siti di *e-commerce*.
* **C. Processo:** Identificazione di informazioni specifiche all'interno di un documento rilevante.
* **D. Tipi di Template:**
    * 1. *Slot*: Spazi da riempire con sottostringhe.
    * 2. *Riempitivi pre-specificati*: Valori fissi.
    * 3. *Multipli riempitivi*: Slot con più valori.
    * 4. *Ordine fisso*: Sequenza fissa degli slot.
* **E. Modelli di Estrazione:**
    * 1. Specificazione di elementi (es. espressioni regolari).
    * 2. Modelli precedenti e successivi (contesto pre e post-slot).
    * 3. Estrazione di termini da un modello (definizione a priori di template e pattern per ogni slot).

**VII. Recupero di Documenti**

* **A. Descrizione:** Selezione di documenti da una collezione in risposta a una query (solitamente in linguaggio naturale).
* **B. Processo:**
    * 1. Classificazione dei documenti per rilevanza.
    * 2. Abbinamento tra rappresentazione del documento e della query.
    * 3. Restituzione di un elenco di documenti pertinenti.
* **C. Modelli:** Booleano, spazio vettoriale, probabilistico.

---

**Applicazioni di Base del Text Mining**

* **Scoperta della Conoscenza:**
    * Estrazione di informazioni.
    * Distillazione di informazioni (estrazione basata su struttura predefinita).
* **Filtraggio dell'Informazione:**
    * Rimozione di informazioni irrilevanti.
    * Applicazioni in web personalization e sistemi di raccomandazione (anche *collaborative based*).
    * Utilizzi tipici:
        * Categorizzazione gerarchica.
        * Riassunto del testo (Summarization).
        * Disambiguazione del senso delle parole (WSD).
        * Filtraggio del testo/informazione.
        * CRM e marketing (cross-selling e raccomandazioni di prodotti).
        * Filtraggio di notizie e spam.


**Tecniche Specifiche**

* **Categorizzazione Gerarchica:**
    * Organizzazione di documenti in struttura gerarchica tramite tassonomie.
    * Navigazione strutturata per affinare la ricerca.
    * Flessibilità nell'aggiunta/rimozione di categorie.
    * Caratteristiche:
        * Analisi degli hyperlink (natura ipertestuale).
        * Struttura gerarchica delle categorie.
        * Decomposizione della classificazione come decisione ramificata (a un nodo interno).
* **Summarization:**
    * Generazione di riassunti di testi.
    * Utilizzo di profili per strutturare informazioni importanti.
    * Tecniche dipendenti dalla natura dei documenti (es. analisi di aspetti specifici per recensioni).
    * Facilita l'accesso alle informazioni:
        * Estrazione di parole chiave da un insieme di documenti.
        * Astrazione di documenti per evitare lettura completa.
        * Riassunto di documenti recuperati da una ricerca.
    * Tipi di riassunti:
        * Riassunti di parole chiave.
        * Riassunti di frasi.
* **Disambiguazione del Senso delle Parole (WSD):**
    * Assegnazione del significato corretto di una parola in base al contesto.
    * Analisi simultanea di tutti i termini nel contesto.
    * Approccio efficace: utilizzo di inventari di sensi esistenti e misure di correlazione semantica.
    * Rinnovato interesse grazie a risorse linguistiche elettroniche (es. Wikipedia).
    * Esempio: disambiguazione del significato di "bank".
* **Filtraggio nel Text Mining:**
    * Classifica documenti come rilevanti o irrilevanti per un utente, bloccando quelli irrilevanti.


---

**I. Filtraggio delle Informazioni**

* **A. Feed di Notizie (Produttore/Consumatore):**
    * 1. Caso di Text Classification (rilevante/irrilevante).
    * 2. Implementazione lato produttore (instradamento) o consumatore (blocco).
    * 3. Richiede profilo utente (produttore) o profilo generale (consumatore).
* **B. Filtraggio Adattivo:**
    * 1. Profilo iniziale definito dall'utente.
    * 2. Aggiornamento profilo basato sul feedback utente.
* **C. Filtraggio - CRM:**
    * 1. Analisi feedback clienti tramite standardizzazione e raggruppamento dati.
* **D. Filtraggio - Raccomandazione Prodotti:**
    * 1. Approcci basati sul contenuto (preferenze utente).
    * 2. Approcci collaborativi (analisi utenti simili).
* **E. Rilevamento Spam:**
    * 1. Sfide legate a costi asimmetrici degli errori (falsi positivi/negativi).
    * 2. Distribuzione non uniforme delle classi (più spam che email legittime).


**II. Modelli di Rappresentazione dei Testi**

* **A. Definizione di Termine:**
    * 1. **Problema:** Selezione termini da includere, bilanciando completezza e importanza, gestendo frequenze.
    * 2. **Contesto:** Composizione testuale e obiettivo dell'indice.
    * 3. **Soluzione:** Modellazione relazioni tra parole, creando pattern relazionali per definire l'importanza.
    * 4. Definizione flessibile: parola singola, coppia di parole, frase, radice, n-gramma, tipo di parola.
    * 5. Modellazione relazioni: relazioni semantiche (*is-a*, *part-of*), evitando relazioni complesse basate su struttura sintattica.
* **B. Argomenti Correlati:**
    * 1. Analisi lessicale e morfologica (punteggiatura, minuscolo, stopwords, stemming, lemmatizzazione, tagging).
    * 2. Analisi semantica del discorso (anafora, ellissi, meronomia).
    * 3. Pragmatica, semiotica e morfologia.


**III. Tokenizzazione: Problemi Lessicali e Morfologici**

* **A. Gestione della Punteggiatura:**
    * 1. Sequenze con trattino (*state-of-the-art*, ecc.):  problema di separazione vs. mantenimento della semantica.
    * 2. Apostrofi (*Italy’s capital*): diverse rappresentazioni possibili (*Italy AND s*, *Italys*, *Italy’s*).
    * 3. Acronimi (*U.S.A*, *USA*): trattamento variabile a seconda del contesto.


---

**Preprocessing del Testo per l'Information Retrieval**

* **Gestione delle Entità Nominate:**
    * Trattamento specifico per entità come *San Francisco* e *Hewlett-Packard* per evitare frammentazione non semantica.

* **Gestione dei Numeri:**
    * Rimozione di stringhe puramente numeriche.
    * Mantenimento di stringhe alfanumeriche e numeri con significato informativo:
        * Preservazione di informazioni numeriche.
        * Identificazione di codici di errore, intervalli temporali, ecc.
    * Varietà di tipi numerici: date, tempo, codici, identificatori, numeri di telefono (esempi forniti nel testo).

* **Gestione dei Metadati:**
    * Indicizzazione separata dei metadati (data di creazione, formato, ecc.).
    * Complessità dell'interazione numeri-testo rende difficile una soluzione universale.

* **Gestione delle Stopwords:**
    * Rimozione di parole grammaticali (articoli, preposizioni) a basso potere informativo.
    * Motivazioni:
        * Scarsa rilevanza semantica.
        * Alta frequenza.
        * Impatto sulla dimensionalità e sparsità della rappresentazione, con conseguente perdita di potere discriminante ($\to$ *perdita di potere discriminante*).
    * Casi in cui potrebbero essere necessarie: query di frase, titoli, query relazionali (esempi forniti nel testo).
    * Gestione ottimale: tecniche di compressione e ottimizzazione delle query.
    * Personalizzazione della Stop-List:
        * Rimozione di termini molto frequenti nel corpus (poco esplicativi).
        * Eventuale esclusione di termini super rari (operazione delicata).
        * Decisione sull'esclusione di termini che appaiono una sola volta dipende dal tipo di termine.

* **Normalizzazione:**
    * Uniformazione della forma delle parole per trovare corrispondenze tra termini con forme diverse ma stesso significato.
    * Operazioni di normalizzazione:
        * Eliminazione di punti, trattini, accenti, umlaut (esempi forniti nel testo).
    * Risultato: creazione di *termini* (voci nel dizionario IR) spesso organizzati in *classi di equivalenza*.
    * Semplificazione della rappresentazione del testo, focalizzandosi sulla struttura sintattica.
    * Espansione asimmetrica come alternativa alle classi di equivalenza.

* **Case delle lettere:**
    * Conversione del testo in minuscolo (lowercase).

---

**Preprocessing del Testo per la Ricerca di Informazioni**

I. **Normalizzazione del Testo:**

    A. **Gestione delle Varianti:** L'inserimento di un termine può generare diverse varianti (es: "window", "windows", "Windows").  Questo approccio è potente ma meno efficiente.
    B. **Influenza della Lingua:** La scelta del metodo dipende dalla lingua.  Spesso si preferisce rimuovere accenti e trattini per gestire le omissioni degli utenti.
    C. **Case Folding:**  Tipicamente si converte tutto in minuscolo, eccetto le named entities (es. General Motors) e gli acronimi.

II. **Riduzione della Matrice dei Dati:**

    A. **Algebra Lineare:**  Applicazione dell'algebra lineare per ridurre la matrice identificando e raggruppando colonne linearmente dipendenti (sinonimi), focalizzandosi sui pattern sintattici.

III. **Spell Checking:**

    A. **Prossimità tra Stringhe:**  Gli algoritmi devono gestire la prossimità tra stringhe per correggere errori di battitura.
    B. **Edit Distance:** Misura la differenza tra stringhe, ma ignora il contesto.
    C. **Contesto Semantico:**  Una correzione accurata richiede il contesto, ma spesso si lavora solo sul campo lessicale.
    D. **Evitare se non Necessario:**  Il spell checking è sconsigliato se non essenziale, soprattutto con corpus rumorosi (es. messaggi istantanei).

IV. **Gestione delle Emoticon:**

    A. **Sostituzione o Markup:**  Le emoticon possono essere sostituite con termini o mantenute con un markup, con la pesatura delegata a fasi successive.  Importanti per l'analisi del sentiment.

V. **Utilizzo dei Tesauri:**

    A. **Gestione Sinonimi e Omonimi:**  I tesauri generalizzano termini correlati, gestendo sinonimi (significato simile, forma diversa) e omonimi (forma uguale, significato diverso).
    B. **Esempio:**  `color = colour`
    C. **Implementazione:**
        1. **Indicizzazione Multipla:**  Indicizzazione di un termine anche con le sue varianti (es. "color-colour").
        2. **Espansione della Query:**  Espansione della query per includere termini correlati.

VI. **Gestione degli Errori di Ortografia:**

    A. **Algoritmi come Soundex:**  Creano classi di equivalenza basate su euristiche fonetiche, raggruppando parole che suonano simili.

---

## Lemmatizzazione e Stemming: Confronto

**I. Lemmatizzazione**

* **Definizione:** Riduzione delle forme flessionali di una parola al suo lemma (forma base).
    * Applicabile a verbi (infinito) e sostantivi (singolare).
    * Richiede analisi morfologica completa.
* **Esempio:** "the boy's cars are different colors" → "the boy car be different color"
* **Benefici per il retrieval:** Generalmente modesti.
* **Ruolo:** Effetto collaterale dell'analisi sintattica.


**II. Stemming**

* **Definizione:** Riduzione delle parole alle loro radici (prefisso) tramite "suffix stripping".
    * Intuitivo per sostantivi, più complesso per verbi.
    * **Perdita di informazioni lessicali:** Distrugge informazioni semantiche.
    * Utilizzo consigliato: Solo quando non è necessaria elaborazione semantica.
    * Spesso combinato con rimozione delle *stop words*.
    * **Non combinare con lemmattizzazione:** Stemming è più aggressivo.
* **Dipendenza dalla lingua:** Algoritmi specifici per ogni lingua.
* **Esempio:** "automate(s)", "automatic", "automation" → "automat"
* **Esempi di equivalenze (semplificazione):** "compressed" e "compression" → "compress"; "compress" → "compress".

**III. Stemming di Porter (inglese)**

* **Algoritmo:** Iterazione su regole pre-costruite, applicando riduzioni basate sul *longest match*.
* **Notazione:** $[c](vc)m[v]$ (c=consonante, v=vocale, m=misura)
* **Struttura:** 5 fasi sequenziali con regole basate su condizioni e sostituzioni di suffissi.
    * Esempi di regole: `sses` → `ss`, `ed` → (vuoto se preceduto da vocale).
* **Esempio di applicazione:** `GENERALIZATIONS` → `GENERALIZATION` → `GENERALIZE` → `GENERAL` → `GENER`
* **Condizioni e sostituzioni (esempio parziale):**
    | Conditions | Suffix | Replacement | Examples |
    |---|---|---|---|
    |  | sses | ss | caresses -> caress |
    |  | ies | i | ponies -> poni, ties -> ti |
    |  | ss | ss | caress -> caress |
    |  | s |  | cats -> cat |
    | (m > 0) | eed | ee | feed -> feed, agreed -> agree |
    | (*v*) | ed |  | plastered -> plaster, bled -> bled |
    | (*v*) | ing |  | motoring -> motor, sing -> sing |
    | (m > 1) | e |  | probate -> probat, rate -> rate |
    | (m = 1 and not *o) | e |  | cease -> ceas |
    * `*v*` = stem contiene vocale.
    * `*o` = stem termina in `cvc` (seconda c ≠ W, X, Y).


**IV. Vantaggi dello Stemming**

* **Miglioramento del retrieval:** Risultati contrastanti, dipende dal vocabolario.
* **Perdita di distinzioni semantiche:** Possibile perdita di sottili sfumature di significato.
* **Inglese:** Migliora il recall ma danneggia la precisione per alcune query.


---

**Schema Riassuntivo: Algoritmi di Stemming**

* **Stemming:** Processo di riduzione delle parole alla loro radice (stem).
    * **Differenze linguistiche:** L'approccio "operativo" (es. "operative" → "oper") è più efficace per alcune lingue (inglese), mentre altri metodi sono preferibili per altre (spagnolo, tedesco, finlandese).
* **Algoritmi di Stemming:** Prestazioni generalmente simili.
    * **Stemmer di Lovins:**
        * Algoritmo a singolo passaggio.
        * Rimuove il suffisso più lungo possibile.
        * Si basa su circa 250 regole.


---

Schema riassuntivo:

* **Algoritmi di stemming**

    * **Stemmer di Paice/Husk:**  (Nessun dettaglio fornito nel testo, necessita di ulteriore informazione per un'espansione)
    * **Stemmer Snowball:** (Nessun dettaglio fornito nel testo, necessita di ulteriore informazione per un'espansione)


Lo schema è conciso, ma la mancanza di dettagli nel testo di input impedisce una maggiore espansione.  Per arricchirlo, è necessario fornire informazioni aggiuntive su ciascun stemmer.

---
