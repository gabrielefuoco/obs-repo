
**Compressione dei Dati in Information Retrieval**

* **Obiettivi della Compressione:**
    * Ridurre spazio di archiviazione su disco (costi ridotti).
    * Aumentare capacità di memoria principale.
    * Accelerare trasferimento dati (disco-memoria).
    * Migliorare prestazioni di lettura (con algoritmi efficienti).  Algoritmi di decompressione sono generalmente molto veloci.

* **Tipi di Compressione:**
    * **Compressione del Dizionario e dei File di Posting:**  Cruciale per la memoria (dizionario e liste di posting) e lo spazio su disco/velocità di lettura (file di posting).
    * **Compressione Senza Perdita vs. Con Perdita:** Prevalentemente senza perdita; con perdita usata con cautela (es. pre-elaborazione testo, poda liste di posting) per minima perdita di qualità.

**Relazione tra Dimensione del Vocabolario e Dimensione della Collezione**

* **Legge di Heaps:**  $M = kT^b$
    * M: dimensione del vocabolario (parole distinte).
    * T: numero di token nella collezione.
    * k e b: costanti (tipicamente $30 ≤ k ≤ 100$ e $b ≈ 0.5$).
    * Relazione lineare in scala log-log: $log M = log k + b log T$.
    * Esempio Reuters RCV1: $log_{10}M = 0.49 \log_{10}T + 1.64$  ($k≈ 44$, $b = 0.49$).  Buona aderenza empirica, tranne una fase transitoria iniziale.

**Distribuzioni Skew di Tipo Power-Law**

* **Caratteristiche:**
    * Concentrazione di massa in una piccola porzione della distribuzione.
    * Coda lunga (valori che diminuiscono lentamente).
    * Spesso osservata in fenomeni naturali e sociali.

* **Distribuzione di tipo Power-Law:** Modello matematico che descrive fenomeni con pochi elementi ad alto valore e molti ad basso valore (simile a funzione esponenziale).

* **Legge di Pareto (Principio 80-20):** Esempio di distribuzione power-law; l'80% degli effetti deriva dal 20% delle cause.

---

**Le leggi di Zipf e Heaps nel linguaggio naturale**

I. **Distribuzioni di tipo Power Law:**

   *   **Definizione:**  Pochi elementi hanno valori molto alti, mentre molti altri hanno valori molto bassi.
   *   **Esempi:**
      *   Distribuzione della ricchezza
      *   Numero di pagine dei siti web
      *   Numero di follower sui social network
      *   Popolazione delle città
      *   Frequenza delle parole in un documento (Legge di Zipf)

II. **Confronto tra Distribuzioni di Poisson e Power Law:**

   *   **Similitudini:** Entrambe asimmetriche.
   *   **Differenze:** La distribuzione di Poisson modella la probabilità di eventi in un intervallo, mentre la legge di potenza descrive la relazione tra due variabili ($y \propto x^k$).  La scelta dipende dal "min rate" nella distribuzione di Poisson.

III. **Legge di Heaps (Prima Legge di Potenza):**

   *   Stima la dimensione del vocabolario di un corpus testuale.

IV. **Legge di Zipf (Seconda Legge di Potenza):**

   *   **Formula:** $cf_{i} \propto \frac{1}{i} = \frac{K}{i}$, dove $cf_i$ è la frequenza del termine i-esimo,  `i` è il suo rango e `K` è una costante.
   *   **Forma logaritmica:** $log(cf_{i}) = log(K) - log(i)$ (relazione lineare inversa).
   *   **Interpretazione:** La frequenza di un termine decresce linearmente con il suo rango su scala logaritmica.
   *   **Esempio:** Se "the" appare $cf_1$ volte, "of" appare $cf_1/2$ volte, "and" $cf_1/3$ volte, ecc.
   *   **Osservazione:** Alta frequenza per ranghi bassi, frequenza decrescente linearmente su scala doppia logaritmica.

V. **Implicazioni della Legge di Zipf secondo Luhn:**

   *   **Parole molto frequenti:** Poco utili per l'indicizzazione (troppo generiche).
   *   **Parole molto rare:** Poco utili per l'indicizzazione (troppo rare).
   *   **Parole più discriminanti:** Hanno frequenza da bassa a media.


---

**I. Selezione dei Termini per l'Indicizzazione**

* **A. Frequenza Ottimale delle Parole:**
    * Le parole con frequenza intermedia sono le più utili per l'indicizzazione.
    * La distribuzione delle parole segue la legge di Zipf: frequenza ∝ 1/rango.
    * Il grafico mostra una distribuzione a coda lunga (vedi immagine).
* **B. Frequenze di Taglio:**
    * Scopo: escludere termini troppo frequenti (es. articoli, preposizioni) e troppo rari.
    * Criticità:
        * Determinazione delle frequenze di taglio è difficile, *task-dependent* e *data-driven*.
        * Dipendenza dal dominio e dal linguaggio.
    * Esempi di regole pratiche:
        * Rimozione termini >50% dei documenti.
        * Rimozione termini <3-5 documenti.

**II. Ponderazione della Rilevanza dei Termini e Recupero Classificato**

* **A. Limiti della Ricerca Booleana:**
    * Restituisce troppi o troppo pochi risultati.
    * Richiede utenti esperti.
    * Non user-friendly, soprattutto sul web.
* **B. Recupero Classificato (Ranked Retrieval):**
    * Risolve i problemi della ricerca booleana ordinando i documenti per pertinenza alla query.
    * Esprime quantitativamente la pertinenza.
    * Si concentra sul recupero, non sull'estrazione di informazioni (a meno che non sia necessario, come nel caso di pagine hub).
* **C. Query di Testo Libero:**
    * Sequenze di parole in linguaggio naturale.



---

**Recupero Classificato di Documenti**

I. **Introduzione al Recupero Classificato:**

    *   Supera il problema dell' "abbondanza o carestia" delle query di testo libero.
    *   Presenta solo i primi *k* risultati (es. 10), migliorando l'esperienza utente.
    *   Accetta l'intrinseca imprecisione delle query di testo libero, riflettendo l'esigenza informativa spesso non perfettamente definita.

II. **Punteggio di Pertinenza:**

    *   Assegna un punteggio di pertinenza (0-1) a ciascun documento.
    *   Il punteggio riflette la corrispondenza tra documento e query.
    *   Determina l'ordine di presentazione dei risultati, mostrando prima i documenti più pertinenti.

III. **Misure di Similarità per Insiemi Finiti:**

    *   Efficienti e normalizzano la lunghezza (indipendentemente dalle dimensioni di documenti e query).
    *   **Limiti:** Non considerano la frequenza del termine (*tf*) nel documento e la scarsità del termine nella collezione.
        *   *tf*:  Più alta la frequenza del termine di query nel documento, maggiore dovrebbe essere il punteggio.
        *   Scarsità: I termini rari sono più informativi di quelli frequenti.

IV. **Esempi di Misure di Similarità:**

    *   **Jaccard:**  $J(A,B) = \frac{\|A \cap B\|}{\|A \cup B\|}$  (Misura la similarità come rapporto tra intersezione e unione di due insiemi).
    *   **Sørensen-Dice:** $DSC(A,B) = \frac{2 \times \|A \cap B\|}{\|A\| + \|B\|}$ (Simile a Jaccard, ma pesa di più l'intersezione).
    *   **Overlap:** $O(A,B) = \frac{\|A \cap B\|}{min(\|A\|, \|B\|)}$ (Misura la sovrapposizione rispetto all'insieme più piccolo).
    *   **Simple Matching:** $SM(A,B) = \frac{\|A \cap B\| + \|\overline{A} \cap \overline{B}\|}{\|A \cup B\|}$ (Considera presenze e assenze di elementi).

V. **Considerazioni Aggiuntive:**

    *   La cardinalità dell'unione di due insiemi include la cardinalità della loro intersezione (formula di inclusione-esclusione).
    *   *Dice* è più "generoso" di *Jaccard*, utile in contesti specifici (indexing, Iris Neighbor Search).
    *   Non tutte le misure di similarità sono metriche (es. *Dice* non soddisfa la disuguaglianza triangolare).


---

## Schema Riassuntivo: Rappresentazione di Testi e Misure di Similarità

**I.  Misure di Similarità e Disuguaglianza Triangolare**

* **A.  Dice Coefficient:** Misura la similarità, non la distanza.
    *  Esempio: Documenti "AB", "A", "B".
    *  $Dice(1,2) = \frac{2}{3}$, $Dice(1,3) = \frac{2}{3}$, $Dice(2,3) = 0$.
    *  Viola la disuguaglianza triangolare: $Distanza(1,2) + Distanza(1,3) \geq Distanza(2,3)$ non è rispettata.

* **B. Overlap Coefficient (Simpson):**
    *  Definizione: $\frac{|X ∩ Y|}{min(|X|, |Y|)}$
    *  Applicazione: Confronto tra summary (es. uno di riferimento e uno generato).


**II.  Frequenze di Parole**

* **A. Collection Frequency:**
    *  Frequenza totale di una parola in un'intera collezione di documenti.
    *  Proprietà globale.
    *  Somma delle Term Frequency su tutti i documenti.

* **B. Term Frequency:**
    *  Frequenza di una parola in un singolo documento.
    *  Proprietà locale.

* **C. Document Frequency:**
    *  Numero di documenti che contengono un determinato termine.


**III. Rappresentazione dei Testi e Calcolo dello Score**

* **A. Matrice di Rappresentazione:**
    *  Ogni cella contiene il peso di un termine in un documento.
    *  Esprime il contributo del termine alla rappresentazione del documento.

* **B. Esempio di Tabella:** (Tabella di personaggi e opere con conteggi numerici)

* **C. Metodi di Calcolo dello Score:** (Menzionato ma non dettagliato nel testo fornito)


**IV.  Modelli di Rappresentazione dei Testi:**

*  (Menzionato ma non dettagliato nel testo fornito - solo il punto 1 è presente, senza descrizione)

---

**I. Metodi per il Calcolo del Peso di un Termine**

* **A. Utilizzo diretto di TF e CF:**
    * Formula:  $w_{t,d} = \text{tf}_{t,d} \frac{1}{\text{cf}_{t}}$
    *  $w_{t,d}$: peso del termine *t* nel documento *d*
    *  $\text{tf}_{t,d}$: frequenza del termine *t* nel documento *d*
    *  $\text{cf}_{t}$: frequenza del termine *t* nell'intera collezione
    * Nessuna normalizzazione iniziale.

* **B. Funzioni Separate per TF e CF:**
    * Formula: $w_{t,d}^{(t)} = f(\text{tf}_{t,d}) + g(\text{cf}_{t}) = \text{tf}_{t,d} + \log(\text{cf}_{t})$
    *  Il termine relativo a CF risulta troppo dominante.


**II. Considerazioni sulla Rilevanza di un Termine**

* **A. Fattori Influenzanti:**
    * Term Frequency (TF)
    * Collection Frequency (CF)
    * Document Frequency (DF)  (menzionata, ma non utilizzata in formule)
    * Tempo di inerzia (idea non sviluppata)
    * Legge di Zipf (influenza la distribuzione delle frequenze)

* **B. Funzione di Scoring:**
    * $Rilevanza(termine) = F(TF) + G(CF)$
    * Il peso relativo di F e G determina l'importanza di TF vs. CF.

* **C. Lunghezza dei Documenti:**
    * Nessuna assunzione sulla lunghezza dei documenti.

**III. Analisi delle Proposte di Scoring (Proposta 1 vs. Proposta 2)**

* **A. Coerenza con la Legge di Zipf:** Analisi focalizzata su frequenze medio-alte e medio-basse.

* **B. Caso 1: Termine ad alta frequenza (testa della distribuzione):**
    * **Proposta 1:** Peso prossimo a 0 indipendentemente da TF.
    * **Proposta 2:** Dominanza del termine logaritmico di CF, potenzialmente smorzabile.

* **C. Caso 2: Termine a bassa/media frequenza:**
    * **Proposta 1:** TF smorzata da CF.
    * **Proposta 2:** TF enfatizzata rispetto a CF.

* **D. Problemi:**
    * Nessuna proposta risulta efficace in modo consistente, il comportamento varia a seconda del termine e delle caratteristiche del documento.


**IV. Problematiche Aggiuntive:**

* Smorzamento lineare inverso di TF con CF troppo aggressivo.
* Moltiplicazione diretta di TF e CF troppo aggressiva.


---

**Calcolo del Peso dei Termini nei Documenti**

I. **Problematiche nel Calcolo del Peso:**

    * A. Necessità di considerare la *document frequency*:
        * 1. Più discriminante della *collection frequency*.
        * 2. Distribuzione potenzialmente più piatta.
    * B. Inefficacia della combinazione lineare:
        * 1. Dominanza di un termine sull'altro.
    * C. Aggressività della funzione reciproca lineare, anche con *document frequency*.
    * D. Soluzione proposta: Smorzamento *smooth* con la formula:  $\frac{1}{log(document frequency)}$
        * 1. Peso basso per termini in quasi tutti i documenti (document frequency ≈ n).


II. **Smorzamento della Term Frequency (TF):**

    * A. Importanza dello smorzamento per il calcolo del peso dei termini.
    * B. Valori di TF > 1 non sono problematici (limitati da n).
    * C. Smorzamento tramite divisione per log₂n.
    * D. Smorzamento più accentuato per termini rari.


III. **Funzione TF-IDF:**

    * A. Formula:  $w_{t,d}=\log(1+tf_{t,d}) \times \log_{10}\left( \frac{N}{df_{t}} \right)$
        * 1.  `tf<sub>t,d</sub>`: Frequenza del termine *t* nel documento *d*.
        * 2.  `N`: Numero totale di documenti.
        * 3.  `df<sub>t</sub>`: Numero di documenti contenenti il termine *t*.
    * B. Interpretazione:
        * 1. `log(1+tf<sub>t,d</sub>)`: Frequenza del termine nel documento (smorzata).
        * 2. `log<sub>10</sub>(N/df<sub>t</sub>)`: Inverse Document Frequency (IDF) (elevata per termini rari).
    * C. Vantaggi:
        * 1. Penalizza termini comuni.
        * 2. Evidenzia termini rari.
        * 3. Bilancia frequenza locale e globale.
    * D. Considerazioni importanti:
        * 1. Rimozione delle stop words.
        * 2. Stemming e lemmatization.


---

**I.  Modellazione della Rilevanza dei Documenti**

* **A.  Bag-of-words (BoW)**
    *   **Pro:** Corrispondenza parziale, punteggi graduati, efficiente per grandi dataset, modello spazio vettoriale.
    *   **Contro:** Ignora sintassi, semantica, controllo booleano;  potenziale preferenza per documenti con alta frequenza di un termine, ma assenza dell'altro in una query a due termini.

* **B.  Frequenza del Termine (TF)**
    *   **1.  Problematiche della frequenza grezza:** La rilevanza non aumenta proporzionalmente alla frequenza.
    *   **2.  Peso di frequenza logaritmica:**  $w_{t,d} \begin{cases} 1+\log_{10}\text{tf}_{td} \ \text{ if tf}_{td} \ >0 \\ 0,\ \text{otherwise} \end{cases}$
        *   Esempi: 0 → 0, 1 → 1, 2 → 1.3, 10 → 2, 1000 → 4,...
    *   **3.  Punteggio documento-query:** $\sum_{t\in q \cap d}(1+\log(tf_{t,d}))$

* **C.  Frequenza Inversa del Documento (IDF)**
    *   Termini rari sono più informativi;  es. "*arachnocentrico*".
    *   Maggiore rarità implica maggiore peso.

* **D.  TF-IDF e Smorzamento**
    *   **1.  Soglie di taglio:**  Escludono termini con frequenza troppo alta o bassa.
    *   **2.  Smorzamento logaritmico:** Gestisce la distribuzione dei termini (Legge di Zipf).
        *   Vantaggi: Evita soglie arbitrarie, gestione di matrici sparse.
    *   **3.  Doppio logaritmo:** Per corpus molto grandi, ulteriore smorzamento del TF.
    *   **4.  Normalizzazione e Similarità:** Aspetti cruciali per il recupero delle informazioni e la classificazione.


---

**TF-IDF (Term Frequency-Inverse Document Frequency)**

I. **Document Frequency (df) vs. Collection Frequency (cf)**
    * df: Numero di documenti in cui un termine appare.  Preferito a cf per la discriminazione tra documenti.
    * Esempio: "assicurazione" (cf=10440, df=3997), "prova" (cf=10422, df=8760).  df è più informativo per la ricerca.

II. **Inverse Document Frequency (idf)**
    * Misura inversa dell'informatività di un termine.
    * Formula:  $$idf_{t}=\log_{10}\left( \frac{N}{df_{t}} \right)$$  dove *N* è il numero totale di documenti.
    * Il logaritmo smorza l'effetto dell'idf.
    * La df di un termine è unica e influenza la classificazione solo per query con pochi termini (*k*<1).

III. **TF-IDF Weighting**
    * Assegna un peso ai termini basato su frequenza nel documento (tf) e rarità nel corpus (idf).
    * **tf (term frequency):** Peso maggiore per termini più frequenti in un documento.
    * **idf (inverse document frequency):** Peso maggiore per termini più rari nel corpus.
    * Formula del peso tf-idf: $$w_{t,d}=\log(1+tf_{t,d})\times\log_{10}\left( \frac{N}{df_{t}} \right)$$
    * Aumenta con il numero di occorrenze nel documento e con la rarità del termine.
    * Punteggio documento-query: somma dei pesi tf-idf dei termini comuni a documento e query.
    * Riflette la distribuzione di probabilità power-law della frequenza dei termini (termini comuni hanno bassa probabilità, termini rari alta probabilità).

IV. **Varianti TF-IDF**
    * Diverse formule per il calcolo di tf: con o senza logaritmi.
    * Ponderazione dei termini nella query: ponderati o non ponderati.
    * Esempi di varianti:  $$ \frac{tf_{i,d}}{\max_{j}tf_{j,d}} ,\ \frac{tf_{id}}{\sqrt{ \sum_{j}(tf_{j,d})^2 }} ,\ \frac{tf_{id} \cdot idf_{i}}{\sqrt{ \sum_{j}(tf_{j,d} \cdot idf_{j})^2 }} $$

V. **Assunzioni e Principi Chiave**
    * **Assunzioni:** Collezione di documenti omogenea per dominio e pattern di frequenza simili tra documenti.
    * **Principi chiave:** Peso variabile per uno stesso termine a seconda del documento; normalizzazione della lunghezza del documento necessaria.


---

**I. Rappresentazione dei Documenti come Vettori**

* **Spazio Vettoriale:** Documenti e query rappresentati come vettori in uno spazio ad alta dimensionalità e molto sparso.
    * Assi: Termini del vocabolario.
    * Punti: Documenti.
* **Similarità:** Prossimità tra documenti e query misurata dalla similarità dei vettori.
    * Prossimità ≈ inversa della distanza.

**II. Matrice di Peso Tf-Idf**

* **Calcolo del Peso:** Utilizza la matrice Tf-Idf per rappresentare la rilevanza dei termini nei documenti.  ![[1) Intro-20241014151203327.png]]
* **Problema della Lunghezza dei Documenti:** Senza normalizzazione, la distanza euclidea favorisce i documenti più lunghi.

**III. Smoothing**

* **Scopo:** Ridurre l'influenza eccessiva dei termini rari.

**IV. Normalizzazione e Distanza Euclidea**

* **Normalizzazione L2:** Divisione per la norma L2.
    * Diluisce il segnale informativo, soprattutto con documenti di lunghezza diversa.
    * Proprietà geometriche specifiche, diverse da altri metodi (es. normalizzazione al massimo).
    * Influenza diretta sulla costruzione della matrice dei dati.
* **Problemi della Distanza Euclidea:**
    * Sensibilità alla lunghezza dei vettori.
    * Distanza elevata anche con distribuzione di termini simile, ma lunghezze diverse.
    * Controesempio: distanza elevata tra un documento e la sua concatenazione con se stesso.
    * Maggiore sensibilità all'alta dimensionalità rispetto a misure di correlazione.

**V. Normalizzazione Implicita e Angolo Coseno**

* **Approccio Scale-Invariant:** Misura della prossimità basata sull'angolo tra i vettori (indipendente dalla lunghezza).
* **Coseno Similitudine:** Classificazione dei documenti in base all'angolo (o al coseno dell'angolo) tra query e documento.
    * $coseno(query, documento) = \frac{query \cdot documento}{||query|| \cdot ||documento||}$
* **Normalizzazione Implicita:** Se i vettori sono normalizzati (lunghezza unitaria), la formula si semplifica al prodotto scalare.



---

**Normalizzazione dei Vettori e Similarità del Coseno**

* **Conseguenze della Normalizzazione:**
    * **Vettore Unitario:**  Dividendo un vettore per la sua norma L2 ($\sqrt{\sum_{i=1}^{n} w_i^2}$), si ottiene un vettore unitario (lunghezza 1) sulla superficie dell'ipersfera unitaria.
    * **Pesi Comparabili:** Permette il confronto equo tra documenti di lunghezza diversa nei modelli di informazione, evitando che la lunghezza influenzi sproporzionatamente la similarità.  La similarità del coseno è un esempio:  $\text{sim}(d_1, d_2) = \frac{d_1 \cdot d_2}{\|d_1\| \cdot \|d_2\|} = \frac{\sum_{i=1}^{n} w_{i,j} \cdot w_{i,k}}{\sqrt{\sum_{i=1}^{n} w_{i,j}^2} \cdot \sqrt{\sum_{i=1}^{n} w_{i,k}^2}}$

* **Algoritmo Cosine Score:**
    * Calcola la similarità tra query e documenti in un modello a spazio vettoriale.
    * Inizializza un array `Scores` a 0 e un array `Length`.
    * Per ogni termine della query:
        * Calcola il peso `w_t,q` e recupera la lista di occorrenze.
        * Per ogni coppia (documento `d`, frequenza del termine `tf_(t,d)`, peso `w_(t,q)`) nella lista: `Scores[d] += w_(t,d) × w_(t,q)`.
    * Legge l'array `Length`.
    * Per ogni documento `d`: `Scores[d] = Scores[d]/Length[d]`.
    * Restituisce i `K` punteggi più alti.


**Varianti di Ponderazione Tf-Idf**

* **Term Frequency (tf):**
    * `n` (naturale): $tf_{r, d}$
    * `l` (logaritmico): $1 + \log(tf_{r, d})$
    * `a` (aumentato): $0.5 + \frac{0.5 \cdot tf_{r, d}}{\max_{r} (tf_{r, d})}$
    * `b` (booleano): $\begin{cases} 1 & \text{if } tf_{r, d} > 0 \\ 0 & \text{otherwise} \end{cases}$

* **Document Frequency (df):**
    * `n` (nessuno): $1$
    * `t` (idf): $\log \frac{N}{df_r}$
    * `p` (prob idf): $\max \{ 0, \log \frac{N - df_r}{df_r} \}$

* **Normalizzazione:**
    * `n` (nessuna): $1$
    * `c` (coseno): $\frac{1}{\sqrt{w_1^2 + w_2^2 + \dots + w_n^2}}$
    * `u` (pivoted unique): $\frac{1}{u}$
    * `b` (byte size): $\frac{1}{\text{CharLength}^{\alpha}}, \alpha < 1$

* **Soluzioni di default:** Spesso le seconde opzioni di ogni tipo.
* **Term frequency aumentata:** Utile per il retrieval puro con query espanse, offre uno smoothing.
* **Notazione SMART (ddd.qqq):**  Combinazione di ponderazioni per documenti e query (es: lnc.ltc - logaritmico, nessun idf, coseno per documenti; logaritmico, idf, coseno per query).



---

**Classifica dello spazio vettoriale per la ricerca di assicurazioni auto**

I. **Metodo:**

   * Rappresentazione: Vettori tf-idf ponderati per documenti e query.
   * Similarità: Calcolo del punteggio di similarità del coseno tra il vettore query e ogni vettore documento.
   * Classifica: Ordinamento dei documenti in base al punteggio di similarità.
   * Output: Restituzione dei primi K documenti.

II. **Vantaggi:**

   * Corrispondenza parziale e punteggi/classifica naturali.
   * Buone prestazioni pratiche nonostante semplificazioni.
   * Implementazione efficiente.

III. **Svantaggi:**

   * Mancanza di informazioni sintattiche.
   * Mancanza di informazioni semantiche.
   * Ipotesi di indipendenza dei termini (Bag-of-Words - BoW).
   * Ipotesi di ortogonalità a coppie dei vettori dei termini.
   * Assenza di controllo booleano (es. richiesta di presenza di un termine).

IV. **Esempio:** (Punteggio = 0 + 0 + 0.27 + 0.53 = 0.8)  -  Questo punto mostra un esempio numerico di calcolo del punteggio, ma non è un punto principale concettuale come gli altri.  Potrebbe essere omesso o spostato come sotto-punto di I.

---
