
**I. Importanza della Compressione dei Dati in Information Retrieval**

*   **A. Vantaggi:**
    *   Riduzione dello spazio di archiviazione su disco (costi ridotti)
    *   Aumento della capacità di memoria principale
    *   Accelerazione del trasferimento dati (disco a memoria)
    *   Miglioramento delle prestazioni di lettura (con algoritmi efficienti)
*   **B. Tipi di Compressione:**
    *   Compressione del dizionario (cruciale per memoria)
    *   Compressione dei file di postings (cruciale per spazio su disco e velocità di lettura)
*   **C. Metodi di Compressione:**
    *   Compressione senza perdita (prevalente)
    *   Compressione con perdita (usata con cautela, minima perdita di qualità)

**II. Dimensione del Vocabolario vs. Dimensione della Collezione**

*   **A. Relazione:**
    *   La dimensione del vocabolario (M) cresce con la dimensione della collezione (T).
    *   Nessun limite superiore definito, specialmente con Unicode.
*   **B. Legge di Heaps:**
    *   Formula: $M = kT^b$
        *   M = dimensione del vocabolario
        *   T = numero di token nella collezione
        *   k = offset (tipicamente 30 ≤ k ≤ 100)
        *   b ≈ 0.5
    *   Descrizione:
        *   Power law (funzione lineare in scala doppia logaritmica con offset k)
        *   Grafico log-log: linea con pendenza di circa ½
        *   Relazione lineare più semplice in spazio log-log: $log M = log k + b log T$
*   **C. Legge di Heaps per Reuters RCV1:**
    *   Formula: $log_{10}M = 0.49 \log_{10}T + 1.64 \to M = 10^{1.64}T^{0.49}$
        *   k = $10^{1.64} ≈ 44$
        *   b = 0.49
    *   Buona aderenza empirica (con fase transitoria iniziale)

**III. Distribuzione Skewd di Tipo Power-law**

*   **A. Caratteristiche:**
    *   Concentrazione di massa in una piccola zona
    *   Coda lunga (valori che diminuiscono lentamente)
*   **B. Distribuzione di Tipo Power-law:**
    *   Modello matematico per fenomeni naturali e sociali
    *   Concentrazione di massa e coda lunga (simile alla funzione esponenziale)
*   **C. Legge di Pareto:**
    *   Esempio di distribuzione power-law (principio 80-20)

---

**I. Distribuzioni di tipo Power Law**

*   **A. Definizione:** Pochi elementi sono molto frequenti, molti altri sono molto rari.
*   **B. Esempi:**
    *   Distribuzione della ricchezza tra individui.
    *   Numero di pagine di siti web.
    *   Numero di follower di un social network.
    *   Dimensione delle città.
    *   Frequenza delle parole in un documento.

**II. Distribuzione di Poisson vs. Legge di Potenza**

*   **A. Similitudini:** Entrambe sono distribuzioni asimmetriche (skewed).
*   **B. Differenze:**
    *   **Poisson:** Distribuzione spaziale, probabilità di eventi in un intervallo di tempo/spazio. Dipende dal tasso minimo (min rate).
    *   **Legge di Potenza:** Relazione tra due variabili, una varia proporzionalmente a una potenza dell'altra.

**III. Legge di Heaps e Legge di Zipf**

*   **A. Legge di Heaps (Prima Legge di Potenza):** Stima la dimensione del vocabolario in un corpus.
*   **B. Legge di Zipf (Seconda Legge di Potenza):** Relazione tra frequenza e rango di un termine.

**IV. Legge di Zipf: Formulazione**

*   **A. Relazione Empirica:** La frequenza di un termine è inversamente proporzionale al suo rango.
*   **B. Formula:**
    *   $cf_{i} \propto \frac{1}{i} = \frac{K}{i}$
        *   $cf_{i}$: Frequenza del termine i-esimo.
        *   $i$: Rango del termine nel vocabolario.
        *   $K$: Costante di normalizzazione.
*   **C. Forma Logaritmica:**
    *   $log(cf_{i}) = log(K) - log(i)$
        *   Relazione lineare inversa tra logaritmo della frequenza e logaritmo del rango.
        *   Power law con slope negativa.
*   **D. Esempio:** Se "the" si verifica $cf1$ volte, "of" si verifica $cf1/2$ volte, "and" si verifica $cf1/3$ volte, ecc.
*   **E. Grafico:** Alta frequenza per rank bassi (origine), frequenza decresce linearmente in scala log-log.

**V. Implicazioni della Legge di Zipf (Luhn)**

*   **A. Parole Comuni:** Non utili per l'indicizzazione (troppo generiche).
*   **B. Parole Rare:** Non utili per l'indicizzazione (troppo rare).
*   **C. Parole Discriminanti:** Frequenza da bassa a media.

---

## Schema Riassuntivo: Indicizzazione, Frequenze di Taglio e Recupero Classificato

**1. Importanza della Frequenza dei Termini nell'Indicizzazione**

   *   Le parole con frequenza intermedia sono le più utili per l'indicizzazione.
        *   Forniscono informazioni specifiche sul contenuto.
        *   Non sono né troppo rare (insignificanti) né troppo comuni (poco discriminanti).
   *   Distribuzione delle parole segue la legge di Zipf.
        *   Frequenza inversamente proporzionale al rango.
        *   Parole più frequenti molto più comuni delle meno frequenti.

**2. Frequenze di Taglio (Pruning del Vocabolario)**

   *   **Definizione:** Frequenze superiore e inferiore per escludere termini non utili.
   *   **Scopo:** Escludere termini troppo frequenti o troppo rari.
        *   **Termini troppo frequenti:** Presenti in molti documenti, poco significativi (es. articoli, preposizioni).
        *   **Termini troppo rari:** Presenti in pochi documenti, poco utili per l'analisi generale.
        *   **Obiettivo:** Migliorare efficienza e accuratezza.
   *   **Criticità:**
        *   Difficile individuare frequenze ottimali.
        *   Dipendenza dal task specifico e dal dominio dei dati.
   *   **Esempi di Regole Pratiche:**
        *   Rimozione termini troppo frequenti: presenti in > 50% dei documenti.
        *   Rimozione termini troppo rari: presenti in < 3-5 documenti.

**3. Ponderazione della Rilevanza e Recupero Classificato**

   *   **Necessità di ordinamento dei risultati:** Utile quando il set di risultati è ampio e l'utente non può ispezionarlo interamente.
   *   **Ricerca Booleana:**
        *   Limiti: troppi/troppo pochi risultati, adatta solo ad utenti esperti.
        *   Non user-friendly, specialmente nel web.
   *   **Recupero Classificato (Ranked Retrieval):**
        *   Risolve i problemi della ricerca booleana.
        *   Restituisce documenti ordinati per pertinenza alla query.
        *   Si basa sull'espressione quantitativa della pertinenza.
        *   Si parla di **recupero** e non di **estrazione di informazioni**.
   *   **Query di Testo Libero:**
        *   Sequenze di parole in linguaggio naturale.
        *   Alternative ai linguaggi di query con operatori ed espressioni.

---

**Schema Riassuntivo: Recupero Classificato e Misure di Similarità**

**1. Introduzione al Recupero Classificato**
    *   Associato a query di testo libero.
    *   Risolve il problema dell'"abbondanza o carestia" presentando solo i primi *k* risultati (circa 10).
    *   Gestisce l'incertezza delle query di testo libero tramite la classificazione dei risultati.

**2. Punteggio di Pertinenza**
    *   Base del recupero classificato.
    *   Assegna un punteggio (0-1) a ciascun documento in base alla corrispondenza con la query.
    *   Determina l'ordine di presentazione dei risultati (dal più rilevante al meno rilevante).

**3. Misure di Similarità per Insiemi Finiti**
    *   Utilizzate per calcolare il punteggio di pertinenza.
    *   Efficienti e normalizzano la lunghezza dei documenti e delle query.
    *   Esempi:
        *   Jaccard
        *   Sørensen-Dice
        *   Overlap
        *   Simple Matching

**4. Limitazioni delle Misure di Similarità Semplici**
    *   Non considerano la frequenza del termine (*tf*) nel documento.
    *   Non considerano la scarsità del termine nella collezione.

**5. Coefficienti di Similarità: Formule e Descrizioni**

    *   **Jaccard:**
        *   Formula: $J(A,B) = \frac{\|A \cap B\|}{\|A \cup B\|}$
        *   Descrizione: Dimensione dell'intersezione divisa per la dimensione dell'unione. Varia tra 0 e 1.
    *   **Sørensen-Dice:**
        *   Formula: $DSC(A,B) = \frac{2 \times \|A \cap B\|}{\|A\| + \|B\|}$
        *   Descrizione: Pesa doppiamente gli elementi in comune. Varia tra 0 e 1.
    *   **Overlap:**
        *   Formula: $O(A,B) = \frac{\|A \cap B\|}{min(\|A\|, \|B\|)}$
        *   Descrizione: Dimensione dell'intersezione divisa per la cardinalità dell'insieme più piccolo. Varia tra 0 e 1.
    *   **Simple Matching:**
        *   Formula: $SM(A,B) = \frac{\|A \cap B\| + \|\overline{A} \cap \overline{B}\|}{\|A \cup B\|}$
        *   Descrizione: Considera sia le presenze che le assenze di elementi. Varia tra 0 e 1.

**6. Relazione tra Cardinalità e Coefficienti**
    *   Cardinalità dell'unione e dell'intersezione sono indipendenti.
    *   Coefficiente di Dice più "generoso" di Jaccard, utile in contesti specifici.

**7. Misure di Distanza e Metriche**
    *   Disuguaglianza triangolare definisce una metrica.
    *   Jaccard è una metrica.
    *   Dice non è una metrica perché non soddisfa la disuguaglianza triangolare.

---

**Schema Riassuntivo**

**1. Violazione della Disuguaglianza Triangolare con Dice**

   *   **1.1. Concetto:** La disuguaglianza triangolare non è sempre soddisfatta quando si usano misure di similarità come Dice.
   *   **1.2. Esempio:**
        *   Documento 1: AB
        *   Documento 2: A
        *   Documento 3: B
        *   Disuguaglianza attesa: $Distanza(1,2) + Distanza(1,3) \geq Distanza(2,3)$
        *   Calcolo con Dice:
            *   $Dice(1,2) = \frac{2}{3}$
            *   $Dice(1,3) = \frac{2}{3}$
            *   $Dice(2,3) = 0$
        *   Risultato: $\frac{2}{3} + \frac{2}{3} \ngeq 0$ (La disuguaglianza non è rispettata)

**2. Overlap Coefficient (Simpson)**

   *   **2.1. Definizione:** Misura di similarità basata sull'intersezione tra due insiemi divisa per la cardinalità minima.
   *   **2.2. Formula:** $Overlap(X,Y) = \frac{{|X ∩ Y|}}{{min(|X|, |Y|)}}$
   *   **2.3. Applicazione:** Confronto di porzioni di testo, come summary di riferimento e summary generati automaticamente.

**3. Frequenze dei Termini**

   *   **3.1. Frequenza di Collezione (Collection Frequency)**
        *   **3.1.1. Definizione:** Numero totale di occorrenze di una parola in un'intera collezione di documenti.
        *   **3.1.2. Proprietà:** Globale (riguarda l'intera collezione).
        *   **3.1.3. Calcolo:** Somma delle Term Frequency su tutti i documenti della collezione.
   *   **3.2. Frequenza di Termine (Term Frequency)**
        *   **3.2.1. Definizione:** Numero di volte in cui un termine appare in un singolo documento.
        *   **3.2.2. Proprietà:** Locale (riguarda un singolo documento).
   *   **3.3. Document Frequency**
        *   **3.3.1. Definizione:** Numero di documenti che contengono un determinato termine.

**4. Rappresentazione dei Testi: Calcolo dello Score**

   *   **4.1. Concetto:** Ogni cella della matrice di rappresentazione contiene un valore reale che indica il peso del termine nel documento.
   *   **4.2. Obiettivo:** Esprimere quanto un termine contribuisce a rappresentare il contenuto del documento.

---

**Schema Riassuntivo: Calcolo dello Score di un Documento**

**1. Rappresentazione dei Testi:**

*   **1.1. Term Frequency (TF) e Collection Frequency (CF):**
    *   Utilizzo diretto di TF e CF per calcolare il peso di un termine.
    *   Formula: $w_{t,d}=\text{tf}_{t,d} \frac{1}{\text{cf}_{t}}$
        *   $w_{t,d}$: peso del termine *t* nel documento *d*.
        *   $\text{tf}_{t,d}$: frequenza del termine *t* nel documento *d*.
        *   $\text{cf}_{t}$: frequenza del termine *t* nell'intera collezione.
    *   Nessuna normalizzazione dei valori.

*   **1.2. Funzioni Separate per TF e CF:**
    *   Utilizzo di due funzioni separate *f* e *g* per modellare l'influenza di TF e CF.
    *   Formula: $w_{t,d}^{(t)}=f(\text{tf}_{t,d})+g(\text{cf}_{t})=\text{tf}_{t,d}+ly(\text{cf}_{t})$
    *   CF risulta troppo dominante.

**2. Considerazioni Aggiuntive:**

*   **2.1. Lunghezza dei Documenti:**
    *   Assunzione di lunghezza media dei documenti.
    *   Possibile alternanza tra documenti lunghi e brevi.

*   **2.2. Smorzamento della Term Frequency:**
    *   Smorzamento lineare inverso con CF appiattisce troppo i valori.

**3. Rilevanza di un Termine in un Documento:**

*   **3.1. Tempo di Inerzia:**
    *   Utilizzo del tempo di inerzia come indicatore di rilevanza.
    *   TF da sola non è sufficiente.

*   **3.2. Importanza di CF e DF:**
    *   CF (Collection Frequency) e DF (Document Frequency) sono importanti.
    *   Legge di Zipf: termini popolari tendono ad essere più intensi localmente.

---

**I. Funzione di Scoring e Rilevanza**

*   **A.** Definizione:** Rilevanza basata su TF e CF:
    *   `Rilevanza(termine) = F(TF) + G(CF)`
*   **B.** Peso Relativo:
    *   F cresce più rapidamente di G: Maggiore peso alla rilevanza locale (TF).
    *   G cresce più rapidamente di F: Maggiore peso alla CF.
*   **C.** TF vs. CF:
    *   TF generalmente minore di CF, specialmente in corpus grandi.
*   **D.** Lunghezza dei Documenti:
    *   Nessuna assunzione sulla lunghezza dei documenti.
*   **E.** Esempio Medico:
    *   Sia CF che TF rilevanti.
*   **F.** Problemi con la Moltiplicazione:
    *   Moltiplicare TF e CF smorza eccessivamente la rilevanza.

**II. Coerenza con la Legge di Zipf e Analisi delle Proposte**

*   **A.** Focus:
    *   Analisi dell'importanza dei termini in intervalli di frequenza medio-alti e medio-bassi.
*   **B.** Caso 1: Termine nella testa della distribuzione
    *   **1.** Proposta 1: Peso del termine ≈ 0 (CF molto alta).
    *   **2.** Proposta 2: log(CF) alto, ma smorzabile; TF alta (raro). Dominante: log(CF).
*   **C.** Caso 2: Termine non nella testa della distribuzione
    *   **1.** Proposta 1: TF / CF (TF smorzata).
    *   **2.** Proposta 2: TF + log(CF) (TF enfatizzata).
*   **D.** Problemi:
    *   Nessuna proposta efficace (comportamento variabile).
    *   Impossibile calcoli precisi senza conoscere le caratteristiche del documento.
*   **E.** Considerazioni Aggiuntive:
    *   Document Frequency (DF) vs. CF: DF potenzialmente più discriminante.
    *   Obiettivo: Valorizzare l'importanza dei termini in un documento rispetto a una query.
    *   Combinazione lineare inefficace: Un termine può dominare l'altro.
    *   Funzione reciproca lineare troppo aggressiva.
*   **F.** Soluzione Proposta:
    *   Smorzamento *smooth*: `1 / log(document frequency)`
    *   Spiegazione:
        *   DF ≈ n (numero totale di documenti): Termine poco discriminante, peso basso.

**III. Smorzamento della Term Frequency (TF) e Inverse Document Frequency (IDF)**

*   **A.** Importanza dello Smorzamento TF:
    *   Cruciale nel calcolo del peso di un termine.
*   **B.** Valori di TF > 1:
    *   Non problematici (TF massima limitata da n).
*   **C.** Metodo di Smorzamento:
    *   TF / log(n) (es. log₂n).
*   **D.** Effetto dello Smorzamento:
    *   Più accentuato per termini rari (presenti in pochi documenti).
*   **E.** Esempio:
    *   Termine in 3 documenti: TF / log₂3.

---

**I. Funzione TF-IDF (Term Frequency - Inverse Document Frequency)**

    A. Definizione: Misura statistica che valuta l'importanza di un termine in un documento rispetto a un corpus.
    B. Formula:
        *  $w_{t,d}=\log(1+tf_{t,d}) \times \log_{10}\left( \frac{N}{df_{t}} \right)$
        *  Dove:
            *  $tf_{t,d}$: Frequenza del termine *t* nel documento *d*.
            *  *N*: Numero totale di documenti nel corpus.
            *  $df_{t}$: Numero di documenti in cui il termine *t* compare.
    C. Interpretazione della Formula:
        *  $\log(1+tf_{t,d})$: Frequenza del termine nel documento (smorzata).
        *  $\log_{10}(N/df_{t})$: Frequenza inversa del documento (IDF).
            *  IDF alto: Termine raro e informativo.
            *  IDF basso: Termine comune e poco informativo.
    D. Vantaggi:
        *  Penalizza i termini comuni.
        *  Evidenzia i termini rari.
        *  Bilancia frequenza locale e globale.
    E. Considerazioni Importanti:
        *  Rimozione delle stop words.
        *  Stemming e lemmatization.
        *  Soglie di taglio.
    F. Smorzamento e Legge di Zipf:
        *  Lo smorzamento logaritmico aiuta a gestire la distribuzione dei termini descritta dalla legge di Zipf.
    G. Vantaggi dello smorzamento:
        *  Evita soglie di taglio arbitrarie.
        *  Permette di lavorare con matrici TF-IDF sparse.
    H. Doppio Logaritmo:
        *  Utilizzabile per smorzare ulteriormente il peso del TF in corpus molto grandi.
    I. Normalizzazione e Calcolo della Similarità:
        *  Cruciali per il recupero delle informazioni e la classificazione dei documenti.

**II. Modello Bag-of-words (BoW)**

    A. Definizione: Modello che ignora l'ordine delle parole, considerando solo la frequenza dei termini.
    B. Ipotesi: Indipendenza dei termini.
    C. Contro:
        *  Informazioni sintattiche mancanti.
        *  Informazioni semantiche mancanti.
        *  Manca il controllo di un modello booleano.

---

## Schema Riassuntivo: Term Frequency-Inverse Document Frequency (tf-idf)

**1. Introduzione al Modello Spazio Vettoriale**

*   **1.1. Motivazione:** Superare i limiti del modello booleano.
    *   Corrispondenza parziale e punteggi di rilevanza.
    *   Buone prestazioni pratiche.
    *   Implementazione efficiente.
    *   Rappresentazione di query e documenti come vettori.

**2. Term Frequency (tf)**

*   **2.1. Problema della Frequenza Grezza:** La rilevanza non è proporzionale alla frequenza grezza.
*   **2.2. Peso di Frequenza Logaritmica:**
    *   Formula:
        $$w_{t,d} \begin{cases} 1+\log_{10}\text{tf}_{td} \ \text{ if tf}_{td} \ >0 \\ 0,\ \text{otherwise} \end{cases}$$
    *   Esempio: $0 → 0, \quad1 → 1,\quad 2 → 1.3,\quad 10 → 2,\quad 1000 → 4,\quad \dots$
*   **2.3. Punteggio Documento-Query:**
    *   Formula: $$\sum_{t\in q \cap d}(1+\log(tf_{t,d}))$$
    *   Il punteggio è 0 se nessun termine della query è presente nel documento.

**3. Inverse Document Frequency (idf)**

*   **3.1. Motivazione:** Termini rari sono più informativi.
*   **3.2. Document Frequency (df) vs. Collection Frequency (cf):**
    *   Si preferisce df perché discrimina meglio tra i documenti.
    *   Esempi: "assicurazione" (cf=10440, df=3997), "prova" (cf=10422, df=8760).
*   **3.3. Formula idf:**
    *   $$idf_{t}=\log_{10}\left( \frac{N}{df_{t}} \right)$$
    *   Dove *N* è il numero totale di documenti.
*   **3.4. Proprietà:**
    *   La df di un termine è unica.
    *   Influisce sulla classificazione solo per query a *k* termini (*k*<1).

**4. Term Frequency-Inverse Document Frequency (tf-idf)**

*   **4.1. Concetto:** Pesa i termini in base alla frequenza nel documento e alla rarità nel corpus.
*   **4.2. Componenti:**
    *   **tf (term frequency):** Maggiore frequenza = maggiore peso.
    *   **idf (inverse document frequency):** Maggiore rarità = maggiore peso.
*   **4.3. Allineamento con la Legge di Distribuzione della Probabilità:**
    *   Riduce il peso dei termini comuni e aumenta il peso dei termini rari.

---

**I. Tf-Idf (Term Frequency-Inverse Document Frequency)**

   *   **A. Definizione:**
        *   Peso tf-idf: Prodotto del peso tf e del peso idf.
        *   Formula:  $$w_{t,d}=\log(1+tf_{t,d})\times\log_{10}\left( \frac{N}{df_{t}} \right)$$
            *   $tf_{t,d}$: Frequenza del termine *t* nel documento *d*.
            *   $N$: Numero totale di documenti nella collezione.
            *   $df_{t}$: Numero di documenti in cui compare il termine *t*.
   *   **B. Caratteristiche:**
        *   Aumenta con il numero di occorrenze del termine in un documento.
        *   Aumenta con la rarità del termine nella collezione.
   *   **C. Punteggio Documento-Query:**
        *   Somma dei pesi tf-idf dei termini *t* presenti sia nella query *q* che nel documento *d*.

**II. Varianti di Tf-Idf**

   *   **A. Differenze Principali:**
        *   Calcolo del termine "tf" (frequenza del termine).
        *   Ponderazione dei termini nella query.
   *   **B. Calcolo del Termine "tf":**
        *   Con logaritmi: Attenua l'influenza dei termini molto frequenti.
        *   Senza logaritmi: Utilizzo diretto della frequenza del termine.
   *   **C. Ponderazione dei Termini nella Query:**
        *   Ponderati: I termini nella query hanno pesi diversi in base alla loro importanza.
        *   Non ponderati: Tutti i termini nella query hanno lo stesso peso.
   *   **D. Esempi di varianti:**
        *   $$ \frac{tf_{i,d}}{\max_{j}tf_{j,d}} $$
        *   $$ \frac{tf_{id}}{\sqrt{ \sum_{j}(tf_{j,d})^2 }} $$
        *   $$ \frac{tf_{id} \cdot idf_{i}}{\sqrt{ \sum_{j}(tf_{j,d} \cdot idf_{j})^2 }} $$

**III. Assunzioni e Principi Chiave di Tf-Idf**

   *   **A. Assunzioni:**
        *   Collezione di documenti omogenea (dominio fissato, lessico comune).
        *   Pattern di frequenza dei termini simili tra i documenti.
   *   **B. Principi Chiave:**
        *   Peso variabile: Il peso di un termine varia a seconda del documento.
        *   Normalizzazione della lunghezza: Compensazione delle diverse lunghezze dei documenti.
        *   Smoothing: Attenuazione dell'influenza dei termini rari.

**IV. Normalizzazione della Lunghezza**

   *   **A. Obiettivo:**
        *   Capire l'impatto della normalizzazione dei vettori di parole sulla rappresentazione dei topic.
   *   **B. Problema:**
        *   La normalizzazione (es. L2) può influenzare la rappresentazione dei topic.
   *   **C. Considerazioni:**
        *   Diluisce il segnale informativo: Soprattutto con testi lunghi.
        *   Proprietà geometriche: La normalizzazione L2 ha proprietà specifiche.
        *   Influenza sulla costruzione della matrice dei dati: Modifica la rappresentazione dei vettori di parole.
   *   **D. Esempio:**
        *   Senza normalizzazione, la distanza euclidea favorisce i documenti più lunghi.

**V. Documenti e Query come Vettori**

   *   **A. Spazio Vettoriale:**
        *   Spazio a |V| dimensioni (V = vocabolario).
        *   Termini = assi dello spazio.
        *   Documenti = punti/vettori nello spazio.
   *   **B. Caratteristiche dello Spazio:**
        *   Alta dimensionalità (decine di milioni di dimensioni).
        *   Molto sparso (la maggior parte delle voci è zero).
   *   **C. Query come Vettori:**
        *   Anche le query sono rappresentate come vettori.
   *   **D. Classificazione:**
        *   I documenti sono classificati in base alla loro prossimità alla query.
        *   Prossimità = similarità dei vettori.
        *   Prossimità ≈ inversa della distanza.

---

**Schema Riassuntivo: Prossimità nel Modello a Spazio Vettoriale**

**1. Introduzione: Scale-Invariance e Normalizzazione**
    *   La prossimità può essere misurata in modo scale-invariant, indipendente dalla lunghezza dei vettori.
    *   L'uso dell'angolo tra i vettori implica una normalizzazione implicita.

**2. Problemi della Distanza Euclidea**
    *   Sensibilità alla lunghezza dei vettori.
        *   La distanza euclidea è grande per vettori di lunghezze diverse, anche se la distribuzione dei termini è simile.
        *   Esempio: La distanza euclidea tra un documento e lo stesso concatenato con se stesso è elevata.
    *   Soffre l'alta dimensionalità più di altre misure (es. coseno).

**3. Normalizzazione Implicita con l'Angolo (Coseno)**
    *   Idea chiave: Classificare i documenti in base all'angolo con la query.
        *   Ordinare i documenti in ordine decrescente dell'angolo o crescente del coseno.
    *   Il coseno è una funzione monotona decrescente per l'intervallo [0°, 180°].
    *   Calcolo del coseno: prodotto scalare diviso per il prodotto delle norme.

**4. Normalizzazione dei Vettori**
    *   Dividere ogni componente del vettore per la sua lunghezza (norma).
    *   Conseguenze:
        *   Vettore Unitario: Si ottiene un vettore di lunghezza 1 (sulla superficie dell'ipersfera unitaria).
        *   Pesi Comparabili: Rende confrontabili documenti di diversa lunghezza.
    *   Similarità del Coseno: Esempio di metrica che utilizza la normalizzazione.
        *   Formula: $\text{sim}(d_1, d_2) = \frac{d_1 \cdot d_2}{\|d_1\| \cdot \|d_2\|} = \frac{\sum_{i=1}^{n} w_{i,j} \cdot w_{i,k}}{\sqrt{\sum_{i=1}^{n} w_{i,j}^2} \cdot \sqrt{\sum_{i=1}^{n} w_{i,k}^2}}$

**5. Algoritmo Cosine Score**
    *   Calcola la similarità tra query e documenti normalizzati.
    *   Passaggi:
        1.  Inizializza `Scores[N] = 0` e `Length[N]`.
        2.  Per ogni termine della query `t`:
            *   Calcola `w_t,q` e recupera la postings list per `t`.
            *   Per ogni coppia `(d, tf_(t,d), w_(t,q))` nella postings list:
                *   `Scores[d] += w_(t,d) × w_(t,q)`
        3.  Leggi l'array `Length`.
        4.  Per ogni documento `d`:
            *   `Scores[d] = Scores[d]/Length[d]`
        5.  Restituisci i primi K componenti di `Scores`.

**6. Varianti di Ponderazione Tf-Idf**
    *   Term Frequency (tf):
        *   n (natural): $tf_{r, d}$
        *   l (logarithm): $1 + \log(tf_{r, d})$
        *   a (augmented): $0.5 + \frac{0.5

---

Ecco lo schema riassuntivo del testo fornito:

**I. Ponderazione Tf-Idf**

   *   **A. Componenti principali:**
        *   **1. Term Frequency (tf):** Frequenza di un termine in un documento.
        *   **2. Document Frequency (df):** Numero di documenti in cui appare un termine.
        *   **3. Inverse Document Frequency (idf):** Misura dell'importanza di un termine.

   *   **B. Variazioni di Term Frequency (tf):**
        *   **1. n (natural):** $tf_{r, d}$
        *   **2. l (logarithm):** $1 + \log(tf_{r, d})$ (se $tf_{r, d} > 0$, altrimenti 0)
        *   **3. a (augmented):** $0.5 + 0.5 \cdot \frac{tf_{r, d}}{\max_{r} (tf_{r, d})}$
        *   **4. b (boolean):** $\begin{cases} 1 & \text{if } tf_{r, d} > 0 \\ 0 & \text{otherwise} \end{cases}$

   *   **C. Variazioni di Document Frequency (df):**
        *   **1. n (no):** $1$
        *   **2. t (idf):** $\log \frac{N}{df_r}$
        *   **3. p (prob idf):** $\max \{ 0, \log \frac{N - df_r}{df_r} \}$

   *   **D. Normalizzazione:**
        *   **1. n (none):** $1$
        *   **2. c (cosine):** $\frac{1}{\sqrt{w_1^2 + w_2^2 + \dots + w_n^2}}$
        *   **3. u (pivoted unique):** $\frac{1}{u}$
        *   **4. b (byte size):** $\frac{1}{\text{CharLength}^{\alpha}}, \alpha < 1$

   *   **E. Soluzioni di default:**
        *   Le seconde di ogni tipo (l, t, c).

   *   **F. Term frequency aumentata:**
        *   Utile in contesti di retrieval puro con query espanse.
        *   Simile allo smoothing nel retrieval probabilistico.

   *   **G. Ponderazioni diverse per query e documenti:**
        *   **Notazione SMART:** ddd.qqq (document.query)

**II. Esempio di Ponderazione lnc.ltc**

   *   **A. Documento:** tf logaritmico, nessun idf, normalizzazione del coseno.
   *   **B. Query:** tf logaritmico, idf, normalizzazione del coseno.
   *   **C. Esempio:**
        *   **Documento:** assicurazione auto assicurazione auto
        *   **Query:** migliore assicurazione auto
        *   **Calcolo del punteggio:** 0.8 (somma dei prodotti normalizzati tf-idf)

**III. Classifica dello Spazio Vettoriale**

   *   **A. Riepilogo:**
        *   Rappresentare documenti e query come vettori tf-idf ponderati.
        *   Calcolare la similarità del coseno tra query e documenti.
        *   Classificare i documenti in base al punteggio.
        *   Restituire i primi K risultati.

   *   **B. Pro:**
        *   Corrispondenza parziale e misura di punteggio/classifica.
        *   Funziona bene nella pratica.
        *   Implementazione efficiente.

   *   **C. Contro:**
        *   Informazioni sintattiche mancanti.
        *   Informazioni semantiche mancanti.
        *   Ipotesi di indipendenza dei termini (Bag of Words).
        *   Ipotesi di ortogonalità dei vettori dei termini.
        *   Manca il controllo di un modello booleano.

---
