
## Modello Booleano di Ricerca

Il modello booleano di ricerca utilizza query booleane per interrogare un indice di termini associati a documenti testuali.  Le query sono espressioni composte da termini di indice (parole chiave) e operatori booleani (`AND`, `OR`, `NOT`).  La corrispondenza è binaria: un documento soddisfa la query o no.  Questo modello, sebbene utilizzato in sistemi come sistemi di posta elettronica, cataloghi di biblioteche e macOS Spotlight (e storicamente in sistemi legali come WestLaw), presenta significative limitazioni.

**WestLaw**, ad esempio, pur gestendo enormi quantità di dati e utilizzando query booleane con operatori di prossimità (es. `/3` per "entro 3 parole", `/S` per "nella stessa frase"), dimostra sia i vantaggi che gli svantaggi del modello.  Le query possono essere precise e lunghe, permettendo un raffinamento incrementale, ma la rigidità del modello limita la flessibilità.

**Vantaggi:** Le query booleane permettono precisione e controllo, consentendo di specificare con accuratezza i criteri di ricerca e di utilizzare operatori di prossimità per definire relazioni spaziali tra i termini.

**Limitazioni:** Il modello è rigido, incoraggiando query brevi e semplici.  L'operatore `AND` richiede la presenza di tutti i termini, mentre `OR` richiede almeno uno.  Questa rigidità porta a:

* **Scarsa flessibilità:** Non gestisce sinonimi o termini semanticamente equivalenti. Ogni termine è considerato in isolamento.
* **Difficoltà nel controllo dei risultati:** Tutti i documenti corrispondenti vengono restituiti senza possibilità di ordinamento per rilevanza o di controllo sulla quantità.  Il feedback di rilevanza dell'utente è inefficace perché non c'è un ranking dei risultati.

Un esempio illustra le difficoltà: trovare opere di Shakespeare contenenti "Brutus" e "Caesar" ma non "Calpurnia" richiede un'elaborazione complessa e inefficiente, impossibile con un semplice approccio booleano.  Un'operazione più sofisticata, come trovare "Romans" vicino a "countrymen", è altrettanto problematica. 
Il modello booleano non supporta il recupero classificato (ranking dei risultati in base alla rilevanza).

---

Il modello booleano per il recupero dell'informazione, pur semplice, presenta limitazioni nell'espressività, soprattutto con query complesse e a causa della sparsità dei dati.  È efficace solo per query brevi e semplici, determinando solo presenza/assenza di elementi.

Per migliorarlo, si possono incorporare metodi di classificazione.  Assegnando pesi (in [0,1]) ai termini nei documenti e nelle query, si può calcolare la somiglianza (es. Jaccard) tra query e documenti, ordinando i risultati in base a questa. Il *feedback di rilevanza* permette di raffinare ulteriormente l'ordinamento.

Un'altra estensione utilizza gli *insiemi fuzzy*, rilassando i confini degli insiemi booleani.  Il grado di appartenenza *wA* di un elemento ad un insieme A permette di definire intersezione ($w(A∩B) = min(wA, wB)$) e unione ($w(A∪B )= max(wA, wB)$) in modo più flessibile.


Il modello MMM (Mixed Min and Max) calcola la similarità $S$ tra una query e un documento usando una combinazione lineare del massimo e del minimo dei pesi dei termini del documento. 
- Sia $d$ il documento con pesi dei termini $w_1, w_2, \dots, w_n$ corrispondenti ai termini $t_1, t_2, \dots, t_n$.

Per una query disgiuntiva $q_{or} = (t_1 \lor t_2 \lor \dots \lor t_n)$, la similarità è definita come:  $S(q_{or}, d) = \lambda_{or} \cdot \max(w_1, \dots, w_n) + (1 - \lambda_{or}) \cdot \min(w_1, \dots, w_n)$.  Il parametro $\lambda_{or}$ ($0 \le \lambda_{or} \le 1$) controlla il peso relativo del massimo e del minimo; in logica booleana standard, $\lambda_{or} = 1$. Se la query è puramente disgiuntiva, solo il peso del primo termine viene considerato.

Per una query congiuntiva $q_{and} = (t_1 \land t_2 \land \dots \land t_n)$, la similarità è: $S(q_{and}, d) = \lambda_{and} \cdot \min(w_1, \dots, w_n) + (1 - \lambda_{and}) \cdot \max(w_1, \dots, w_n)$. Analogamente, $\lambda_{and}$ ($0 \le \lambda_{and} \le 1$) bilancia il peso del minimo e del massimo, assumendo il valore 1 nella logica booleana standard.


Il *modello di Paice*, a differenza di MMM, considera tutti i pesi dei termini, migliorando la risposta ma con un costo computazionale maggiore.

Il *modello P-norma* rappresenta documenti e query come punti multidimensionali, considerando pesi dei termini e operatori con coefficienti che indicano il grado di rigore.

Infine, la sparsità della rappresentazione matrice termine-documento è un problema significativo.  Un esempio con 1 milione di documenti da 1000 parole ciascuno richiede 6 GB di dati, evidenziando la sfida nella gestione di grandi quantità di informazioni.

---

L'indicizzazione di grandi corpora di documenti, con milioni di termini e documenti, presenta sfide significative a causa della sparsità delle matrici termine-documento.  Una matrice di incidenza termine-documento di dimensioni 500K x 1M sarebbe estremamente inefficiente.  Una soluzione ottimale è l'utilizzo di un **indice inverso**, che mappa ogni termine all'insieme dei documenti in cui appare.

L'indice inverso offre vantaggi significativi in termini di efficienza di memorizzazione e velocità di ricerca, gestendo efficacemente la sparsità dei dati.  È costruito identificando, per ogni termine, i documenti in cui compare e memorizzando i corrispondenti docID.  La struttura dati più comune è la **lista di postings**, un elenco di docID per ogni termine, spesso implementata con liste concatenate o array di lunghezza variabile, a seconda delle esigenze di performance e di aggiornamento.  La memorizzazione può avvenire su disco (sequenza continua di postings) o in memoria.

Il core dell'indice inverso è composto da un **dizionario**, che mappa ogni termine alla sua lista di postings, e dalle liste di postings stesse, contenenti i docID e, spesso, la frequenza del termine in ciascun documento.  Questo permette un accesso rapido sia per termine (attraverso il dizionario) che per documento (attraverso le liste di postings). ![[1) Intro-20241003104139467.png|393]]

Il processo di indicizzazione (indexing) prevede quattro fasi: 1) generazione di una sequenza di coppie (termine, docID); 2) ordinamento per termine; 3) raggruppamento e ordinamento per docID delle coppie con lo stesso termine; 4) (implicito) creazione della struttura dati dell'indice inverso (dizionario e liste di postings).

---

Questo documento descrive l'elaborazione di query booleane nei motori di ricerca, focalizzandosi sull'ottimizzazione dell'efficienza.

**Indici Invertiti:** Il sistema utilizza indici invertiti, strutturati in un dizionario (termini e loro `termID`) e in liste di *postings* (documenti contenenti ciascun termine, con le rispettive frequenze).  I termini ripetuti nello stesso documento vengono uniti, mantenendo la frequenza totale.

**Elaborazione di Query Booleane:** L'elaborazione di query come "Brutus AND Caesar" avviene intersecando le liste di *postings* dei singoli termini.  L'algoritmo `INTERSECT` (fornito nel codice) esegue questa intersezione in tempo lineare rispetto alla somma delle dimensioni delle liste, grazie all'ordinamento delle liste stesse.  ![[]]

**Ottimizzazione delle Query:** Per query con più termini congiunti da `AND`, l'ordine di elaborazione influenza significativamente l'efficienza.  La strategia ottimale consiste nell'elaborare i termini in ordine crescente di frequenza documentale, iniziando dal termine meno frequente. Questo minimizza la dimensione degli insiemi intermedi durante le intersezioni.  Per query booleane arbitrarie (incluse `OR` e `NOT`), si stima la dimensione degli insiemi `OR` (come somma delle frequenze dei termini) e si procede in ordine crescente di queste stime.  Il tempo di elaborazione rimane lineare rispetto al numero totale di voci nei *postings*.

**Query di Frase:**  Le query di frase ("Dipartimenti Unical") richiedono un approccio diverso dalla semplice ricerca di termini individuali.  Un metodo efficace è l'utilizzo di *indici biword*, che indicizzano ogni coppia consecutiva di termini nel testo, trattandola come una singola unità. Questo permette di individuare le occorrenze esatte della frase nella query.
```
INTERSECT(p1, p2)
1 answer ← ⟨ ⟩
2 while p1 ≠ NIL and p2 ≠ NIL
3 do if docID(p1) = docID(p2)
4 then ADD(answer, docID(p1))
5 p1 ← next(p1)
6 p2 ← next(p2)
7 else if docID(p1) < docID(p2)
8 then p1 ← next(p1)
9 else p2 ← next(p2)
10 return answer
```

---

## Riassunto delle Tecniche di Indicizzazione per il Recupero dell'Informazione

Questo documento descrive diverse tecniche di indicizzazione per il recupero dell'informazione, confrontando i loro vantaggi e svantaggi.

### Indici Biword

Gli indici biword creano voci di dizionario per ogni coppia di parole consecutive in un testo (es. "amici romani", "romani concittadini").  L'elaborazione di query a due parole è efficiente, ma l'estensione a frasi più lunghe richiede query booleane complesse (es. "Corsi del dipartimento DIMES" -> "dipartimento DIMES AND corsi dipartimento"), portando a falsi positivi e ad un indice di dimensioni eccessive.  Non sono quindi una soluzione standard, ma possono essere parte di una strategia più ampia.

### Etichettatura Morfologica (POST) e Biword Estensi

Questa tecnica migliora gli indici biword.  Attraverso l'analisi POST (Part-of-Speech tagging), vengono identificate sequenze di termini della forma  $NX*N$ (dove N è un nome e X un articolo o preposizione), creando "biword estesi" (es. "il cacciatore nella segale" -> "cacciatore segale").  Questo permette di gestire frasi più complesse rispetto agli indici biword semplici.

### Indici Posizionali

Gli indici posizionali memorizzano, per ogni termine, le posizioni di ogni occorrenza nei documenti (es. `<termine, numero di documenti; doc1: posizione1, posizione2 … ; doc2: posizione1, posizione2 … ; …>`).  Questo permette di supportare query di frase e di prossimità, non gestibili dagli indici biword.  L'algoritmo di unione ricorsivo è utilizzato per processare le query di frase, gestendo la complessità delle posizioni.  Sebbene richiedano maggiore spazio di archiviazione (2-4 volte un indice non posizionale, 35-50% del volume del testo originale), sono lo standard per la loro efficacia.

### Costruzione dell'Indice basata sull'Ordinamento

La costruzione dell'indice basata sull'ordinamento analizza i documenti uno alla volta.  Questo approccio, sebbene semplice, richiede molto spazio per collezioni di grandi dimensioni perché i postings per ogni termine sono incompleti fino alla fine del processo.

---

Il testo descrive la costruzione di indici invertiti per sistemi di Information Retrieval (IR) a grande scala, affrontando le sfide poste dalla dimensione dei dati.  La costruzione in memoria, pur fattibile per indici relativamente piccoli (es. 100 milioni di postings), non è scalabile per collezioni di dimensioni reali come quella del New York Times (150 anni di notizie).  Questo richiede l'utilizzo del disco, con conseguenti vincoli di performance dovuti all'accesso ai dati.

I sistemi IR moderni sfruttano server con ampia memoria (GB) e spazio su disco molto maggiore (2-3 ordini di grandezza).  La tolleranza ai guasti è costosa, quindi si preferisce l'utilizzo di molte macchine standard. L'accesso alla memoria è significativamente più veloce di quello al disco, e l'I/O del disco è ottimizzato leggendo e scrivendo blocchi di dati (8KB-256KB), minimizzando il numero di accessi.

L'ordinamento diretto su disco di grandi quantità di dati (es. 100 milioni di record) è inefficiente.  L'algoritmo BSBI (Block Sort-Based Indexing) risolve questo problema con un approccio basato sull'ordinamento esterno.  BSBI suddivide i dati in blocchi più piccoli (es. 10 milioni di record), che vengono ordinati in memoria (es. con Quicksort) e scritti su disco.  Successivamente, questi blocchi ordinati (run) vengono uniti in un unico indice ordinato tramite un algoritmo di merge esterno.

Il codice seguente illustra la struttura di BSBI:

```
BSBIndexConstruction()
1 n ← 0
2 while (all documents have not been processed)
3 do n ← n + 1
4 block ← ParseNextBlock()
5 BSBI-Invert(block) //costruisce inverted index
6 WriteBlockToDisk(block, fn) //scriviamo sul disco, presuppone la ù presenza di una struttura di blocco sul disco
7 MergeBlocks(f_1, ..., f_n; f_merged)
```

L'ordinamento di 10 blocchi da 10 milioni di record ciascuno avviene in due fasi: ordinamento interno di ogni blocco e successiva fusione delle run ordinate.  L'unione delle run ordinate può essere effettuata tramite merge binario o multi-way merge, con l'obiettivo di ottimizzare l'utilizzo dello spazio su disco.

---

Questo documento descrive tecniche di merge e l'algoritmo SPIMI per la costruzione di indici invertiti su grandi collezioni di documenti.

**Tecniche di Merge:**  Il merge binario costruisce un indice invertito attraverso un albero di merge con $\log_2(n)$ livelli (dove *n* è il numero di run), effettuando merge parziali a ogni livello.  Ogni livello prevede partizionamento dei dati, merge in memoria di ogni partizione generando un nuovo indice, e aggiornamento dell'indice per il livello successivo.  `![[]]`.  Il multi-way merge, più efficiente, legge simultaneamente da tutti i blocchi usando un buffer per ogni blocco e un buffer di scrittura, impiegando una coda di priorità per selezionare il `termID` più basso e unire le liste di postings corrispondenti.  L'efficienza dipende dalle dimensioni dei blocchi letti e scritti. Si assume una condivisione solo parziale del lessico tra i documenti.  La gestione del lessico durante il merge richiede una corretta gestione di termini e ID, e strategie per la compressione con perdita (solo per termini non essenziali).

**SPIMI (Single-Pass In-Memory Indexing):**  Questo algoritmo risolve il problema della crescita del lessico durante l'indicizzazione di grandi collezioni.  Supera le limitazioni degli algoritmi basati sull'ordinamento, che potrebbero non riuscire a mantenere in memoria la mappatura (termine, termID) e il dizionario in crescita.  SPIMI si basa su due idee chiave: 1) generare dizionari separati per ogni blocco, evitando la mappatura tra blocchi; 2) non ordinare i postings, accumulandoli nelle liste man mano che si verificano.  Questo genera indici invertiti completi per ogni blocco, successivamente mergeabili in un unico indice.  SPIMI è più veloce e risparmia memoria perché evita l'ordinamento e la memorizzazione dei `termID` intermedi.  Le liste di postings sono strutture dinamiche riallocate all'occorrenza.

**SPIML-Invert (Pseudocodice):**

```python
1. output_file = NEWFILE()
2. dictionary = NEWHASH()
3. while (free memory available)
4. do token ← next(token_stream)
5. if term(token) ∉ dictionary
6. then postings_list = ADDToDICTIONARY(dictionary, term(token))
7. else postings_list = GETPOSTINGSLIST(dictionary, term(token))
8. if full(postings_list)
9. then postings_list = DOUBLEPOSTINGSLIST(dictionary, term(token))
10. ADDToPOSTINGSLIST(postings_list, docID(token))
11. sorted_terms ← SORTTERMS(dictionary)
12. WRITEBLOCKToDISK(sorted_terms, dictionary, output_file)
13.
```

L'algoritmo itera finché c'è memoria disponibile, aggiungendo i token al dizionario e alle liste di postings.  Quando una lista è piena, viene raddoppiata.  Infine, il dizionario ordinato e le liste di postings vengono scritti su disco.

---

## Indicizzazione Distribuita e MapReduce

Questo documento descrive l'indicizzazione distribuita di documenti, presentandola come un'applicazione del modello MapReduce.  L'indicizzazione è suddivisa in due fasi principali, **parsing** e **inversione dell'indice**, eseguite in parallelo da diversi nodi coordinati da una macchina master.

### Parsing Distribuito

Il master assegna porzioni di documenti (split) a macchine parser inattive. Ogni parser legge i documenti, estrae le coppie (termine, documento ID) e le scrive in *j* partizioni (fold) basate su una suddivisione lessicografica dei termini (es. a-f, g-p, q-z per *j*=3).

### Inversione Distribuita dell'Indice

Ogni inverter raccoglie le coppie (termine, documento ID) per una specifica partizione, ordina le liste di occorrenze (postings) per termine e le scrive su disco. Questo processo è illustrato in `![1) Intro-20241003093756427.png]` e `![1) Intro-20241003093804693.png]`, che mostrano come l'intero processo sia un'istanza di MapReduce.

* **Fase Map:** Genera liste di coppie (termine, documento ID) a partire dalla collezione di documenti.
* **Fase Reduce:** Prende le liste di coppie (termine, documento ID) e genera le liste di postings (liste di documenti per ogni termine).

### MapReduce e Indicizzazione

MapReduce è un framework per il calcolo distribuito che semplifica l'implementazione di algoritmi complessi.  L'indicizzazione descritta è un esempio di applicazione di MapReduce, dove:

* `map`: input (collezione di documenti) → list(termID, docID)
* `reduce`: (k,list(v))  → output (liste di postings)


### Gestione di Documenti Dinamici

Per gestire l'arrivo, l'eliminazione e la modifica di documenti, si possono adottare diverse strategie:

* **Indice Principale e Ausiliario:**  Mantenere un indice principale e uno o più indici ausiliari per i nuovi documenti. La ricerca avviene su entrambi, combinando i risultati.
* **Eliminazione:** Utilizzare un vettore di bit per indicare i documenti eliminati, filtrando i risultati di ricerca.
* **Re-indicizzazione:** Periodicamente, re-indicizzare tutto in un unico indice principale.  L'aggiornamento di termini esistenti richiede modifiche alle liste di postings, mentre i nuovi termini vanno aggiunti al dizionario.

---

Questo documento tratta le sfide dell'indicizzazione dinamica nei motori di ricerca, focalizzandosi sulle inefficienze della fusione di indici e sulle soluzioni proposte.

**Problematiche della Fusione di Indici:** La semplice fusione di un indice ausiliario in uno principale, sebbene efficiente con file separati per ogni lista di postings (simile ad una semplice append), diventa inefficiente con un unico grande file per l'indice, come spesso accade nella pratica.  Questo porta a problemi di performance durante i merge frequenti.

**Fusione Logaritmica:**  Questa tecnica utilizza una serie di indici di dimensioni crescenti (1, 2, 4, 8...). L'indice più piccolo risiede in memoria, mentre gli altri sono sul disco. Quando l'indice in memoria supera una certa dimensione, viene scritto sul disco e fuso con l'indice successivo. Questo processo iterativo riduce la complessità della fusione da  `O(T²/n)` (dove `T` è il numero di postings e `n` la dimensione dell'indice ausiliario) a `O(T * log(T/n))`.  Mentre migliora la costruzione dell'indice, aumenta la complessità delle query da `O(1)` a `O(log(T/n))`.  

**Indicizzazione Dinamica:** L'utilizzo di un indice ausiliario e uno principale comporta complessità nella gestione delle statistiche a livello di collezione (es. correzione ortografica). Una possibile soluzione è ignorare l'indice ausiliario per l'ordinamento, basando il ranking solo sull'indice principale.

**Indicizzazione Dinamica nei Motori di Ricerca:** I motori di ricerca gestiscono l'indicizzazione dinamica tramite modifiche incrementali frequenti e ricostruzioni periodiche dell'indice da zero, passando le query al nuovo indice una volta completato.

**Requisiti per la Ricerca in Tempo Reale:** La ricerca in tempo reale richiede bassa latenza, elevato tasso di ingestione, gestione di letture e scritture concorrenti e priorità ai dati più recenti (dominanza del segnale temporale).

---

## Costruzione dell'Indice: Riepilogo

L'indicizzazione basata sull'ordinamento prevede diverse tecniche, tra cui l'inversione in memoria naive e l'indicizzazione basata sull'ordinamento bloccato (BSBI).  L'ordinamento per fusione risulta particolarmente efficiente per l'ordinamento su disco rigido, evitando costose operazioni di ricerca.

Un'alternativa è l'indicizzazione in memoria a passaggio singolo (SPIMI), che differisce dalle precedenti per alcuni aspetti chiave: non utilizza un dizionario globale, ma genera dizionari separati per ogni blocco di dati; non ordina le liste di *postings* durante la fase di creazione, ma le accumula direttamente nell'ordine di apparizione.

---
