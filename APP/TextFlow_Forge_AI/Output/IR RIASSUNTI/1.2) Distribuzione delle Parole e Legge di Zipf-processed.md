
## Compressione dei Dati in Information Retrieval

La compressione dei dati è fondamentale in Information Retrieval per ridurre lo spazio di archiviazione, aumentare la capacità di memoria, accelerare il trasferimento dati e migliorare le prestazioni di lettura.  Gli algoritmi di decompressione sono generalmente molto veloci.  Si distinguono la compressione del dizionario, cruciale per mantenere in memoria il dizionario e alcune liste di postings, e la compressione dei file di postings, importante per ridurre lo spazio su disco e accelerare la lettura.  Si usa prevalentemente la compressione *senza perdita*, mentre la compressione *con perdita* (es. pre-elaborazione del testo, poda delle liste di postings) è utilizzata con cautela, introducendo una minima perdita di qualità.

## Dimensione del Vocabolario

La dimensione del vocabolario (M, numero di parole distinte) cresce con la dimensione della collezione (T, numero di token).  Non esiste un limite superiore definito, soprattutto con Unicode.  La Legge di Heaps, una relazione empirica, modella questa crescita con una *power law*:  `M = kT^b`, dove `k` e `b` sono costanti (tipicamente `30 ≤ k ≤ 100` e `b ≈ 0.5`).  In un grafico log-log, questa legge si rappresenta come una linea retta con pendenza `b`.  L'applicazione della Legge di Heaps a Reuters RCV1, con `log₁₀M = 0.49 log₁₀T + 1.64`, mostra una buona aderenza empirica (es. per i primi 1.000.020 token, prevede 38.323 termini, contro i 38.365 reali). ![[1) Intro-20241007160038284.png|371]]

---

Le distribuzioni di tipo power-law descrivono fenomeni naturali e sociali dove pochi elementi hanno valori molto alti, mentre molti ne hanno valori bassi.  Questo è caratterizzato da una concentrazione di massa in una piccola porzione della distribuzione, seguita da una lunga coda.  Un esempio chiave è la **legge di Pareto (80/20)**, dove il 20% delle cause genera l'80% degli effetti.

La **legge di Zipf**, un esempio di distribuzione power-law nel linguaggio naturale, mostra la relazione tra la frequenza di una parola e il suo rango nel vocabolario: le parole più frequenti hanno un rango basso.  La **legge di Heaps** stima la dimensione del vocabolario di un corpus testuale.  Entrambe sono considerate leggi di potenza.

Le distribuzioni power-law differiscono dalle distribuzioni di **Poisson**, che descrivono la probabilità di un certo numero di eventi in un dato intervallo.  Mentre entrambe sono asimmetriche, la power-law modella la relazione tra due variabili con una proporzionalità di potenza, mentre la Poisson modella eventi rari in uno spazio o tempo.

Esempi di distribuzioni power-law includono la distribuzione della ricchezza, il numero di pagine web, i follower sui social media, la dimensione delle città e la frequenza delle parole in un testo.  Inizialmente, vi è una fase transitoria, poi la distribuzione si stabilizza a regime.

---

La legge di Zipf descrive la distribuzione delle frequenze delle parole in un corpus testuale: la frequenza del termine *i*-esimo è proporzionale a $\frac{1}{i}$, ovvero $cf_{i} \propto \frac{K}{i}$, dove $K$ è una costante.  In forma logaritmica,  $log(cf_{i}) = log(K) - log(i)$, rappresentando una relazione lineare inversa (legge di potenza) tra frequenza e rango del termine.  ![[]]  Questo significa che poche parole sono molto frequenti, mentre la maggior parte delle parole ha una frequenza bassa o media. 

Luhn (1958) osservò che termini molto frequenti (es. articoli, preposizioni) o molto rari sono poco utili per l'indicizzazione.  I termini più discriminanti hanno una frequenza intermedia.  La distribuzione di Zipf implica la necessità di un *pruning* del vocabolario, eliminando termini troppo frequenti o troppo rari per migliorare efficienza e accuratezza.

Determinare le frequenze di taglio ottimali per questo pruning è complesso, dipendente dal task specifico e dal dominio dei dati (linguaggio e corpus).  Non esiste una regola universale. L'obiettivo è mantenere solo i termini con frequenza intermedia,  che sono più informativi per la caratterizzazione del contenuto.

---

## Preprocessing del Testo e Ponderazione della Rilevanza

L'elaborazione del testo per la ricerca di informazioni richiede la definizione di regole pratiche per la selezione dei termini.  Si consiglia di rimuovere termini troppo frequenti (presenti in oltre il 50% dei documenti) e termini troppo rari (presenti in meno di 3-5 documenti).  Un approccio conservativo, che mantiene un vocabolario ampio, è preferibile, soprattutto per l' *Information Retrieval* e il *Data Mining tradizionale*, dove la frequenza dei termini è fondamentale.

## Dal Recupero Booleano al Recupero Classificato

La ricerca booleana, basata su operatori logici, presenta limiti: restituisce spesso troppi o troppo pochi risultati,  è adatta solo ad utenti esperti e non è user-friendly per la maggior parte degli utenti, soprattutto nel contesto del web.

Il **recupero classificato (ranked retrieval)** risolve questi problemi restituendo i documenti ordinati per pertinenza alla query.  A differenza dell'estrazione di informazioni, il recupero classificato si concentra sulla presenza di contenuti corrispondenti alla query nel documento, senza analizzare la complessità strutturale del documento stesso.

Questo approccio è tipicamente associato alle **query di testo libero**, ovvero sequenze di parole in linguaggio naturale, evitando il problema dell' "abbondanza o carestia" di risultati.  Il sistema presenta solo i primi *k* risultati (circa 10), evitando di sovraccaricare l'utente e accettando la potenziale imprecisione insita nelle query di testo libero.

## Punteggio di Pertinenza

Il recupero classificato si basa sull'assegnazione di un punteggio di pertinenza (tra 0 e 1) a ciascun documento, misurando la corrispondenza tra il documento e la query.  Questo punteggio determina l'ordine di presentazione dei risultati, mostrando prima i documenti più probabilmente utili all'utente.

---

Il testo descrive diverse misure di similarità per insiemi finiti, utilizzate per confrontare documenti o query in Information Retrieval.  Queste misure superano le limitazioni di metodi semplici, normalizzando per la lunghezza dei documenti e delle query, a differenza di metodi che considerano solo la presenza o assenza di termini.

Le misure descritte sono:

* **Jaccard:**  $J(A,B) = \frac{\|A \cap B\|}{\|A \cup B\|}$. Misura la similarità come rapporto tra l'intersezione e l'unione di due insiemi. Varia tra 0 e 1.

* **Sørensen-Dice:** $DSC(A,B) = \frac{2 \times \|A \cap B\|}{\|A\| + \|B\|}$. Simile a Jaccard, ma dà doppio peso all'intersezione. Varia tra 0 e 1.

* **Overlap:** $O(A,B) = \frac{\|A \cap B\|}{min(\|A\|, \|B\|)}$. Misura la sovrapposizione come rapporto tra l'intersezione e la cardinalità dell'insieme più piccolo. Varia tra 0 e 1.

* **Simple Matching:** $SM(A,B) = \frac{\|A \cap B\| + \|\overline{A} \cap \overline{B}\|}{\|A \cup B\|}$. Considera sia le presenze che le assenze di elementi. Varia tra 0 e 1.

Il testo sottolinea che, mentre termini come "misure" o "criteri" sono usati genericamente,  "metrica" si riferisce a misure che soddisfano la disuguaglianza triangolare.  Dice, ad esempio, non soddisfa questa proprietà, come dimostrato da un esempio con tre documenti (A, AB, B), dove $Dice(1,2) + Dice(1,3) \geq Dice(2,3)$ non è rispettata.  Infine, viene definito l'Overlap Coefficient (o Simpson) come il rapporto tra l'intersezione e la cardinalità minore dei due insiemi.

---

Il testo descrive metodi per rappresentare testi numericamente, focalizzandosi sul calcolo del peso di un termine in un documento.  Vengono introdotte tre metriche chiave:

* **Term Frequency (tf):**  indica quante volte un termine appare in un singolo documento.
* **Collection Frequency (cf):** indica quante volte un termine appare nell'intera collezione di documenti.
* **Document Frequency (df):** indica quanti documenti contengono un determinato termine.

Inoltre, viene definita una metrica per calcolare l'overlap tra due insiemi di parole:

$$Overlap(X,Y) = \frac{{|X ∩ Y|}}{{min(|X|, |Y|)}}$$

Questa formula è utile, ad esempio, per confrontare due riassunti.

Il testo propone due approcci per calcolare il peso ($w_{t,d}$) di un termine (*t*) in un documento (*d*):

1.  Utilizzando direttamente la *term frequency* e la *collection frequency*:

    $$w_{t,d}=tf_{t,d} \frac{1}{cf_{t}}$$

    dove  `tf<sub>t,d</sub>` è la frequenza del termine *t* nel documento *d*, e `cf<sub>t</sub>` è la frequenza del termine *t* nell'intera collezione.  La normalizzazione dei valori non viene considerata in questo primo approccio.

2. Un secondo approccio non viene descritto completamente nel testo fornito.

Infine, viene presentata una tabella di esempio che mostra la frequenza di alcuni termini in diverse opere di Shakespeare, illustrando concettualmente l'applicazione delle metriche descritte.

---

Questo documento analizza diverse strategie per definire la rilevanza di un termine in un documento, considerando la *term frequency* (tf), la *collection frequency* (cf) e la legge di Zipf.

L'approccio iniziale di utilizzare due funzioni separate,  `w_{t,d}^{(t)}=f(tf_{t,d})+g(cf_{t})=tf_{t,d}+ly(cf_{t})`, si rivela inadeguato perché il termine legato alla `cf` domina eccessivamente.  La semplice smorzatura lineare inversa di `tf` con `cf` appiattisce troppo i valori.

La rilevanza di un termine è quindi definita da una funzione: `Rilevanza(termine) = F(TF) + G(CF)`, dove il peso relativo di `F` e `G` determina l'importanza di `tf` e `cf`.  La lunghezza dei documenti non viene considerata.

Due proposte per la funzione di rilevanza vengono analizzate in relazione alla legge di Zipf, considerando termini con frequenze medio-alte e medio-basse.

**Proposta 1:**  Utilizza una divisione tra `tf` e `cf`.

**Proposta 2:** Somma `tf` al logaritmo di `cf`.

Entrambe le proposte presentano problemi:  nel caso di termini molto frequenti (testa della distribuzione), la `cf` domina, mentre per termini meno frequenti, la Proposta 1 smorza eccessivamente `tf`, mentre la Proposta 2 la enfatizza.  Nessuna delle due risulta efficace in modo consistente.

Infine, il documento solleva la questione se la *document frequency* (df) potrebbe essere un indicatore più discriminante della `cf`, data la sua potenziale distribuzione più piatta.

---

Il documento descrive la funzione TF-IDF (Term Frequency-Inverse Document Frequency), una metrica per valutare l'importanza di un termine in un documento rispetto a un corpus.  L'obiettivo è superare i limiti di una semplice combinazione lineare delle frequenze, che può essere dominata da termini molto frequenti.

Una soluzione per smorzare l'influenza dei termini frequenti è usare la formula  `1/log(document frequency)`.  Se un termine appare in quasi tutti i documenti (alta *document frequency*), il suo peso sarà basso, riflettendo la sua scarsa capacità discriminante.  Al contrario, termini rari avranno un peso maggiore.

La funzione TF-IDF combina la *term frequency* (tf) con l' *inverse document frequency* (IDF). La formula è:

$$w_{t,d}=\log(1+tf_{t,d}) \times \log_{10}\left( \frac{N}{df_{t}} \right)$$

dove:

* `tf<sub>t,d</sub>`: frequenza del termine *t* nel documento *d*.
* `N`: numero totale di documenti.
* `df<sub>t</sub>`: numero di documenti contenenti il termine *t*.

Il logaritmo in `log(1+tf<sub>t,d</sub>)` smorza l'influenza di termini molto frequenti in un singolo documento.  `log<sub>10</sub>(N/df<sub>t</sub>)` rappresenta l'IDF: un valore alto indica un termine raro e informativo, mentre un valore basso indica un termine comune e poco informativo.

TF-IDF penalizza i termini comuni (stop words) ed evidenzia quelli rari, bilanciando la frequenza locale (nel documento) e globale (nel corpus).  Per migliorare l'accuratezza, si consiglia di rimuovere le stop words, applicare stemming o lemmatization e, eventualmente, impostare soglie per escludere termini con frequenze troppo alte o basse.

---

## Riassunto: Smorzamento, TF-IDF e Bag-of-Words

Questo documento tratta il modello Bag-of-Words (BoW) e il calcolo della similarità tra documenti tramite la metrica TF-IDF, includendo tecniche di smorzamento per gestire la distribuzione dei termini secondo la legge di Zipf.

### Smorzamento e Legge di Zipf

La legge di Zipf descrive la distribuzione delle parole in un corpus, dove la frequenza di una parola è inversamente proporzionale al suo rango. Lo smorzamento logaritmico nella formula TF-IDF mitiga l'influenza dei termini molto frequenti, aumentando il peso di quelli meno frequenti.  Questo evita soglie arbitrarie e permette di lavorare con matrici sparse. Per corpus molto grandi, si può applicare un doppio logaritmo per uno smorzamento più accentuato. La normalizzazione dei vettori TF-IDF è cruciale per il calcolo della similarità, utilizzato in compiti come il recupero di informazioni e la classificazione di documenti.

### Modello Bag-of-Words (BoW)

Il modello BoW ignora l'ordine delle parole, considerando solo la frequenza di ogni termine in un documento.  Questo semplifica il calcolo ma presenta limiti: perdita di informazioni sintattiche e semantiche, mancanza di controllo booleano e potenziale penalizzazione di documenti con termini della query presenti ma con bassa frequenza.  Nonostante ciò, BoW offre una corrispondenza parziale, punteggi graduati (non booleani), efficienza computazionale e rappresentazione dei documenti e query come vettori nello spazio vettoriale.

### Calcolo della Frequenza del Termine (TF)

La frequenza grezza del termine non è un indicatore ottimale di rilevanza.  Un peso di frequenza logaritmica è più appropriato:

$$w_{t,d} \begin{cases} 1+\log_{10}\text{tf}_{td} \ \text{ if tf}_{td} \ >0 \\ 0,\ \text{otherwise} \end{cases}$$

Il punteggio di similarità tra un documento e una query è la somma dei pesi dei termini comuni ad entrambi:

$$\sum_{t\in q \cap d}(1+\log(tf_{t,d}))$$

### Frequenza Inversa del Documento (IDF)

La frequenza inversa del documento (IDF) considera che i termini rari sono più informativi.  Un termine raro in un corpus, presente in un documento, indica alta rilevanza.  Il concetto è illustrato con l'esempio del termine "arachnocentrico".  La frequenza di collezione (cf) è un elemento chiave nel calcolo dell'IDF, ma non viene dettagliato nel testo fornito.

---

## Riassunto del metodo TF-IDF

Il metodo TF-IDF (Term Frequency-Inverse Document Frequency) assegna un peso ai termini di un documento basandosi sulla loro frequenza nel documento stesso (TF) e sulla loro rarità nell'intero corpus (IDF).  Questo peso riflette la rilevanza del termine per quel documento specifico, considerando la distribuzione di probabilità dei termini (power-law), che favorisce i termini rari.

### Frequenza del Documento (DF) e Frequenza Inversa del Documento (IDF)

La *document frequency* (df) conta le occorrenze di un termine nell'intero corpus.  Si preferisce alla *collection frequency* (cf) perché fornisce una migliore discriminazione tra documenti.  L' *inverse document frequency* (idf) è una misura inversa dell'informatività di un termine, calcolata come:

$$idf_{t}=\log_{10}\left( \frac{N}{df_{t}} \right)$$

dove *N* è il numero totale di documenti. Il logaritmo smorza l'effetto dell'idf.

### Calcolo del peso TF-IDF

Il peso TF-IDF di un termine *t* in un documento *d* è dato da:

$$w_{t,d}=\log(1+tf_{t,d})\times\log_{10}\left( \frac{N}{df_{t}} \right)$$

dove  `tf<sub>t,d</sub>` è la frequenza del termine *t* nel documento *d*.  Il peso aumenta con il numero di occorrenze del termine nel documento e con la rarità del termine nel corpus. Il punteggio di rilevanza documento-query è la somma dei pesi TF-IDF dei termini comuni a entrambi.

### Varianti del TF-IDF

Esistono diverse varianti del TF-IDF, che differiscono nel calcolo di TF (con o senza logaritmi) e nella ponderazione dei termini nella query (ponderati o non ponderati).  Esempi di varianti per il calcolo di TF sono:

$$ \frac{tf_{i,d}}{\max_{j}tf_{j,d}} ,\ \frac{tf_{id}}{\sqrt{ \sum_{j}(tf_{j,d})^2 }} ,\ \frac{tf_{id} \cdot idf_{i}}{\sqrt{ \sum_{j}(tf_{j,d} \cdot idf_{j})^2 }} $$

### Assunzioni e Principi chiave

Il TF-IDF assume un corpus omogeneo in termini di dominio e pattern di frequenza dei termini simili tra i documenti.  I principi chiave sono il peso variabile di uno stesso termine a seconda del documento e la necessità implicita di una normalizzazione della lunghezza dei documenti, dato che questi possono avere dimensioni diverse.

---

La rappresentazione di documenti e query come vettori in uno spazio ad alta dimensionalità, usando matrici di peso come Tf-Idf, presenta sfide legate alla normalizzazione.  La normalizzazione, in particolare la divisione per la norma L2, è cruciale per mitigare l'influenza della lunghezza del documento sulla similarità.

La normalizzazione L2, pur offrendo vantaggi come la scale-invariance, può diluire il segnale informativo, soprattutto confrontando documenti di lunghezza diversa.  La distanza euclidea, sensibile alla lunghezza dei vettori, non è ideale per questo tipo di confronto, poiché favorisce i documenti più lunghi indipendentemente dal contenuto.  Un documento concatenato a se stesso, ad esempio, risulterebbe più simile a se stesso che a una versione più corta, nonostante il contenuto sia identico.

Un approccio alternativo, che introduce una normalizzazione implicita, consiste nell'utilizzare la similarità del coseno tra i vettori.  Il coseno dell'angolo tra i vettori query e documento è indipendente dalla lunghezza dei vettori stessi, risolvendo il problema della sensibilità alla lunghezza intrinseco alla distanza euclidea.  La similarità del coseno si calcola tramite il prodotto scalare dei vettori diviso per il prodotto delle loro norme.  Se i vettori fossero normalizzati a lunghezza unitaria, la normalizzazione sarebbe esplicita e il calcolo semplificato.  In sintesi, la normalizzazione, sia esplicita che implicita, è fondamentale per una rappresentazione accurata e robusta dei topic nella modellazione vettoriale dei documenti.

---

La normalizzazione dei vettori, ottenuta dividendo un vettore per la sua norma L2, genera vettori unitari (lunghezza 1) posizionati sulla superficie dell'ipersfera unitaria.  Questo processo è cruciale nei modelli di informazione per rendere confrontabili documenti di lunghezza diversa, evitando che la lunghezza influenzi sproporzionatamente la similarità.  La similarità del coseno, calcolata come prodotto interno normalizzato di due vettori (formula:  `sim(d_1, d_2) = (d_1 ⋅ d_2) / (||d_1|| ⋅ ||d_2||) = Σᵢ(wᵢⱼ ⋅ wᵢₖ) / (√Σᵢ(wᵢⱼ²) ⋅ √Σᵢ(wᵢₖ²))`), ne è un esempio. ![[1) Intro-20241123122258468.png]]

L'algoritmo Cosine Score calcola la similarità tra una query e i documenti in uno spazio vettoriale: inizializza punteggi e lunghezze dei documenti; per ogni termine della query, calcola il peso e itera sulla sua lista di occorrenze, aggiornando i punteggi dei documenti; infine, normalizza i punteggi dividendoli per la lunghezza del documento e restituisce i K punteggi più alti.  ```` 1. float Scores[N] = 0 2. float Length[N] 3. for each query term t 4. do calculate w_t,q and fetch postings list for t 4.1. for each pair(d, tf_(t,d), w_(t,q)) in postings list 4.2. do Scores[d] += w_(t,d) × w_(t,q) 5. Read the array Length 6. for each d 7. do Scores[d] = Scores[d]/Length[d] 8. return Top K components of Scores ````

La ponderazione Tf-Idf offre diverse varianti per `Term Frequency` (n, l, a, b) e `Document Frequency` (n, t, p), e per la `Normalization` (n, c, u, b).  Le soluzioni di default sono spesso la seconda opzione di ogni tipo. La `term frequency` aumentata (a) introduce uno smoothing utile nel retrieval di query espanse. La notazione SMART (ddd.qqq) specifica la combinazione di ponderazione usata (es: lnc.ltc indica tf logaritmico, nessun idf, normalizzazione coseno per i documenti e tf logaritmico, idf, normalizzazione coseno per le query).  Una tabella riassume le diverse opzioni e un esempio di schema standard è fornito.

---

Questo documento descrive un sistema di recupero dell'informazione basato sulla similarità coseno per classificare documenti di assicurazione auto in risposta a una query.

Il sistema rappresenta sia i documenti che la query come vettori tf-idf.  Il punteggio di similarità coseno viene calcolato tra il vettore query e ogni vettore documento. I documenti vengono poi classificati in base a questo punteggio, restituendo i primi K risultati all'utente.  Un esempio numerico è fornito nella tabella iniziale, mostrando il calcolo del punteggio per un documento specifico.  Il punteggio finale è la somma di diversi contributi (nell'esempio 0.8).

I vantaggi di questo approccio includono la capacità di fornire corrispondenze parziali, punteggi naturali di classifica, e un'implementazione efficiente.  Tuttavia, presenta anche degli svantaggi:  mancanza di informazioni sintattiche e semantiche, l'ipotesi di indipendenza dei termini (Bag-of-Words - BoW), l'ipotesi di ortogonalità dei vettori termine, e l'assenza di un controllo booleano.

---
