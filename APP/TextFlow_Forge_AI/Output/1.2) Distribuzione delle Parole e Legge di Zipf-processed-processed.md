
# Compressione dei Dati in Information Retrieval

## Obiettivi della Compressione

* Ridurre lo spazio di archiviazione su disco (costi ridotti).
* Aumentare la capacità di memoria principale.
* Accelerare il trasferimento dati (disco-memoria).
* Migliorare le prestazioni di lettura (con algoritmi efficienti). Gli algoritmi di decompressione sono generalmente molto veloci.

## Tipi di Compressione

* **Compressione del Dizionario e dei File di Posting:** Cruciale per la memoria (dizionario e liste di posting) e lo spazio su disco/velocità di lettura (file di posting).
* **Compressione Senza Perdita vs. Con Perdita:** Prevalentemente senza perdita; con perdita usata con cautela (es. pre-elaborazione testo, poda liste di posting) per una minima perdita di qualità.


## Relazione tra Dimensione del Vocabolario e Dimensione della Collezione

* **Legge di Heaps:**  $M = kT^b$
    * M: dimensione del vocabolario (parole distinte).
    * T: numero di token nella collezione.
    * k e b: costanti (tipicamente $30 ≤ k ≤ 100$ e $b ≈ 0.5$).
    * Relazione lineare in scala log-log: $log M = log k + b log T$.
    * Esempio Reuters RCV1: $log_{10}M = 0.49 \log_{10}T + 1.64$ ($k≈ 44$, $b = 0.49$). Buona aderenza empirica, tranne una fase transitoria iniziale.


## Distribuzioni Skew di Tipo Power-Law

* **Caratteristiche:**
    * Concentrazione di massa in una piccola porzione della distribuzione.
    * Coda lunga (valori che diminuiscono lentamente).
    * Spesso osservata in fenomeni naturali e sociali.
* **Distribuzione di tipo Power-Law:** Modello matematico che descrive fenomeni con pochi elementi ad alto valore e molti ad basso valore (simile a funzione esponenziale).
* **Legge di Pareto (Principio 80-20):** Esempio di distribuzione power-law; l'80% degli effetti deriva dal 20% delle cause.


## Le leggi di Zipf e Heaps nel linguaggio naturale

**I. Distribuzioni di tipo Power Law:**

* **Definizione:** Pochi elementi hanno valori molto alti, mentre molti altri hanno valori molto bassi.
* **Esempi:**
    * Distribuzione della ricchezza
    * Numero di pagine dei siti web
    * Numero di follower sui social network
    * Popolazione delle città
    * Frequenza delle parole in un documento (Legge di Zipf)

**II. Confronto tra Distribuzioni di Poisson e Power Law:**

* **Similitudini:** Entrambe asimmetriche.
* **Differenze:** La distribuzione di Poisson modella la probabilità di eventi in un intervallo, mentre la legge di potenza descrive la relazione tra due variabili ($y \propto x^k$). La scelta dipende dal "min rate" nella distribuzione di Poisson.

**III. Legge di Heaps (Prima Legge di Potenza):**

* Stima la dimensione del vocabolario di un corpus testuale.

**IV. Legge di Zipf (Seconda Legge di Potenza):**

* **Formula:** $cf_{i} \propto \frac{1}{i} = \frac{K}{i}$, dove $cf_i$ è la frequenza del termine i-esimo, `i` è il suo rango e `K` è una costante.
* **Forma logaritmica:** $log(cf_{i}) = log(K) - log(i)$ (relazione lineare inversa).
* **Interpretazione:** La frequenza di un termine decresce linearmente con il suo rango su scala logaritmica.
* **Esempio:** Se "the" appare $cf_1$ volte, "of" appare $cf_1/2$ volte, "and" $cf_1/3$ volte, ecc.
* **Osservazione:** Alta frequenza per ranghi bassi, frequenza decrescente linearmente su scala doppia logaritmica.

**V. Implicazioni della Legge di Zipf secondo Luhn:**

* **Parole molto frequenti:** Poco utili per l'indicizzazione (troppo generiche).
* **Parole molto rare:** Poco utili per l'indicizzazione (troppo rare).
* **Parole più discriminanti:** Hanno frequenza da bassa a media.


## I. Selezione dei Termini per l'Indicizzazione

**A. Frequenza Ottimale delle Parole:**

* Le parole con frequenza intermedia sono le più utili per l'indicizzazione.
* La distribuzione delle parole segue la legge di Zipf: frequenza ∝ 1/rango.
* Il grafico mostra una distribuzione a coda lunga (![[immagine_coda_lunga]]).

**B. Frequenze di Taglio:**

* Scopo: escludere termini troppo frequenti (es. articoli, preposizioni) e troppo rari.


---

# Recupero Classificato di Documenti

## I. Introduzione al Recupero Classificato

Il recupero classificato supera i limiti delle query booleane, risolvendo il problema dell' "abbondanza o carestia" di risultati.  Presenta solo i primi *k* risultati (es. 10), migliorando significativamente l'esperienza utente.  Accetta l'intrinseca imprecisione delle query di testo libero, riflettendo l'esigenza informativa spesso non perfettamente definita.

## II. Ponderazione della Rilevanza dei Termini e Recupero Classificato

### A. Limiti della Ricerca Booleana

La ricerca booleana presenta diversi limiti: restituisce troppi o troppo pochi risultati, richiede utenti esperti ed è poco user-friendly, soprattutto nel contesto del web.

### B. Recupero Classificato (Ranked Retrieval)

Il recupero classificato risolve i problemi della ricerca booleana ordinando i documenti per pertinenza alla query.  Esprime quantitativamente la pertinenza e si concentra sul recupero, non sull'estrazione di informazioni (a meno che non sia necessario, come nel caso di pagine hub).

### C. Query di Testo Libero

Le query di testo libero sono sequenze di parole in linguaggio naturale.


## III. Punteggio di Pertinenza

Ad ogni documento viene assegnato un punteggio di pertinenza compreso tra 0 e 1, che riflette la corrispondenza tra documento e query. Questo punteggio determina l'ordine di presentazione dei risultati, mostrando prima i documenti più pertinenti.


## IV. Misure di Similarità per Insiemi Finiti

Queste misure sono efficienti e normalizzano la lunghezza, indipendentemente dalle dimensioni di documenti e query.  Tuttavia, presentano un limite: non considerano la frequenza del termine (*tf*) nel documento e la sua scarsità nella collezione.  La *tf* (Term Frequency) indica che più alta è la frequenza del termine di query nel documento, maggiore dovrebbe essere il punteggio. La scarsità, invece, indica che i termini rari sono più informativi di quelli frequenti.


## V. Esempi di Misure di Similarità

* **Jaccard:**  $J(A,B) = \frac{\|A \cap B\|}{\|A \cup B\|}$ (Misura la similarità come rapporto tra intersezione e unione di due insiemi).
* **Sørensen-Dice:** $DSC(A,B) = \frac{2 \times \|A \cap B\|}{\|A\| + \|B\|}$ (Simile a Jaccard, ma pesa di più l'intersezione).
* **Overlap:** $O(A,B) = \frac{\|A \cap B\|}{min(\|A\|, \|B\|)}$ (Misura la sovrapposizione rispetto all'insieme più piccolo).
* **Simple Matching:** $SM(A,B) = \frac{\|A \cap B\| + \|\overline{A} \cap \overline{B}\|}{\|A \cup B\|}$ (Considera presenze e assenze di elementi).


## VI. Considerazioni Aggiuntive

La cardinalità dell'unione di due insiemi include la cardinalità della loro intersezione (formula di inclusione-esclusione).  *Dice* è più "generoso" di *Jaccard*, utile in contesti specifici (indexing, Iris Neighbor Search).  Non tutte le misure di similarità sono metriche (es. *Dice* non soddisfa la disuguaglianza triangolare).


## VII. Criticità nella Determinazione delle Frequenze di Taglio

La determinazione delle frequenze di taglio per la selezione dei termini è difficile, *task-dependent* e *data-driven*.  Esiste inoltre una dipendenza dal dominio e dal linguaggio.  Esempi di regole pratiche includono la rimozione di termini presenti in più del 50% dei documenti e la rimozione di termini presenti in meno di 3-5 documenti.


## Schema Riassuntivo: Rappresentazione di Testi e Misure di Similarità

### I. Misure di Similarità e Disuguaglianza Triangolare

#### A. Dice Coefficient

Il Dice Coefficient misura la similarità, non la distanza.  Esempio: Documenti "AB", "A", "B".  $Dice(1,2) = \frac{2}{3}$, $Dice(1,3) = \frac{2}{3}$, $Dice(2,3) = 0$.  Viola la disuguaglianza triangolare: $Distanza(1,2) + Distanza(1,3) \geq Distanza(2,3)$ non è rispettata.

#### B. Overlap Coefficient (Simpson)

Definizione: $\frac{|X ∩ Y|}{min(|X|, |Y|)}$. Applicazione: Confronto tra summary (es. uno di riferimento e uno generato).


### II. Frequenze di Parole

#### A. Collection Frequency

Frequenza totale di una parola in un'intera collezione di documenti. Proprietà globale. Somma delle Term Frequency su tutti i documenti.

#### B. Term Frequency

Frequenza di una parola in un singolo documento. Proprietà locale.

#### C. Document Frequency

Numero di documenti che contengono un determinato termine.


### III. Rappresentazione dei Testi e Calcolo dello Score

#### A. Matrice di Rappresentazione

Ogni cella contiene il peso di un termine in un documento. Esprime il contributo del termine alla rappresentazione del documento.  `![[]]`

---

# Metodi di Calcolo del Peso di un Termine e Scoring di Rilevanza

## I. Metodi per il Calcolo del Peso di un Termine

### A. Utilizzo diretto di TF e CF

Formula:  $w_{t,d} = \text{tf}_{t,d} \frac{1}{\text{cf}_{t}}$

* $w_{t,d}$: peso del termine *t* nel documento *d*
* $\text{tf}_{t,d}$: frequenza del termine *t* nel documento *d*
* $\text{cf}_{t}$: frequenza del termine *t* nell'intera collezione

Nessuna normalizzazione iniziale.

### B. Funzioni Separate per TF e CF

Formula: $w_{t,d}^{(t)} = f(\text{tf}_{t,d}) + g(\text{cf}_{t}) = \text{tf}_{t,d} + \log(\text{cf}_{t})$

Il termine relativo a CF risulta troppo dominante.


## II. Considerazioni sulla Rilevanza di un Termine

### A. Fattori Influenzanti

* Term Frequency (TF)
* Collection Frequency (CF)
* Document Frequency (DF) (menzionata, ma non utilizzata in formule)
* Tempo di inerzia (idea non sviluppata)
* Legge di Zipf (influenza la distribuzione delle frequenze)

### B. Funzione di Scoring

$Rilevanza(termine) = F(TF) + G(CF)$

Il peso relativo di F e G determina l'importanza di TF vs. CF.

### C. Lunghezza dei Documenti

Nessuna assunzione sulla lunghezza dei documenti.


## III. Analisi delle Proposte di Scoring (Proposta 1 vs. Proposta 2)

### A. Coerenza con la Legge di Zipf

Analisi focalizzata su frequenze medio-alte e medio-basse.

### B. Caso 1: Termine ad alta frequenza (testa della distribuzione)

* **Proposta 1:** Peso prossimo a 0 indipendentemente da TF.
* **Proposta 2:** Dominanza del termine logaritmico di CF, potenzialmente smorzabile.

### C. Caso 2: Termine a bassa/media frequenza

* **Proposta 1:** TF smorzata da CF.
* **Proposta 2:** TF enfatizzata rispetto a CF.

### D. Problemi

Nessuna proposta risulta efficace in modo consistente, il comportamento varia a seconda del termine e delle caratteristiche del documento.


## IV. Problematiche Aggiuntive

* Smorzamento lineare inverso di TF con CF troppo aggressivo.
* Moltiplicazione diretta di TF e CF troppo aggressiva.


## Calcolo del Peso dei Termini nei Documenti

## I. Problematiche nel Calcolo del Peso

### A. Necessità di considerare la *document frequency*

1. Più discriminante della *collection frequency*.
2. Distribuzione potenzialmente più piatta.

### B. Inefficacia della combinazione lineare

1. Dominanza di un termine sull'altro.

### C. Aggressività della funzione reciproca lineare, anche con *document frequency*.

### D. Soluzione proposta: Smorzamento *smooth* con la formula: $\frac{1}{\log(document \ frequency)}$

1. Peso basso per termini in quasi tutti i documenti (document frequency ≈ n).


## II. Smorzamento della Term Frequency (TF)

### A. Importanza dello smorzamento per il calcolo del peso dei termini.

### B. Valori di TF > 1 non sono problematici (limitati da n).

### C. Smorzamento tramite divisione per $\log_2 n$.

### D. Smorzamento più accentuato per termini rari.


## III. Funzione TF-IDF

### A. Formula: $w_{t,d}=\log(1+tf_{t,d}) \times \log_{10}\left( \frac{N}{df_{t}} \right)$

1. `tf<sub>t,d</sub>`: Frequenza del termine *t* nel documento *d*.
2. `N`: Numero totale di documenti.
3. `df<sub>t</sub>`: Numero di documenti contenenti il termine *t*.

### B. Interpretazione

1. `log(1+tf<sub>t,d</sub>)`: Frequenza del termine nel documento (smorzata).
2. `log<sub>10</sub>(N/df<sub>t</sub>)`: Inverse Document Frequency (IDF) (elevata per termini rari).

### C. Vantaggi

1. Penalizza termini comuni.
2. Evidenzia termini rari.
3. Bilancia frequenza locale e globale.

### D. Considerazioni importanti

1. Rimozione delle stop words.
2. Stemming e lemmatization.


## I. Modellazione della Rilevanza dei Documenti

### A. Bag-of-words (BoW)

**Pro:** Corrispondenza parziale, punteggi graduati, efficiente per grandi dataset, modello spazio vettoriale.

**(Tabella di personaggi e opere con conteggi numerici) è menzionata ma non fornita nel testo.  Allo stesso modo, i dettagli su "C. Metodi di Calcolo dello Score" e "IV. Modelli di Rappresentazione dei Testi" sono assenti.)**

---

# TF-IDF (Term Frequency-Inverse Document Frequency)

## A. Limiti della Frequenza Grezza dei Termini

**Contro:**  Un approccio basato sulla semplice frequenza grezza dei termini ignora la sintassi, la semantica e il controllo booleano.  Inoltre, può preferire erroneamente documenti con alta frequenza di un termine, ma assenza dell'altro in una query a due termini.

## B. Frequenza del Termine (TF)

**1. Problematiche della frequenza grezza:** La rilevanza di un termine non aumenta proporzionalmente alla sua frequenza grezza nel documento.

**2. Peso di frequenza logaritmica:** Per mitigare questo problema, si utilizza un peso di frequenza logaritmico:

$w_{t,d} = \begin{cases} 1+\log_{10}(\text{tf}_{td}) & \text{ if } \text{tf}_{td} > 0 \\ 0 & \text{otherwise} \end{cases}$

Esempi: 0 → 0, 1 → 1, 2 → 1.3, 10 → 2, 1000 → 4,...

**3. Punteggio documento-query:** Il punteggio di rilevanza di un documento rispetto a una query è dato dalla somma dei pesi logaritmici dei termini comuni a entrambi:

$\sum_{t\in q \cap d}(1+\log(tf_{t,d}))$


## C. Frequenza Inversa del Documento (IDF)

Termini rari sono generalmente più informativi; ad esempio, "*arachnocentrico*". Maggiore è la rarità di un termine, maggiore dovrebbe essere il suo peso nella valutazione della rilevanza.


## D. TF-IDF e Smorzamento

**1. Soglie di taglio:** Metodi semplici possono escludere termini con frequenza troppo alta o troppo bassa.

**2. Smorzamento logaritmico:**  Questo approccio gestisce meglio la distribuzione dei termini, spesso seguendo la Legge di Zipf. Vantaggi: evita soglie arbitrarie e gestisce efficacemente matrici sparse.

**3. Doppio logaritmo:** Per corpus molto grandi, si può applicare un ulteriore smorzamento logaritmico al TF.

**4. Normalizzazione e Similarità:** La normalizzazione dei vettori e la scelta di una metrica di similarità (es. coseno) sono aspetti cruciali per il recupero delle informazioni e la classificazione.


---

## TF-IDF: Dettagli Aggiuntivi

### I. Document Frequency (df) vs. Collection Frequency (cf)

* **df:** Numero di documenti in cui un termine appare.  È preferito a *cf* (Collection Frequency, frequenza totale di un termine nell'intero corpus) per la discriminazione tra documenti.
* Esempio: "assicurazione" (cf=10440, df=3997), "prova" (cf=10422, df=8760).  `df` è più informativo per la ricerca perché riflette la distribuzione del termine tra i documenti.

### II. Inverse Document Frequency (idf)

* Misura inversa dell'informatività di un termine.
* Formula:  $$idf_{t}=\log_{10}\left( \frac{N}{df_{t}} \right)$$ dove *N* è il numero totale di documenti.
* Il logaritmo smorza l'effetto dell'idf, evitando che termini estremamente rari dominino il punteggio.
* La `df` di un termine è unica e influenza la classificazione principalmente per query con pochi termini (*k*<1).

### III. TF-IDF Weighting

* Assegna un peso ai termini basato sulla frequenza nel documento (tf) e sulla rarità nel corpus (idf).
* **tf (term frequency):** Peso maggiore per termini più frequenti in un documento.
* **idf (inverse document frequency):** Peso maggiore per termini più rari nel corpus.
* Formula del peso tf-idf: $$w_{t,d}=\log(1+tf_{t,d})\times\log_{10}\left( \frac{N}{df_{t}} \right)$$
* Il peso aumenta con il numero di occorrenze nel documento e con la rarità del termine.
* Punteggio documento-query: somma dei pesi tf-idf dei termini comuni a documento e query.
* Riflette la distribuzione di probabilità power-law della frequenza dei termini (termini comuni hanno bassa probabilità, termini rari alta probabilità).

### IV. Varianti TF-IDF

* Diverse formule per il calcolo di tf: con o senza logaritmi.
* Ponderazione dei termini nella query: ponderati o non ponderati.
* Esempi di varianti: $$ \frac{tf_{i,d}}{\max_{j}tf_{j,d}} ,\ \frac{tf_{id}}{\sqrt{ \sum_{j}(tf_{j,d})^2 }} ,\ \frac{tf_{id} \cdot idf_{i}}{\sqrt{ \sum_{j}(tf_{j,d} \cdot idf_{j})^2 }} $$

### V. Assunzioni e Principi Chiave

* **Assunzioni:** Collezione di documenti omogenea per dominio e pattern di frequenza simili tra documenti.
* **Principi chiave:** Peso variabile per uno stesso termine a seconda del documento; normalizzazione della lunghezza del documento necessaria.


---

## Rappresentazione Vettoriale dei Documenti

### I. Spazio Vettoriale

* **Spazio Vettoriale:** Documenti e query sono rappresentati come vettori in uno spazio ad alta dimensionalità e molto sparso.
* Assi: Termini del vocabolario.
* Punti: Documenti.
* **Similarità:** La prossimità tra documenti e query è misurata dalla similarità dei vettori (es. coseno).  Prossimità ≈ inversa della distanza.

### II. Matrice di Peso Tf-Idf

* **Calcolo del Peso:** Si utilizza la matrice Tf-Idf per rappresentare la rilevanza dei termini nei documenti. ![[1) Intro-20241014151203327.png]]
* **Problema della Lunghezza dei Documenti:** Senza normalizzazione, la distanza euclidea favorisce i documenti più lunghi.

### III. Smoothing

* **Scopo:** Ridurre l'influenza eccessiva dei termini rari.

### IV. Normalizzazione e Distanza Euclidea

* **Normalizzazione L2:** Divisione per la norma L2.
* Diluisce il segnale informativo, soprattutto con documenti di lunghezza diversa.
* Presenta proprietà geometriche specifiche, diverse da altri metodi (es. ...).

---

# Normalizzazione e Similarità Coseno nella Ricerca di Informazioni

## Normalizzazione e Distanza Euclidea

La normalizzazione dei vettori è fondamentale nella ricerca di informazioni, influenzando direttamente la costruzione della matrice dei dati e la scelta della metrica di similarità.  La distanza euclidea, ad esempio, presenta alcuni problemi:

* **Sensibilità alla lunghezza dei vettori:**  Vettori con distribuzioni di termini simili ma lunghezze diverse possono avere una distanza euclidea elevata.
* **Controesempio:** La distanza euclidea tra un documento e la sua concatenazione con se stesso è elevata.
* **Maggiore sensibilità all'alta dimensionalità:** Rispetto a misure di correlazione, la distanza euclidea è più sensibile alla "maledizione della dimensionalità".


## Normalizzazione Implicita e Angolo Coseno

Un approccio *scale-invariant*, indipendente dalla lunghezza dei vettori, è quello basato sull'angolo tra i vettori.  La similarità del coseno è una misura di prossimità che classifica i documenti in base all'angolo (o al coseno dell'angolo) tra il vettore query e il vettore documento:

$\cos(\text{query, documento}) = \frac{\text{query} \cdot \text{documento}}{||\text{query}|| \cdot ||\text{documento}||}$

Se i vettori sono normalizzati (lunghezza unitaria), la formula si semplifica al prodotto scalare, rappresentando una *normalizzazione implicita*.


## Normalizzazione dei Vettori e Similarità del Coseno

La normalizzazione dei vettori, dividendoli per la loro norma L2 ($\sqrt{\sum_{i=1}^{n} w_i^2}$), produce vettori unitari (lunghezza 1) sulla superficie dell'ipersfera unitaria. Questo permette un confronto equo tra documenti di lunghezza diversa, evitando che la lunghezza influenzi sproporzionatamente la similarità.

La similarità del coseno è definita come:

$\text{sim}(d_1, d_2) = \frac{d_1 \cdot d_2}{\|d_1\| \cdot \|d_2\|} = \frac{\sum_{i=1}^{n} w_{i,j} \cdot w_{i,k}}{\sqrt{\sum_{i=1}^{n} w_{i,j}^2} \cdot \sqrt{\sum_{i=1}^{n} w_{i,k}^2}}$

Un algoritmo per il calcolo del *cosine score* potrebbe essere strutturato come segue:

1. Inizializzare un array `Scores` a 0 e un array `Length`.
2. Per ogni termine `t` della query:
    * Calcolare il peso `w_{t,q}` e recuperare la lista di occorrenze.
    * Per ogni coppia (documento `d`, frequenza del termine `tf_{t,d}`, peso `w_{t,q}`) nella lista: `Scores[d] += w_{t,d} × w_{t,q}`.
3. Leggere l'array `Length`.
4. Per ogni documento `d`: `Scores[d] = Scores[d]/Length[d]`.
5. Restituire i `K` punteggi più alti.


## Varianti di Ponderazione Tf-Idf

La ponderazione Tf-Idf prevede diverse varianti per la `Term Frequency` (tf) e la `Document Frequency` (df), oltre alla normalizzazione:

**Term Frequency (tf):**

* `n` (naturale): $tf_{r, d}$
* `l` (logaritmico): $1 + \log(tf_{r, d})$
* `a` (aumentato): $0.5 + \frac{0.5 \cdot tf_{r, d}}{\max_{r} (tf_{r, d})}$
* `b` (booleano): $\begin{cases} 1 & \text{if } tf_{r, d} > 0 \\ 0 & \text{otherwise} \end{cases}$

**Document Frequency (df):**

* `n` (nessuno): $1$
* `t` (idf): $\log \frac{N}{df_r}$
* `p` (prob idf): $\max \{ 0, \log \frac{N - df_r}{df_r} \}$

**Normalizzazione:**

* `n` (nessuna): $1$
* `c` (coseno): $\frac{1}{\sqrt{w_1^2 + w_2^2 + \dots + w_n^2}}$
* `u` (pivoted unique): $\frac{1}{u}$
* `b` (byte size): $\frac{1}{\text{CharLength}^{\alpha}}, \alpha < 1$

Spesso vengono utilizzate le seconde opzioni di ogni tipo. La *term frequency* aumentata è utile per il retrieval puro con query espanse, offrendo uno smoothing.  La notazione SMART (ddd.qqq) combina le ponderazioni per documenti e query (es: lnc.ltc - logaritmico, nessun idf, coseno per documenti; logaritmico, idf, coseno per query).


## Classifica dello Spazio Vettoriale per la Ricerca di Assicurazioni Auto

**I. Metodo:**

* **Rappresentazione:** Vettori tf-idf ponderati per documenti e query.
* **Similarità:** Calcolo del punteggio di similarità del coseno tra il vettore query e ogni vettore documento.
* **Classifica:** Ordinamento dei documenti in base al punteggio di similarità.
* **Output:** Restituzione dei primi K documenti.

**II. Vantaggi:**

* Corrispondenza parziale e punteggi/classifica naturali.
* Buone prestazioni pratiche nonostante semplificazioni.
* Implementazione efficiente.

**III. Svantaggi:**

* Mancanza di informazioni sintattiche.
* Mancanza di informazioni semantiche.
* Ipotesi di indipendenza dei termini (Bag-of-Words - BoW).
* Ipotesi di ortogonalità a coppie dei vettori dei termini.
* Assenza di controllo booleano (es. richiesta di presenza di un termine).

**IV. Esempio:** (Punteggio = 0 + 0 + 0.27 + 0.53 = 0.8)

---

Non è possibile formattare il testo fornito perché consiste solo di una singola frase che descrive un punto da includere o meno in un testo più ampio, non fornendo il testo stesso da formattare.  Per poter formattare il testo secondo le istruzioni, è necessario fornire il testo completo che contiene il punto menzionato.

---

Per favore, forniscimi il testo da formattare.  Non ho ricevuto alcun testo da elaborare nell'input precedente.  Inserisci il testo che desideri formattare e lo elaborerò secondo le istruzioni fornite.

---
