
## Schema Riassuntivo: IR, NLP e Analisi di Dati Testuali

**1. Linguaggio Naturale e Discipline Correlate**

    *   Al centro di IR e NLP, complesso e multimodale.
    *   **1.1 Recupero delle Informazioni (IR)**
        *   Ritrovamento (semi-)automatico di dati testuali non strutturati che soddisfano un bisogno informativo.
        *   **1.1.1 Problema di Base:** Trovare documenti rilevanti a una query in una collezione.
    *   **1.2 Elaborazione del Linguaggio Naturale (NLP)**
        *   Sviluppo di sistemi automatici che comprendono e generano linguaggi naturali.
    *   **1.3 Sfide dei Dati Testuali**
        *   **1.3.1 Sfide Generali:** Grandi dataset, alta dimensionalità, dati rumorosi, evoluzione continua.
        *   **1.3.2 Sfide Specifiche:**
            *   Mancanza di struttura progettata per i computer.
            *   Complessità semantica e strutturale del linguaggio naturale (ambiguità a livello morfologico, sintattico, semantico e pragmatico).
            *   Gestione di linguaggi diversi e multilinguismo.

**2. IR vs. Altre Discipline**

    *   **2.1 Obiettivo Comune:** Rendere più facile trovare le cose (es. sul Web).
    *   **2.2 Attività Comuni:**
        *   Ottimizzazione dei motori di ricerca (SEO).
        *   Crawling dei dati.
        *   Estrazione di documenti di interesse.
    *   **2.3 Differenze:**
        *   Web scraping: Estrazione di informazioni.
        *   Trovare schemi in grandi collezioni: NLP, Linguistica computazionale.
        *   Scoprire informazioni finora sconosciute: NLP, Estrazione di testo.
        *   Discriminazione del testo: NLP (Machine Learning).
        *   Comprensione del testo: NLP (Machine Learning, AI generativa, ...).

**3. Topic Detection and Tracking**

    *   Acquisizione e analisi incrementale di dati per identificare topic emergenti e obsoleti.
    *   **3.1 Caratteristiche:**
        *   Analisi dei dati incrementale.
        *   Supervisionato o non supervisionato.
    *   **3.2 Difficoltà:**
        *   Task incrementale (riconoscimento di nuovi topic e identificazione di quelli obsoleti).
        *   Capacità limitate (fase di tracking).
        *   Definizione di topic (insieme di termini).
        *   Segnale (tracciamento dei dati in ingresso).

**4. Modellazione di Topic Stocastica**

    *   Tecnica che utilizza modelli probabilistici per analizzare grandi quantità di testo e identificare i temi principali (topic).
    *   Ogni documento è una combinazione di diversi topic.
    *   Ogni parola ha una probabilità di appartenere a un determinato topic.
    *   Esempio: **Latent Dirichlet Allocation (LDA).**

**5. Relation Extraction**

    *   **5.1 Relazione con Named Entity Recognition:** Identificazione delle entità prima dell'estrazione delle relazioni.
    *   **5.2 Pattern Frequenti:** Ricerca di pattern frequenti.
    *   **5.3 Definizione in NLP:** Identificazione di relazioni lessicali tra entità nominate.

---

**Schema Riassuntivo del Testo**

**1. Summarization**
    * 1.1. Approccio Tradizionale: Estrazione di parole chiave (anni '90).
    * 1.2. Guida: Utilizzo delle proprietà del documento per guidare il processo.

**2. KDD Pipeline in NLP**
    * 2.1. Fasi Sequenziali (Unfolding):
        * 2.1.1. Indicizzazione degli elementi costitutivi.
        * 2.1.2. Rappresentazione dei contenuti informativi.
        * 2.1.3. Apprendimento a valle del *result set*.
    * 2.2. Feature Selection: Utilizzo del machine learning.

**3. Valutazione**
    * 3.1. Criteri Statistici: *Accuracy* (con limitazioni).
    * 3.2. Gold Standard vs. Silver Standard:
        * 3.2.1. Gold Standard: Set di dati perfetto creato da esperti.
        * 3.2.2. Silver Standard: Generato automaticamente (es. GPT-4).

**4. Funzioni Principali in NLP**
    * 4.1. Funzione Primaria: Estrarre informazioni da testi.
    * 4.2. Macro Categorie:
        * 4.2.1. Indicativa:
            * 4.2.1.1. Rivelare elementi per determinare la rilevanza rispetto alle query.
            * 4.2.1.2. Fondamentale per l'esplorazione dei corpus e per il retrieval.
        * 4.2.2. Informativa:
            * 4.2.2.1. Ottenere un surrogato del testo senza riferimento al testo originale.
            * 4.2.2.2. Valido per il retrieval: condensare i dati originali in un formato surrogato.

**5. Browsing**
    * 5.1. Definizione: Esplorazione di collezioni di testo senza specificare interessi.
    * 5.2. Utilità: Utenti con bisogni poco chiari o inespressi.
    * 5.3. Funzionalità:
        * 5.3.1. Indicazione di documenti rilevanti.
        * 5.3.2. Supporto all'annotazione.

**6. Estrazione di Informazioni**
    * 6.1. Definizione: Recuperare informazioni specifiche da documenti rilevanti.
    * 6.2. Processo: Estrarre o inferire risposte dalla rappresentazione del testo.
    * 6.3. Esempi: *Web wrapping*, estrazione da siti di *e-commerce*.
    * 6.4. Dettagli:
        * 6.4.1. Sistema identifica il documento come rilevante.
        * 6.4.2. Focus sull'individuazione di informazioni specifiche.
        * 6.4.3. Facilitato da schemi o database pre-esistenti.
    * 6.5. Tipi di Template:
        * 6.5.1. Slot: Spazi da riempire.
        * 6.5.2. Riempitivi pre-specificati: Valori fissi.
        * 6.5.3. Multipli riempitivi: Slot con più valori.
        * 6.5.4. Ordine fisso: Sequenza fissa degli slot.
    * 6.6. Modelli di Estrazione:
        * 6.6.1. Specificazione di elementi: Espressioni regolari.
        * 6.6.2. Modelli precedenti e successivi: Contesto pre e post-slot.
        * 6.6.3. Esempio: Estrazione di termini da un modello.

**7. Recupero di Documenti**
    * 7.1. Definizione: Selezionare documenti da una collezione in risposta a una query.
    * 7.2. Processo:
        * 7.2.1. Classificazione dei documenti per rilevanza.
        * 7.2.2. Abbinamento tra rappresentazione del documento e della query.
        * 7.2.3. Restituzione di un elenco di documenti pertinenti.
    * 7.3. Modelli: Booleano, spazio vettoriale, probabilistico.

---

**Schema Riassuntivo: Applicazioni di Base del Text Mining**

**1. Differenze Fondamentali nel Text Mining**
    *   Rappresentazione dei contenuti testuali
    *   Rappresentazione dei bisogni informativi
    *   Abbinamento tra contenuti e bisogni

**2. Applicazioni Principali**
    *   **2.1 Scoperta della Conoscenza**
        *   Estrazione di informazioni
        *   Distillazione delle informazioni (estrazione basata su struttura predefinita)
    *   **2.2 Filtraggio dell'Informazione**
        *   Rimozione di informazioni irrilevanti
        *   Applicazioni:
            *   Web personalization
            *   Sistemi di raccomandazione (anche *collaborative based*)

**3. Utilizzi Tipici**
    *   Categorizzazione gerarchica
    *   Riassunto del testo
    *   Disambiguazione del senso delle parole
    *   Filtraggio del Testo (o dell'Informazione)
    *   CRM e marketing: Cross-selling e raccomandazioni di prodotti
    *   Raccomandazione di prodotti
    *   Filtraggio di notizie e spam

**4. Categorizzazione Gerarchica**
    *   Organizzazione di documenti in una struttura gerarchica (tassonomie)
    *   Navigazione strutturata attraverso le categorie
    *   Flessibilità: Aggiunta/rimozione di categorie
    *   **4.1 Caratteristiche:**
        *   Natura ipertestuale dei documenti: Analisi degli hyperlink
        *   Struttura gerarchica dell'insieme di categorie
        *   Decomposizione della classificazione come decisione ramificata (A un nodo interno)

**5. Summarization (Riassunto del Testo)**
    *   Generazione di riassunti concisi
    *   Profili per strutturare informazioni importanti
    *   Tecnica dipendente dalla natura dei documenti (es. recensioni)
    *   **5.1 Facilitazione dell'Accesso alle Informazioni:**
        *   Estrazione parole chiave per descrivere un cluster
        *   Astrazione documenti in una collezione
        *   Riassunto documenti recuperati da una ricerca
    *   **5.2 Tipi di Riassunto:**
        *   Alto livello (panoramica)
        *   Dettagliato
    *   **5.3 Approcci basati sulla dimensione dell'unità di testo:**
        *   Riassunti di parole chiave
        *   Riassunti di frasi

**6. Disambiguazione del Senso delle Parole (WSD)**
    *   Assegnazione del significato corretto a una parola in base al contesto
    *   Analisi simultanea di tutti i termini nel contesto
    *   Utilizzo di inventari di sensi e misure di correlazione semantica
    *   Rinnovo dell'interesse grazie a risorse linguistiche elettroniche (Wikipedia)
    *   **6.1 Esempio:**
        *   "bank" (istituto finanziario vs. argine fluviale)
        *   "Last week I borrowed some money from the bank" (istituto finanziario)

**7. Filtraggio nel Text Mining**
    *   Classifica documenti come rilevanti o irrilevanti
    *   Blocco di documenti irrilevanti

---

**Schema Riassuntivo: Text Mining e Modellazione del Testo**

**1. Filtraggio di Informazioni**

   *   **1.1. Definizione:** Feed di notizie (agenzia di stampa -> giornale)
   *   **1.2. Caratteristiche:**
        *   Text Classification (rilevante/irrilevante)
        *   Implementabile lato produttore (instradamento selettivo) o consumatore (blocco)
        *   Richiede "profilo" per consumatore (produttore) o profilo generale (consumatore)
   *   **1.3. Filtraggio Adattivo:**
        *   Profilo iniziale definito dall'utente
        *   Aggiornamento del profilo basato sul feedback dell'utente
   *   **1.4. Applicazioni:**
        *   CRM: Analisi del feedback clienti (standardizzazione e raggruppamento)
        *   Raccomandazione di Prodotti: Approcci basati sul contenuto e collaborativi
        *   Rilevamento dello Spam:
            *   Sfide: Costi asimmetrici degli errori (falsi positivi/negativi), distribuzione non uniforme delle classi

**2. Vocabolario: Modello di Rappresentazione dei Testi**

   *   **2.1. Definizione di Termine:**
        *   **Problema:** Selezione dei termini (bilanciamento tra inclusione e importanza)
        *   **Contesto:** Composizione testuale e obiettivo dell'indice
        *   **Soluzione:** Modellazione delle relazioni tra le parole (pattern relazionali)
   *   **2.2. Cosa è un termine?**
        *   Parola singola, coppia di parole, frase, word stem, n-gramma, tipo di parola
   *   **2.3. Modellazione delle relazioni tra termini:**
        *   Preferenza per relazioni semantiche (is-a, part-of)
        *   Obiettivo: Identificare termini che rappresentano la semantica del testo, minimizzando la codifica manuale
   *   **2.4. Altri argomenti correlati:**
        *   Analisi lessicale e morfologica: Punteggiatura, minuscolo, stopwords, stemming, lemmatizzazione, POS tagging
        *   Analisi della semantica del discorso (Anafora): Anafora letterale/pronominale, ellissi testuale, meronomia referenziale
        *   Pragmatica, Semiotica e Morfologia

**3. Tokenizzazione: Problemi Lessicali e Morfologici**

   *   **3.1. Definizione:** Organizzazione del testo in unità significative (token = termine candidato)
   *   **3.2. Punteggiatura:**
        *   Sequenze con trattino: *state-of-the-art*, *co-education*, *lowercase*, *lower-case*, *lower case* (gestione complessa)
        *   Apostrofi: *Italy’s capital* (rappresentazione: *Italy AND s*, *Italys*, *Italy’s*?)
        *   Acronimi: *U.S.A* e *USA* (trattamento dipendente dal contesto)

---

**Schema Riassuntivo del Testo**

**1. Pre-processing del Testo**

   *   **1.1 Entità Nominate:**
        *   Richiedono gestione specifica per evitare frammentazione non semantica (es. *San Francisco*, *Hewlett-Packard*).

   *   **1.2 Numeri:**
        *   Gestione: rimozione (stringhe puramente numeriche) vs. mantenimento (stringhe alfanumeriche).
        *   Motivazioni per il mantenimento:
            *   Preservazione informazioni numeriche.
            *   Individuazione codici errore, intervalli temporali, ecc.
        *   Varietà di tipi numerici:
            *   Date: *3/20/91*, *Mar. 12, 1991*, *20/3/91*.
            *   Tempo: *55 B.C.*
            *   Codici: *B-52*.
            *   Identificatori: *My PGP key is 324a3df234cb23e*.
            *   Numeri di telefono: *(800) 234-2333*.
        *   Complessità: Interazione numeri-testo rende difficile soluzione universale.

   *   **1.3 Metadati:**
        *   Spesso indicizzati separatamente (data di creazione, formato, ecc.).

   *   **1.4 Case delle Lettere:**
        *   Generalmente convertite in minuscolo (lowercase).

**2. Stopwords**

   *   **2.1 Definizione:**
        *   Parole grammaticali (articoli, preposizioni) con scarso potere informativo.
        *   Specifiche della lingua.

   *   **2.2 Rimozione:**
        *   Fondamentale nella pipeline di data analysis.
        *   Motivi:
            *   Non si riferiscono a oggetti o concetti.
            *   Poco contenuto semantico.
            *   Parole più comuni.
            *   Impatto sulla dimensionalità della rappresentazione dei testi (sparsità).
            *   Perdita di potere discriminante nelle misure di distanza. $\to$ *perdita di potere discriminante*

   *   **2.3 Casi in cui sono necessarie:**
        *   Query di frase: "King of Denmark".
        *   Titoli: "Let it be", "To be or not to be".
        *   Query "relazionali": "flights to London".

   *   **2.4 Gestione:**
        *   Tecniche di compressione e ottimizzazione delle query per minimizzare l'impatto.

   *   **2.5 Personalizzazione della Stop-List:**
        *   Scopo: ridurre e arricchire.
        *   Rimozione: termini molto frequenti nel corpus (poco esplicativi).
        *   Considerazione: esclusione di termini super rari (operazione delicata).
        *   Esclusione termini che appaiono una sola volta: dipende dal tipo di termine.

**3. Normalizzazione**

   *   **3.1 Scopo:**
        *   Uniformare la forma delle parole per trovare corrispondenze tra termini con forme diverse ma stesso significato.
        *   Garantire l'efficacia della ricerca.

   *   **3.2 Risultato:**
        *   Creazione di *termini* (voci nel dizionario di IR).
        *   Organizzazione in *classi di equivalenza* (raggruppano termini equivalenti).

   *   **3.3 Esempi:**
        *   Eliminazione di punti: "U.S.A.", "USA" → "USA".
        *   Eliminazione di trattini: "anti-discriminatory", "antidiscriminatory" → "antidiscriminatory".
        *   Eliminazione di accenti: "French résumé" vs. "resume".
        *   Eliminazione di umlaut: "German Tuebingen" vs. "Tübingen".

   *   **3.4 Effetti:**
        *   Semplificazione della rappresentazione del testo.
        *   Focalizzazione sulla struttura sintattica.
        *   Riduzione di parole con significati simili a una sola forma.

   *   **3.5 Espansione asimmetrica:**
        *   Alternativa alle classi di equivalenza.

---

**Schema Riassuntivo: Tecniche di Normalizzazione del Testo**

**1. Gestione delle Varianti di Termini**

*   Inclusione di varianti durante la ricerca (es: "window" -> "window", "windows")
*   Potenza vs. Efficienza: Approccio potente ma potenzialmente meno efficiente.

**2. Normalizzazione Linguistica**

*   Importanza dell'uso della lingua nella scelta del metodo di normalizzazione.
*   Rimozione di accenti e trattini: Spesso preferibile per gestire query utente.
*   Interdipendenza di Tokenizzazione e Normalizzazione: Dipendenza dalla lingua (es: formati data).

**3. Case Folding (Gestione delle Maiuscole/Minuscole)**

*   Riduzione a minuscolo: Approccio tipico.
*   Gestione delle Named Entity: Riconoscimento e potenziale mantenimento (acronimi).
*   Eccezioni: Maiuscole a metà frase (Es: General Motors).
*   Preferenza per il minuscolo: Utenti tendono ad usare il minuscolo.

**4. Riduzione della Matrice dei Dati (Algebra Lineare)**

*   Post-processing: Applicazione dell'algebra lineare.
*   Identificazione di colonne linearmente dipendenti (sinonimi).
*   Obiettivo: Riduzione dei sinonimi a una sola dimensione (pattern sintattici).

**5. Spell Checking (Controllo Ortografico)**

*   Prossimità tra stringhe: Gestione di correzioni multiple equivalenti.
*   Edit distance: Misura la differenza tra stringhe (manca il contesto).
*   Contesto: Necessario per correzioni accurate (non considerato).
*   Evitare spell checking: Se non strettamente necessario.
*   Corpus rumorosi: Inefficacia in contesti come messaggi istantanei.

**6. Emoticon**

*   Sostituzione o mantenimento: Con markup.
*   Pesatura: Delegata a fasi successive.
*   Importanza: Analisi del sentiment.

**7. Tesauri**

*   Generalizzazione: Termini correlati ma con forme diverse.
*   Gestione di sinonimi e omonimi.
    *   Sinonimi: Parole con significato simile ma forma diversa.
    *   Omonimi: Parole con forma uguale ma significato diverso.
*   Esempio: Classi di equivalenza (color = colour).
*   Implementazione:
    *   Indicizzazione multipla: Indicizzazione di "color" anche sotto "color-colour".
    *   Espansione della query: Ricerca estesa a "colour" quando la query contiene "color".

**8. Errori di Ortografia**

*   Soundex: Algoritmo per creare classi di equivalenza basate su euristiche fonetiche.

---

**I. Normalizzazione del Testo: Lemmatizzazione e Stemming**

    A. **Lemmatizzazione:**
        1.  Definizione: Processo di normalizzazione che riduce le forme flessionali di una parola alla sua forma base (lemma).
            *   Verbi: Riduzione all'infinito.
            *   Sostantivi: Riduzione al singolare.
        2.  Caratteristiche:
            *   Richiede un'analisi morfologica completa.
            *   Preserva i concetti principali (es. "the boy's cars are different colors" → "the boy car be different color").
        3.  Benefici:
            *   Generalmente modesti per il retrieval.
            *   Effetto collaterale dell'analisi sintattica.

    B. **Stemming:**
        1.  Definizione: Processo che riduce le parole alle loro radici (prefisso) prima dell'indicizzazione.
        2.  Caratteristiche:
            *   Intuitivo per i sostantivi, più complesso per i verbi.
            *   **Distrugge informazioni lessicali**.
            *   Spesso combinato con la rimozione delle *stop words*.
            *   Non ha senso combinarlo con la lemmattizzazione.
        3.  Dipendenza dalla lingua: Gli algoritmi di stemming sono specifici per ogni lingua.
        4.  Esempio: "automate(s)", "automatic" e "automation" → "automat".
        5.  Esempi di equivalenze create dallo stemming:
            *   "compressed" e "compression" → "compress".
            *   "compress" e "compress" → "compress".
        6.  Quando usarlo: Solo quando non è necessaria un'elaborazione semantica dei testi.

**II. Stemming di Porter**

    A. Descrizione: Algoritmo di stemming più comune per l'inglese.
    B. Funzionamento:
        1.  Itera su un insieme di regole pre-costruite.
        2.  Applica ripetutamente riduzioni basate sul *longest match*.
    C. Notazione: Descrive le parole come $[c](vc)m[v]$
        *   `c`: Consonante.
        *   `v`: Vocale.
        *   `m`: "Misura" (ripetizioni di `vc`).
    D. Fasi: Composto da 5 fasi, applicate in sequenza.
        *   Ogni fase ha un insieme di regole basate su condizioni e sostituzioni di suffissi (es. `sses` → `ss`, `ed` → (vuoto se preceduto da vocale)).
    E. Esempio di applicazione: `GENERALIZATIONS` → `GENERALIZATION` → `GENERALIZE` → `GENERAL` → `GENER`.
    F. Condizioni:
        *   `*v*`: Lo stem contiene una vocale.
        *   `*o`: Lo stem termina in `cvc`, dove la seconda `c` non è W, X o Y (es. -WIL, -HOP).

**III. Vantaggi e Svantaggi dello Stemming**

    A. Vantaggi: Può migliorare le prestazioni del retrieval.
    B. Svantaggi:
        1.  Risultati contrastanti.
        2.  L'efficacia dipende dal vocabolario specifico.
        3.  Può portare alla perdita di sottili distinzioni semantiche.
        4.  Per l'inglese: Migliora il recall ma danneggia la precisione per alcune query.

---

## Schema Riassuntivo: Algoritmi di Stemming

**1. Introduzione allo Stemming**

   *   Processo di riduzione delle parole alla loro radice (stem).
   *   Utile per l'analisi del testo e il recupero dell'informazione.
   *   Esempio: "operative" → "oper"

**2. Efficacia dello Stemming in Diverse Lingue**

   *   Efficace per lingue come spagnolo, tedesco e finlandese.
   *   L'efficacia varia a seconda della lingua.

**3. Algoritmi di Stemming**

   *   Prestazioni simili tra i vari algoritmi.

**4. Esempio di Algoritmo: Stemmer di Lovins**

   *   **Caratteristiche:**
        *   Passaggio singolo.
        *   Rimozione del suffisso più lungo.
        *   Utilizza circa 250 regole.

---

Ok, ecco uno schema riassuntivo conciso basato sul testo fornito:

**I. Algoritmi di Stemming**

    A. Paice/Husk Stemmer
    B. Snowball Stemmer

---
