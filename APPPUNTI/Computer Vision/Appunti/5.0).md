![[5.Reti neurali_parte1.pdf#page=44&rect=45,33,831,514|5.Reti neurali_parte1, p.44]]

![[5.Reti neurali_parte1.pdf#page=45&rect=51,53,617,521|5.Reti neurali_parte1, p.45]]
operazioni sui tensori: operazioni punto o punto o matriciali
noi vogliamo ottimizzare questi tensori

![[5.Reti neurali_parte1.pdf#page=46&rect=47,19,633,490|5.Reti neurali_parte1, p.46]]
gradiente:derivata output rispetto ogni dimnenione dell input
![[5.Reti neurali_parte1.pdf#page=47&rect=54,12,609,495|5.Reti neurali_parte1, p.47]]
![[5.Reti neurali_parte1.pdf#page=50&rect=54,47,718,510|5.Reti neurali_parte1, p.50]]
dovremmo avere un framework che permette di compiere queste operazioni

la propagazione della derivata viene fatta con la propagazione del calcolo vettoriale




## Loss
$l=\sum_{i} Cost(y_{i},\hat{y_{i}}(\theta))$

questa funzione è una componente del grafo di computazione. deve essere derivabile. 
![[5.Reti neurali_parte1.pdf#page=66&rect=46,37,802,494|5.Reti neurali_parte1, p.66]]

costo: concetto generale. ci sono diversi modi per esprimerla: l'importante è che sia derivabile

$\theta^*=\arg \min_{\theta} \ l(\theta)$


![[5.Reti neurali_parte1.pdf#page=69&rect=42,40,806,523|5.Reti neurali_parte1, p.69]]
lo aggiustiamo combinando i vari gradienti
![[5.Reti neurali_parte1.pdf#page=70&rect=61,59,652,408|5.Reti neurali_parte1, p.70]]
vi sono diverse varianti che pesano il gradiente in qualche modo
la più famosa è adam
