# Output processing per: 9.1) Funzioni di attivazione in reti neurali

## Metodo di splitting: headers
## Prompt utilizzati (1):
- TMP

---

## Chunk 1/3

### Risultato da: TMP
| Metodo/Funzione | Descrizione | Parametri | Output |
|---|---|---|---|
| `nn.Sigmoid()` | Applica la funzione sigmoide ad un tensore. | Tensore di input | Tensore con la funzione sigmoide applicata |
| `nn.Tanh()` | Applica la funzione tangente iperbolica ad un tensore. | Tensore di input | Tensore con la funzione tangente iperbolica applicata |
| `F.leaky_relu(x, .1)` | Applica la funzione Leaky ReLU ad un tensore. | Tensore di input `x`, parametro `negative_slope` (default 0.1) | Tensore con la funzione Leaky ReLU applicata |
| `plt.plot()` | Crea un grafico. | Dati x e y, opzioni di stile (colore, label, etc.) | Grafico visualizzato |
| `plt.title()` | Imposta il titolo di un grafico. | Stringa con il titolo | Grafico con titolo modificato |
| `plt.show()` | Mostra il grafico. | Nessuno | Grafico visualizzato |
| `plt.axhline()` | Aggiunge una linea orizzontale al grafico. | Valore y, opzioni di stile | Grafico modificato |
| `plt.axvline()` | Aggiunge una linea verticale al grafico. | Valore x, opzioni di stile | Grafico modificato |
| `torch.tensor()` | Crea un tensore PyTorch. | Dati di input, opzioni (es. `requires_grad=True`) | Tensore PyTorch |
| `np.linspace()` | Crea un array NumPy con valori equispaziati. | Valore iniziale, valore finale, numero di punti | Array NumPy |
| `y.backward()` | Calcola il gradiente di `y` rispetto alle variabili che richiedono il calcolo del gradiente (`requires_grad=True`). | Tensore opzionale per la ponderazione del gradiente (default: `torch.ones_like(y)`) | Aggiorna i gradienti delle variabili |
| `x.grad.zero_()` | Azzera i gradienti di `x`. | Nessuno | Gradienti di `x` azzerati |
| `x.detach().numpy()` | Detach il tensore PyTorch dal grafo computazionale e lo converte in un array NumPy. | Nessuno | Array NumPy |
| `torch.ones()` | Crea un tensore PyTorch riempito con uni. | Forma del tensore | Tensore PyTorch |


**Nota:**  Il codice include anche l'importazione di diverse librerie e l'utilizzo di funzioni di queste librerie (es. `mp_image.imread`, `ListedColormap`, `LinearSegmentedColormap`, `cm.get_cmap`), ma non sono state incluse nella tabella perché non sono definite esplicitamente come funzioni o metodi nel codice fornito.  Inoltre, alcune funzioni di `matplotlib` sono usate implicitamente nel codice per la visualizzazione dei grafici (es. impostazione degli assi, legende, etc.).


---

## Chunk 2/3

### Risultato da: TMP
## Metodi e Funzioni del Codice Python

| Nome della Funzione/Metodo | Descrizione | Parametri | Output |
|---|---|---|---|
| `transforms.ToTensor()` | Converte un'immagine in un tensore PyTorch. | Immagine | Tensore PyTorch |
| `__init__(self, input_size)` | Costruttore della classe `LeNet`. Inizializza i layers della rete neurale. | `input_size`: dimensione dell'input (larghezza/altezza dell'immagine). | Nessuno (inizializza gli attributi della classe). |
| `forward(self, x)` | Metodo della classe `LeNet`. Definisce il flusso di dati in avanti attraverso la rete. | `x`: tensore di input. | Tensore di output (probabilità di appartenenza alle classi). |
| `nn.Conv2d(in_channels, out_channels, kernel_size)` | Layer convoluzionale 2D. | `in_channels`: numero di canali di input, `out_channels`: numero di canali di output, `kernel_size`: dimensione del kernel. | Tensore con i risultati della convoluzione. |
| `nn.Tanh()` | Funzione di attivazione tangente iperbolica. | Tensore di input. | Tensore con i risultati dell'applicazione della funzione Tanh. |
| `nn.AvgPool2d(kernel_size, stride)` | Layer di pooling medio 2D. | `kernel_size`: dimensione del kernel, `stride`: passo del pooling. | Tensore con i risultati del pooling. |
| `nn.Linear(in_features, out_features)` | Layer lineare (completamente connesso). | `in_features`: numero di features di input, `out_features`: numero di features di output. | Tensore con i risultati della trasformazione lineare. |
| `F.log_softmax(input, dim)` | Applica la funzione softmax logaritmica lungo una dimensione specificata. | `input`: tensore di input, `dim`: dimensione lungo cui applicare la softmax. | Tensore con le probabilità logaritmiche. |
| `torch.optim.Adam(params, lr)` | Ottimizzatore Adam. | `params`: parametri del modello da ottimizzare, `lr`: learning rate. | Oggetto ottimizzatore. |
| `nn.CrossEntropyLoss()` | Funzione di perdita entropia incrociata. | Nessuno (prende in input i valori previsti e le etichette). | Valore della perdita. |
| `train(epoch, model, criterion, optimizer, reshape)` | Funzione per addestrare il modello. | `epoch`: numero dell'epoca, `model`: modello da addestrare, `criterion`: funzione di perdita, `optimizer`: ottimizzatore, `reshape`: flag per il reshape dell'input. | Nessuno (aggiorna i pesi del modello). |
| `test(model, criterion, reshape)` | Funzione per testare il modello. | `model`: modello da testare, `criterion`: funzione di perdita, `reshape`: flag per il reshape dell'input. | Perdita e accuratezza. |
| `plt.subplot(nrows, ncols, index)` | Crea un subplot in una figura. | `nrows`: numero di righe, `ncols`: numero di colonne, `index`: indice del subplot. | Nessuno (crea il subplot). |
| `plt.imshow(X, cmap, interpolation)` | Visualizza un'immagine. | `X`: immagine da visualizzare, `cmap`: mappa di colori, `interpolation`: tipo di interpolazione. | Nessuno (visualizza l'immagine). |
| `plt.title(s)` | Imposta il titolo di un subplot. | `s`: titolo. | Nessuno (imposta il titolo). |
| `plt.axis('off')` | Disattiva gli assi di un subplot. | Nessuno. | Nessuno (disattiva gli assi). |
| `plt.tight_layout()` | Aggiusta gli spazi tra i subplot. | Nessuno. | Nessuno (aggiusta gli spazi). |
| `extractor_1`, `extractor_2` (lambda functions) | Funzioni lambda per estrarre le feature map da specifici layers della rete LeNet. | Immagine di input. | Feature map estratte. |


**Nota:**  Il codice per `train` e `test` non è stato incluso nel dettaglio, ma la tabella riporta le informazioni generali basate sulla descrizione fornita.  Alcune funzioni di `matplotlib.pyplot` (`plt`) sono state incluse per completezza.


---

## Chunk 3/3

### Risultato da: TMP
Il codice fornito nel testo mostra solo l'utilizzo di una singola funzione:

| Metodo/Funzione | Descrizione | Parametri | Output |
|---|---|---|---|
| `torchvision.datasets.CIFAR10` | Carica il dataset CIFAR-10. | `root` (stringa, percorso di salvataggio), `train` (booleano, indica se caricare il training set o il validation set), `download` (booleano, indica se scaricare il dataset se non presente), `transform` (oggetto di trasformazione, applicato alle immagini) | Oggetto dataset CIFAR-10 (contenente le immagini e le relative etichette). |


Il testo menziona altre funzioni e metodi che sarebbero necessari per completare l'esercizio (definizione della rete LeNet, training, valutazione), ma non fornisce il codice per queste.  Queste includerebbero:

* **Funzioni per la definizione dell'architettura di LeNet:**  Queste sarebbero funzioni PyTorch per creare i layer convoluzionali, i layer di pooling, i layer fully connected, ecc.,  componendo l'architettura della rete LeNet.  Non c'è un nome specifico per queste funzioni, in quanto dipendono dalla implementazione.
* **`nn.CrossEntropyLoss()` (o simili):** Funzione di loss per la classificazione.  Parametri:  nessuno o parametri specifici a seconda dell'implementazione. Output: valore di loss scalare.
* **`torch.optim.Adam()` (o `torch.optim.SGD()`):** Funzioni per creare l'ottimizzatore. Parametri: parametri del modello, learning rate, ecc. Output: oggetto ottimizzatore.
* **Funzioni per il training loop:**  Queste funzioni iterano sul dataset, eseguono la forward pass, calcolano la loss, eseguono la backpropagation e aggiornano i pesi.  Non c'è un nome specifico per queste funzioni, in quanto dipendono dall'implementazione.
* **Funzioni per la valutazione del modello:**  Queste funzioni calcolano metriche di performance (es. accuratezza) sul dataset di validazione.  Non c'è un nome specifico per queste funzioni, in quanto dipendono dall'implementazione.


La tabella sopra include solo la funzione esplicitamente utilizzata nel codice fornito. Le altre sono menzionate come necessarie ma non definite nel testo.


---

