

Questo documento presenta le implementazioni PyTorch di alcune principali Deep Neural Networks (DNN): AlexNet, VGG, GoogleNet e ResNet.  Analizzeremo in dettaglio AlexNet e VGG.

## AlexNet

### Funzioni ausiliarie

Prima di analizzare l'architettura di AlexNet, esaminiamo le funzioni ausiliarie definite:

```python
def count_parameters(model):
  return sum(p.numel() for p in model.parameters() if p.requires_grad)
```

Questa funzione `count_parameters` conta il numero di parametri addestrabili in un modello PyTorch.  Essa itera sui parametri del modello (`model.parameters()`) e somma il numero di elementi (`p.numel()`) di ogni parametro che richiede gradiente (`p.requires_grad`), ovvero i parametri che vengono aggiornati durante l'addestramento.  Restituisce il numero totale di parametri addestrabili.

```python
def test(net, img_size=32):
  x = torch.randn(1, 3, img_size, img_size)
  y = net(x)
  print("Input: {}; Output: {}; N. of params: {:,}".format(list(x.size()), list(y.size()), count_parameters(net)))
```

La funzione `test` esegue un test rapido di una rete neurale (`net`). Crea un tensore di input casuale `x` di dimensione (1, 3, `img_size`, `img_size`), rappresentante un singolo esempio di immagine a 3 canali con dimensione specificata da `img_size`.  Passa l'input alla rete (`net(x)`) ottenendo l'output `y`. Infine, stampa le dimensioni dell'input, le dimensioni dell'output e il numero di parametri della rete, utilizzando la funzione `count_parameters`.


### Local Response Normalization (LRN) Layer

```python
class LRN(nn.Module):
  # ... (inizializzazione) ...
  def forward(self, x):
    if self.ACROSS_CHANNELS:
      div = x.pow(2).unsqueeze(1)
      div = self.average(div).squeeze(1)
    else:
      div = x.pow(2)
      div = self.average(div)
    div = div.mul(self.alpha).add(1.0).pow(self.beta)
    x = x.div(div)
    return x
```

La classe `LRN` implementa il layer di normalizzazione di risposta locale.  Il metodo `forward` esegue la normalizzazione.  Se `ACROSS_CHANNELS` è `True`, la normalizzazione viene eseguita attraverso i canali, altrimenti all'interno di ogni canale.  Il codice calcola la norma locale di potenza 2, la scala con `alpha` e `beta`, e normalizza l'input `x` dividendo per questa norma.


### Architettura AlexNet

```python
class AlexNet(nn.Module):
  # ... (inizializzazione dei layer) ...
  def forward(self, x):
    x = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))
    print(x.size())
    x = x.view(-1, 6*6*256)
    x = self.layer8(self.layer7(self.layer6(x)))
    return x
```

La classe `AlexNet` definisce l'architettura della rete.  Il metodo `forward` descrive il flusso del dato attraverso i diversi layer.  Si nota una sequenza di layer convoluzionali (`nn.Conv2d`), funzioni di attivazione ReLU (`nn.ReLU`), layer di max pooling (`nn.MaxPool2d`) e layer LRN.  Infine, il dato viene appiattito (`x.view(-1, 6*6*256)`) prima di passare attraverso i layer completamente connessi (`nn.Linear`) per la classificazione.


```python
net = AlexNet()
test(net, 227)
```

Questo codice crea un'istanza di `AlexNet` e la testa usando la funzione `test` con immagini di dimensione 227x227. L'output mostra le dimensioni dell'input e dell'output, e il numero di parametri della rete.


```
torch.Size([1, 256, 6, 6])
Input: [1, 3, 227, 227]; Output: [1, 1000]; N. of params: 62,071,144
```


## VGG

### Architettura VGG

```python
class VGG(nn.Module):
    # ... (inizializzazione) ...
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
    # ... (inizializzazione dei pesi) ...
```

La classe `VGG` definisce l'architettura VGG. Il metodo `forward` passa l'input attraverso i layer di features (`self.features`), appiattisce il risultato e lo passa attraverso i layer di classificazione (`self.classifier`).  La funzione `make_layers` crea i layer di features in base alla configurazione `cfg`.

```python
def make_layers(cfg, batch_norm=False):
    layers = []
    in_channels = 3
    for v in cfg:
        if v == 'M':
            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
        else:
            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
            if batch_norm:
                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]
            else:
                layers += [conv2d, nn.ReLU(inplace=True)]
            in_channels = v
    return nn.Sequential(*layers)
```

`make_layers` costruisce una sequenza di layer convoluzionali e di max pooling basandosi sulla configurazione `cfg`.  `cfg` è un dizionario che specifica l'architettura (es. 'A', 'B', 'D', 'E').  `batch_norm` indica se includere la normalizzazione batch.


```python
vgg16 = VGG(make_layers(cfg['D']))
test(vgg16, 224)
```

Questo codice crea un'istanza di VGG16 (usando la configurazione 'D') e la testa con immagini di dimensione 224x224.

```
Input: [1, 3, 224, 224]; Output: [1, 1000]; N. of params: 138,357,544
```

Questo output mostra le dimensioni dell'input e dell'output, e il numero di parametri di VGG16.  Si noti che VGG16 ha un numero significativamente maggiore di parametri rispetto ad AlexNet.


In sintesi, questo notebook fornisce implementazioni PyTorch di AlexNet e VGG, illustrando le loro architetture e mostrando come calcolare il numero di parametri.  L'uso di funzioni ausiliarie come `count_parameters` e `test` rende il codice più modulare e leggibile.  L'analisi dettagliata dei metodi `forward` di ciascuna rete evidenzia il flusso di dati e le operazioni eseguite in ogni layer.


Questo documento presenta le implementazioni di due architetture di reti neurali convoluzionali (CNN) popolari: GoogleNet e ResNet, utilizzando la libreria PyTorch.  Analizziamo separatamente le due architetture.

## GoogleNet

La GoogleNet è implementata attraverso due classi principali: `Inception` e `GoogLeNet`.

**1. Classe `Inception`:**

Questa classe implementa il modulo Inception, un blocco fondamentale dell'architettura GoogleNet.  Il modulo Inception concatena i risultati di diverse convoluzioni con dimensioni di kernel diverse (1x1, 3x3, 5x5) e un ramo di max pooling, aumentando la capacità del modello di estrarre features a diverse scale.

```python
class Inception(nn.Module):
    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):
        # ... (inizializzazione dei layers) ...
    def forward(self, x):
        y1 = self.b1(x) # 1x1 conv branch
        y2 = self.b2(x) # 1x1 conv -> 3x3 conv branch
        y3 = self.b3(x) # 1x1 conv -> 5x5 conv branch
        y4 = self.b4(x) # 3x3 pool -> 1x1 conv branch
        return torch.cat([y1,y2,y3,y4], 1) # Concatenazione dei risultati
```

* **`__init__`:** Il costruttore inizializza quattro rami (`b1`, `b2`, `b3`, `b4`) ognuno dei quali è una sequenza di strati convoluzionali e di attivazione ReLU.  I parametri in ingresso definiscono il numero di canali in ingresso (`in_planes`) e il numero di filtri per ogni ramo.  Ad esempio, `n1x1` specifica il numero di filtri nella convoluzione 1x1 del primo ramo.

* **`forward`:** Questo metodo esegue la propagazione in avanti.  Ogni ramo elabora l'input `x` in parallelo, e i risultati vengono concatenati lungo la dimensione dei canali (asse 1) utilizzando `torch.cat`.  Questo permette al modello di imparare features da diverse scale spaziali.


**2. Classe `GoogLeNet`:**

Questa classe implementa l'intera architettura GoogleNet, concatenando diversi moduli Inception e altri strati convoluzionali e di pooling.

```python
class GoogLeNet(nn.Module):
    def __init__(self):
        # ... (inizializzazione dei layers, inclusi diversi moduli Inception) ...
    def forward(self, x):
        out = self.pre_layers(x) # Livelli iniziali
        # ... (propagazione attraverso i moduli Inception e altri layers) ...
        out = self.linear(out) # Livello completamente connesso
        return out
```

* **`__init__`:** Il costruttore inizializza i livelli iniziali (`pre_layers`), una serie di moduli Inception (`a3`, `b3`, `a4`, ecc.), un livello di max pooling, un livello di average pooling, un dropout layer e un livello completamente connesso (`linear`).

* **`forward`:** Questo metodo esegue la propagazione in avanti attraverso l'intera rete.  L'input `x` viene elaborato sequenzialmente attraverso i diversi strati, fino a produrre l'output finale.


## ResNet

ResNet utilizza il concetto di "residual connections" per facilitare l'addestramento di reti profonde.

**1. Classe `ResidualBlock`:**

Questo blocco costituisce l'unità fondamentale di ResNet.  Implementa una connessione residuale, aggiungendo l'input al risultato della funzione non lineare.

```python
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, bn_channels, stride=1, bottleneck=False):
        # ... (inizializzazione dei layers) ...
    def forward(self, x):
        out = self.block(x)
        out += self.shortcut(x) # Connessione residuale
        out = F.relu(out)
        return out
```

* **`__init__`:** Il costruttore inizializza il blocco residuale.  `bottleneck` indica se utilizzare una versione "bottleneck" del blocco (con tre convoluzioni invece di due).  `in_channels` e `bn_channels` specificano il numero di canali in ingresso e nel blocco.

* **`forward`:** Questo metodo esegue la propagazione in avanti.  L'input `x` viene elaborato attraverso `self.block`, e il risultato viene sommato all'input originale tramite una connessione di shortcut (`self.shortcut`).  Questa somma viene poi passata attraverso una funzione ReLU.


**2. Classe `ResNet`:**

Questa classe implementa l'intera architettura ResNet.

```python
class ResNet(nn.Module):
    def __init__(self, layers, bottleneck=False):
        # ... (inizializzazione dei layers) ...
    def _make_layer(self, out_channels, blocks, stride=1):
        # ... (crea una sequenza di blocchi residuali) ...
    def forward(self, x):
        # ... (propagazione attraverso i layers) ...
        return x
```

* **`__init__`:** Il costruttore inizializza i livelli iniziali (`conv1`) e una serie di blocchi residuali (`conv2_x`, `conv3_x`, ecc.) utilizzando il metodo `_make_layer`.

* **`_make_layer`:** Questo metodo crea una sequenza di blocchi residuali.

* **`forward`:** Questo metodo esegue la propagazione in avanti attraverso l'intera rete.


Infine, il codice mostra esempi di creazione e test delle reti (`net = GoogLeNet()`, `test(net,224)`,  `net = ResNet([3,4,6,3],bottleneck = False)`, `test(net,224)`),  con indicazione dell'input, dell'output e del numero di parametri.  La funzione `test` non è definita nel codice fornito, ma si presume che sia una funzione di utilità per testare le reti.  Il codice per ResNet include anche una rappresentazione parziale della rete ad albero, mostrando la struttura dei suoi livelli.


Nessuna immagine è presente nel testo fornito.


## Spiegazione del codice Python per il training di un modello di riconoscimento di immagini

Questo documento spiega il codice Python fornito, focalizzandosi sui metodi e sulle operazioni principali. Il codice si occupa di preparare un dataset di immagini (Caltech101) per l'addestramento di un modello di riconoscimento di oggetti.

### 1. Importazione delle librerie e inizializzazione

```python
# PyTorch import torch from torchvision import transforms, datasets, models from torch import optim, cuda from torch.utils.data import DataLoader, sampler import torch.nn as nn import warnings warnings.filterwarnings('ignore', category=FutureWarning) # Data science tools import numpy as np import pandas as pd import os # Image manipulations from PIL import Image # Useful for examining network from torchsummary import summary # Timing utility from timeit import default_timer as timer # Visualizations import matplotlib.pyplot as plt %matplotlib inline plt.rcParams['font.size'] = 14 DATASET_DIR = 'image/101_ObjectCategories' DATASET_PREPROCESSED = 'image/dataset' os.environ['CUDA_VISIBLE_DEVICES'] = '0' if torch.cuda.is_available(): device = torch.device("cuda") print('Run on GPU') else: device = torch.device("cpu")
```

Questo blocco importa le librerie necessarie: PyTorch per la parte di deep learning, NumPy e Pandas per la manipolazione dei dati, scikit-learn per la creazione del dataset, PIL per la gestione delle immagini, Matplotlib per la visualizzazione e altre librerie utili per la gestione del tempo e la visualizzazione della rete.  Vengono inoltre definite le variabili `DATASET_DIR` e `DATASET_PREPROCESSED` che indicano rispettivamente la directory del dataset originale e quella dove verrà creato il dataset pre-processato. Infine, viene verificata la disponibilità della GPU e impostata la variabile `device` di conseguenza.


### 2. Esplorazione dei dati

```python
categories = {} for folder in os.listdir(DATASET_DIR): categories[folder] = os.listdir(os.path.join(DATASET_DIR, folder)) cnt = [len(x) for x in categories.values()] minNumberOfImages = min(cnt) print(f'Total classes {len(categories)} with total images {sum(cnt)}, min = {minNumberOfImages}, max = {max(cnt)}') items = sorted([(key, len(categories[key])) for key in categories], key=lambda c: -c[1]) x = [k for k, v in items] y = [v for k, v in items] fig = plt.figure(figsize=(20, 8)) plt.bar(x, y) plt.xticks(rotation=80) plt.ylabel('Count') plt.title('Images by Category');
```

Questo codice esplora il dataset `Caltech101`.  Crea un dizionario `categories` dove le chiavi sono i nomi delle categorie e i valori sono le liste dei file immagine in ogni categoria.  Calcola poi il numero minimo e massimo di immagini per categoria e visualizza un istogramma che mostra la distribuzione del numero di immagini per ogni categoria.  L'immagine risultante è mostrata qui sotto:

![png](7.Network_architectures_extended_23_1.png)


Come si può notare dall'istogramma e dal messaggio di output (`Total classes 102 with total images 9145, min = 31, max = 800`), la distribuzione delle immagini tra le classi è sbilanciata.


### 3. Costruzione dei set di training, validation e test

```python
import shutil train_images = [] valid_images = [] test_images = [] i = minNumberOfImages // 2 j = i + minNumberOfImages // 4 for k, v in categories.items(): sample = np.random.choice(v, minNumberOfImages, replace=False) train_images.extend([os.path.join(k, x) for x in sample[:i]]) valid_images.extend([os.path.join(k, x) for x in sample[i:j]]) test_images.extend([os.path.join(k, x) for x in sample[j:]]) # build new dataset if os.path.exists(DATASET_PREPROCESSED): shutil.rmtree(DATASET_PREPROCESSED) os.mkdir(DATASET_PREPROCESSED) for dname, sourceImages in zip(('train', 'valid', 'test'), (train_images, valid_images, test_images)): dst_directory = os.path.join(DATASET_PREPROCESSED, dname) os.mkdir(dst_directory) for fname in sourceImages: fsrc = os.path.join(DATASET_DIR, fname) fdst = os.path.join(dst_directory, fname) if not os.path.exists(os.path.dirname(fdst)): os.mkdir(os.path.dirname(fdst)) os.symlink(os.path.abspath(fsrc), os.path.abspath(fdst)) traindir = os.path.join(DATASET_PREPROCESSED, 'train') validdir = os.path.join(DATASET_PREPROCESSED, 'valid') testdir = os.path.join(DATASET_PREPROCESSED, 'test')
```

Questo codice crea i set di training, validation e test. Per ogni categoria, vengono selezionate casualmente `minNumberOfImages` immagini. Queste immagini vengono poi suddivise in modo da avere il 50% per il training, il 25% per la validation e il 25% per il test.  Il codice crea poi una nuova directory (`DATASET_PREPROCESSED`) e crea dei link simbolici alle immagini originali, organizzate nelle sottodirectory `train`, `valid` e `test`.  Questo evita di copiare i file, risparmiando spazio su disco.


### 4. Esempio di visualizzazione di un'immagine

```python
# Example image image = Image.open(fdst) plt.figure(figsize=(6, 6)) plt.imshow(image) plt.axis('off') plt.show();
```

Questo codice apre e visualizza un'immagine dal dataset pre-processato per verificare la corretta creazione dei set di dati. L'immagine risultante è mostrata qui sotto:

![png](7.Network_architectures_extended_26_0.png)


In sintesi, il codice prepara il dataset Caltech101 per l'addestramento di un modello di classificazione di immagini, gestendo la distribuzione sbilanciata delle classi e creando set di training, validation e test bilanciati.  Il codice utilizza link simbolici per evitare la copia dei file, ottimizzando l'utilizzo dello spazio su disco.  Manca la parte di definizione e training del modello di rete neurale, che presumibilmente segue questo codice di pre-processing.


## Preprocessing e Data Augmentation

Il testo descrive un processo di elaborazione di immagini per l'addestramento di un classificatore di immagini, utilizzando la libreria PyTorch.  Inizia con la fase di preprocessing e data augmentation.

### Preprocessing

Il preprocessing mira a normalizzare le immagini per migliorare le prestazioni del modello.  Le immagini vengono ridimensionate a 224x224 pixel (standard per VGG) e i canali colore vengono normalizzati sottraendo la media e dividendo per la deviazione standard.  Questi valori (media e deviazione standard) sono specifici per ImageNet: `[0.485, 0.456, 0.406]` per la media e `[0.229, 0.224, 0.225]` per la deviazione standard.

### Data Augmentation

Dato il numero limitato di immagini nel dataset, si utilizza il data augmentation per aumentare la varietà dei dati di addestramento.  Questo viene fatto applicando trasformazioni casuali alle immagini durante l'addestramento.

Il codice seguente definisce le trasformazioni per l'insieme di addestramento (`train`), convalida (`val`) e test (`test`).

```python
image_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(),
        transforms.RandomHorizontalFlip(),
        transforms.CenterCrop(size=224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(size=256),
        transforms.CenterCrop(size=224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([ # Identical to 'val'
        transforms.Resize(size=256),
        transforms.CenterCrop(size=224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}
```

`transforms.Compose` applica una sequenza di trasformazioni.  Le trasformazioni per l'insieme di addestramento includono: `RandomResizedCrop`, `RandomRotation`, `ColorJitter`, `RandomHorizontalFlip`, e `CenterCrop`.  Le trasformazioni per gli insiemi di convalida e test sono più semplici, includendo solo `Resize` e `CenterCrop`.  Infine, tutte le trasformazioni includono `ToTensor` (converte l'immagine in un tensore PyTorch) e `Normalize` (normalizza i canali colore).


## Visualizzazione di un Tensore

La funzione `imshow_tensor` visualizza un tensore di immagini.

```python
def imshow_tensor(image, ax=None, title=None):
    """Imshow for Tensor."""
    # ... (codice per la visualizzazione dell'immagine) ...
```

Questa funzione prende un tensore di immagine come input, inverte i passaggi di preprocessing (denormalizzazione), e visualizza l'immagine usando `matplotlib.pyplot.imshow`.


![png](7.Network_architectures_extended_30_0.png)

Questa immagine mostra un esempio di applicazione delle trasformazioni di data augmentation.


Il codice seguente applica le trasformazioni di addestramento ad un'immagine e visualizza il risultato:

```python
t = image_transforms['train']
plt.figure(figsize=(24, 24))
for i in range(16):
    ax = plt.subplot(4, 4, i + 1)
    _ = imshow_tensor(t(image), ax=ax)
plt.tight_layout();
```

![png](7.Network_architectures_extended_31_0.png)

Questa immagine mostra 16 immagini trasformate, evidenziando l'effetto del data augmentation.


## Caricamento dei Dataset e Dataloader

Il codice carica i dataset di immagini dalle cartelle `traindir`, `validdir` e `testdir` usando `datasets.ImageFolder`.  Poi crea i `DataLoader` per iterare sui dataset in batch.

```python
data = {
    'train': datasets.ImageFolder(root=traindir, transform=image_transforms['train']),
    'val': datasets.ImageFolder(root=validdir, transform=image_transforms['val']),
    'test': datasets.ImageFolder(root=testdir, transform=image_transforms['test'])
}

batch_size = 16
trainloader = DataLoader(data['train'], batch_size=batch_size, shuffle=True)
validloader = DataLoader(data['val'], batch_size=batch_size, shuffle=True)
testloader = DataLoader(data['test'], batch_size=batch_size, shuffle=True)
```

Il codice stampa il numero di classi nel dataset:

```python
print(trainloader.dataset.classes)
n_classes = len(categories)
print(f'There are {n_classes} different classes.')
len(data['train'].classes)
```

Questo mostra che ci sono 102 classi diverse.  La forma di un batch è (batch_size, color_channels, height, width), come mostrato dal codice seguente:

```python
trainiter = iter(trainloader)
features, labels = next(trainiter)
features.shape, labels.shape
```

(torch.Size([16, 3, 224, 224]), torch.Size([16]))


## Addestramento del Classificatore VGG16

Il codice addestra un classificatore VGG16 su questo dataset.

```python
vgg16 = VGG(make_layers(cfg['D']), num_classes=n_classes)
test(vgg16, 224)
model = vgg16.to(device)
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=0.003)
```

`VGG` è presumibilmente una classe definita altrove che implementa l'architettura VGG16.  `make_layers` è una funzione (non mostrata) che crea i layers della rete.  `num_classes` è il numero di classi nel dataset (102).  `nn.NLLLoss()` è la funzione di perdita, e `optim.Adam` è l'ottimizzatore.

Il codice di addestramento itera sugli epochs e sui batch, calcola la perdita, esegue il backpropagation e aggiorna i pesi del modello.  Include anche la valutazione del modello sull'insieme di convalida ad ogni `print_every` step.

```python
epochs = 100
steps = 0
running_loss = 0
print_every = 20
train_losses, test_losses = [], []
# ... (codice di addestramento) ...
```

L'output mostra la perdita di addestramento e convalida, e l'accuratezza di convalida ad ogni intervallo specificato.  Il codice include anche un blocco `try...except` per gestire le interruzioni da tastiera durante l'addestramento.  Le informazioni sull'input e output del modello VGG16 sono fornite: Input: [1, 3, 224, 224]; Output: [1, 102]; N. of params: 134,678,438.  Questo indica che il modello accetta immagini con 3 canali di colore (RGB), altezza e larghezza 224 pixel, e produce un output con 102 classi.  Il numero di parametri indica la complessità del modello.


## Analisi del codice Python per la valutazione di un modello di rete neurale

Questo documento analizza il codice Python fornito, focalizzandosi sui metodi definiti e sul loro utilizzo nella valutazione di un modello di rete neurale convoluzionale (CNN) addestrato con PyTorch.

**1. Salvataggio del modello:**

```python
torch.save(model, 'vgg16model.pth')
```

Questo codice salva il modello PyTorch addestrato (`model`) in un file chiamato `vgg16model.pth`.  `torch.save()` è una funzione di PyTorch che serializza il modello, permettendo di caricarlo e riutilizzarlo in seguito senza doverlo riaddestrare.

**2. Grafico delle perdite durante l'addestramento:**

```python
plt.plot(train_losses, label='Training loss')
plt.plot(test_losses, label='Validation loss')
plt.legend(frameon=False)
plt.show();
```

Questo frammento di codice, utilizzando la libreria Matplotlib, genera un grafico che visualizza le perdite durante l'addestramento (`train_losses`) e la validazione (`test_losses`).  `plt.plot()` disegna le curve, `plt.legend()` aggiunge una legenda e `plt.show()` visualizza il grafico.  Questo grafico è fondamentale per monitorare il processo di apprendimento e individuare eventuali problemi come overfitting o underfitting.

**3. Immagine della rete neurale:**

![png](7.Network_architectures_extended_42_0.png)

Questa immagine (non visualizzabile qui, ma presente nel testo originale) presumibilmente mostra l'architettura della rete neurale convoluzionale utilizzata.  Fornisce una rappresentazione visiva della struttura del modello, mostrando i diversi strati e le loro connessioni.

**4. Analisi dei risultati e valutazione sul test set:**

Questa sezione del codice si concentra sulla valutazione delle prestazioni del modello addestrato su un set di dati di test.

**4.1. `predict_on_image(img_tensor, real_class, classes_list, model, topk=5)`:**

```python
def predict_on_image(img_tensor, real_class, classes_list, model, topk=5):
    # ...
    img_tensor = img_tensor.view(1, 3, 224, 224)
    with torch.no_grad():
        model.eval()
        out = model(img_tensor)
        ps = torch.exp(out)
        topk, topclass = ps.topk(topk, dim=1)
        top_classes = [classes_list[class_] for class_ in topclass.cpu().numpy()[0]]
        top_p = topk.cpu().numpy()[0]
    return img_tensor.cpu().squeeze(), top_p, top_classes, real_class
```

Questo metodo effettua una predizione su una singola immagine (`img_tensor`).  Prima di tutto, rimodella l'immagine nella forma attesa dal modello (1, 3, 224, 224).  Poi, imposta il modello in modalità valutazione (`model.eval()`) disabilitando il dropout e la batch normalization.  Esegue la predizione (`model(img_tensor)`), calcola le probabilità (`torch.exp(out)`), trova le `topk` predizioni più probabili (`ps.topk()`) e restituisce l'immagine, le probabilità, le classi predette e la classe reale.

**4.2. `display_prediction(dataloader, idx, model, topk)`:**

```python
def display_prediction(dataloader, idx, model, topk):
    # ...
    img, ps, classes, y_obs = predict_on_image(img_tensor, real_class, dataloader.dataset.classes, model, topk)
    result = pd.DataFrame({'p': ps}, index=classes)
    # ...  (visualizzazione dell'immagine e del grafico a barre)
```

Questo metodo visualizza l'immagine e le predizioni del modello.  Utilizza `predict_on_image` per ottenere le predizioni, quindi crea un DataFrame Pandas per visualizzare le probabilità predette con un grafico a barre usando Matplotlib.

**4.3. `compute_accuracy(output, target, topk=(1, ))`:**

```python
def compute_accuracy(output, target, topk=(1, )):
    # ...
    _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))
    # ... (calcolo dell'accuratezza topk)
```

Questo metodo calcola l'accuratezza top-k del modello, confrontando le predizioni (`output`) con le etichette vere (`target`).  Trova le `topk` predizioni più probabili e calcola la percentuale di predizioni corrette per ogni valore di `k` in `topk`.

**4.4. `evaluate(model, test_loader, criterion, topk=(1, 5))`:**

```python
def evaluate(model, test_loader, criterion, topk=(1, 5)):
    # ...
    model.eval()
    with torch.no_grad():
        for data, targets in test_loader:
            data, targets = data.to(device), targets.to(device)
            out = model(data)
            # ... (calcolo delle perdite e dell'accuratezza per ogni esempio)
```

Questo metodo valuta le prestazioni del modello sul set di test (`test_loader`).  Iterando sul `test_loader`, esegue le predizioni, calcola le perdite usando `criterion` e l'accuratezza usando `compute_accuracy`.  Restituisce un DataFrame con i risultati.


In sintesi, il codice fornisce una pipeline completa per addestrare, salvare, e valutare un modello di rete neurale convoluzionale usando PyTorch.  Le funzioni definite offrono modularità e riutilizzabilità del codice, facilitando l'analisi delle prestazioni del modello.


## Analisi del codice Python per la classificazione di immagini

Questo documento analizza il codice Python fornito, focalizzandosi sui metodi e sulle tecniche utilizzate per la classificazione di immagini, includendo il *transfer learning* con un modello VGG16 pre-addestrato.

### 1. Valutazione del modello: la funzione `evaluate`

Il codice inizia con la definizione implicita (non esplicitamente dichiarata con `def`) di una funzione `evaluate`, utilizzata per valutare le prestazioni di un modello di classificazione su un dataset di test.  Non viene mostrato il codice completo della funzione, ma si può dedurre il suo funzionamento dai frammenti forniti:

```python
acc_results[i, :] = compute_accuracy(pred.unsqueeze(0), true.unsqueeze(0), topk)
classes.append(test_loader.dataset.classes[true.item()])
loss = criterion(pred.view(1, n_classes), true.view(1))
losses.append(loss.item())
```

* **`compute_accuracy(pred.unsqueeze(0), true.unsqueeze(0), topk)`:** Questa funzione (non mostrata) calcola l'accuratezza *top-k*.  `pred` rappresenta le predizioni del modello (probabilità per ogni classe), `true` è il valore vero (la classe corretta), e `topk` è una lista di valori interi (es. `[1, 5]`) che indicano le posizioni da considerare per il calcolo dell'accuratezza (top-1, top-5, ecc.).  `unsqueeze(0)` aggiunge una dimensione aggiuntiva ai tensori `pred` e `true`, probabilmente per compatibilità con la funzione `compute_accuracy`. La funzione restituisce l'accuratezza per ogni valore in `topk`.

* **`classes.append(test_loader.dataset.classes[true.item()])`:**  Questa riga aggiunge la classe vera alla lista `classes`.  `test_loader.dataset.classes` è un attributo del dataset che contiene i nomi delle classi, e `true.item()` estrae il valore intero (indice della classe) dal tensore `true`.

* **`loss = criterion(pred.view(1, n_classes), true.view(1))`:** Questa riga calcola la perdita (loss) utilizzando la funzione `criterion` (che è definita come `nn.NLLLoss()` più avanti nel codice).  `pred.view(1, n_classes)` e `true.view(1)` rimodellano i tensori `pred` e `true` per essere compatibili con la funzione di perdita.  `nn.NLLLoss()` è la Negative Log-Likelihood Loss, adatta per problemi di classificazione con output probabilistico.  `losses.append(loss.item())` aggiunge il valore della perdita alla lista `losses`.

La funzione `evaluate` quindi itera sulle immagini del dataset di test, calcola l'accuratezza top-k e la perdita per ogni immagine, e poi restituisce un DataFrame Pandas che riassume i risultati, raggruppati per classe e calcolando la media dell'accuratezza e della perdita per ogni classe.

```python
results = pd.DataFrame(acc_results, columns=[f'top{i}' for i in topk])
results['class'] = classes
results['loss'] = losses
results = results.groupby(classes).mean()
return results.reset_index().rename(columns={'index': 'class'})
```

Il risultato di `evaluate` è mostrato nella tabella seguente:

<div>
<style scoped>
.dataframe tbody tr th:only-of-type { vertical-align: middle; }
.dataframe tbody tr th { vertical-align: top; }
.dataframe thead th { text-align: right; }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>class</th>
<th>top1</th>
<th>top5</th>
<th>loss</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>BACKGROUND_Google</td>
<td>0.0</td>
<td>0.0</td>
<td>-1.982368e+27</td>
</tr>
<tr>
<th>1</th>
<td>Faces</td>
<td>0.0</td>
<td>0.0</td>
<td>-5.248622e+26</td>
</tr>
<tr>
<th>2</th>
<td>Faces_easy</td>
<td>0.0</td>
<td>0.0</td>
<td>-9.208511e+26</td>
</tr>
<tr>
<th>3</th>
<td>Leopards</td>
<td>0.0</td>
<td>0.0</td>
<td>-3.987136e+26</td>
</tr>
<tr>
<th>4</th>
<td>Motorbikes</td>
<td>0.0</td>
<td>0.0</td>
<td>-2.154838e+27</td>
</tr>
</tbody>
</table>
</div>

Un grafico a barre dell'accuratezza top-1 e top-5 è poi generato con:

```python
results[['top1', 'top5']].plot.bar(figsize=(20, 4));
```


### 2. Visualizzazione delle predizioni: `display_prediction`

Il codice include chiamate alla funzione `display_prediction` (non mostrata):

```python
display_prediction(testloader, 13, model, 5)
display_prediction(testloader, 43, model, 5)
display_prediction(testloader, 53, model, 5)
```

Si presume che questa funzione prenda come input il dataloader di test (`testloader`), l'indice di un'immagine, il modello e il numero di predizioni top-k da visualizzare.  Probabilmente visualizza l'immagine e le prime k predizioni del modello.

Le immagini generate da queste chiamate sono mostrate qui sotto:

![png](7.Network_architectures_extended_46_0.png)
![png](7.Network_architectures_extended_48_0.png)
![png](7.Network_architectures_extended_48_1.png)
![png](7.Network_architectures_extended_48_2.png)


### 3. Transfer Learning con VGG16

La sezione successiva del codice illustra l'utilizzo del *transfer learning* con un modello VGG16 pre-addestrato su ImageNet:

```python
from torchvision import models
modelPreTrained = models.vgg16(pretrained=True, progress=False)
```

Questo codice carica un modello VGG16 pre-addestrato (`pretrained=True`).  `progress=False` disabilita la barra di progresso durante il download.

Per evitare che i pesi dei layer convoluzionali vengano aggiornati durante l'addestramento, vengono "congelati":

```python
for param in modelPreTrained.parameters():
    param.requires_grad = False
```

Questo setta l'attributo `requires_grad` a `False` per tutti i parametri del modello, impedendo la loro ottimizzazione durante l'addestramento.

Il numero di parametri del modello congelato viene verificato con:

```python
test(modelPreTrained, 224)
```

(La funzione `test` non è mostrata, ma presumibilmente stampa il numero di parametri).  L'output indica 0 parametri addestrabili, confermando che i pesi sono congelati.

Infine, vengono aggiunti nuovi layer fully connected per adattare il modello al nuovo dataset:

```python
n_inputs = modelPreTrained.classifier[6].in_features
n_classes = len(trainloader.dataset.classes)
# ... (Codice per aggiungere i nuovi layer fully connected) ...
```

Questo codice estrae il numero di features in output dall'ultimo layer del classificatore originale (`n_inputs`) e il numero di classi nel nuovo dataset (`n_classes`).  Il codice mancante (indicato dai puntini) aggiunge nuovi layer fully connected con attivazione ReLU, dropout e log softmax, con `requires_grad=True` per permetterne l'addestramento.  La dimensione del primo layer fully connected è definita da `n_inputs`, mentre quella dell'ultimo layer è `n_classes`.


In sintesi, il codice dimostra un flusso di lavoro completo per la classificazione di immagini, dalla valutazione di un modello all'utilizzo del transfer learning con un modello pre-addestrato, mostrando le tecniche per congelare i layer e aggiungere nuovi layer fully connected per adattare il modello a un nuovo dataset.  La mancanza di alcuni snippet di codice rende l'analisi parziale, ma fornisce una comprensione generale del processo.


Questo documento descrive l'addestramento di un modello di classificazione di immagini basato su una rete neurale convoluzionale pre-addestrata (VGG16, probabilmente).  Analizziamo passo passo il codice e i risultati.

**1. Modifica del classificatore:**

Il codice inizia modificando il classificatore del modello pre-addestrato `modelPreTrained`.

```python
modelPreTrained.classifier[6] = nn.Sequential(
    nn.Linear(n_inputs, 256),
    nn.ReLU(),
    nn.Dropout(0.4),
    nn.Linear(256, n_classes),
    nn.LogSoftmax(dim=1)
)
```

Questo snippet sostituisce il sesto layer del classificatore esistente (`modelPreTrained.classifier[6]`).  Il nuovo classificatore è una sequenza (`nn.Sequential`) di:

* `nn.Linear(n_inputs, 256)`: Un layer lineare che mappa gli `n_inputs` (numero di features in ingresso dal precedente layer convoluzionale) a 256 neuroni.
* `nn.ReLU()`: Una funzione di attivazione ReLU (Rectified Linear Unit).
* `nn.Dropout(0.4)`: Un layer di dropout con probabilità di dropout del 40%, per prevenire l'overfitting.
* `nn.Linear(256, n_classes)`: Un altro layer lineare che mappa i 256 neuroni a `n_classes` (numero di classi nel dataset).
* `nn.LogSoftmax(dim=1)`: Applica la funzione LogSoftmax lungo la dimensione 1 (di solito le classi), producendo una distribuzione di probabilità sulle classi.

Il codice mostra poi la struttura del modello modificato:

```
Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Sequential(
        (0): Linear(in_features=4096, out_features=256, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.4, inplace=False)
        (3): Linear(in_features=256, out_features=102, bias=True)
        (4): LogSoftmax()
    )
)
```
Questo indica che il modello ha un totale di 1.075.046 parametri addestrabili.


**2. Test del modello:**

```python
test(modelPreTrained, 224)
```

Questa riga chiama una funzione `test` (non definita nel testo fornito) che probabilmente valuta le prestazioni del modello su un set di dati di test, usando immagini di dimensione 224x224.  L'output indica che l'input è un tensore di forma `[1, 3, 224, 224]` (1 immagine, 3 canali di colore, 224x224 pixel) e l'output è `[1, 102]` (1 immagine, 102 classi).


**3. Addestramento del modello:**

```python
modelPreTrained = modelPreTrained.to(device)
criterion = nn.NLLLoss()
optimizer = optim.Adam(modelPreTrained.parameters())
```

Questo codice sposta il modello sulla GPU (`device`), definisce la funzione di perdita come Negative Log-Likelihood Loss (`nn.NLLLoss()`, adatta per output LogSoftmax) e sceglie l'ottimizzatore Adam.

Il loop di addestramento itera per un numero di epoche (`epochs = 10`):

```python
for epoch in range(epochs):
    for inputs, labels in trainloader:
        # ... (logica di addestramento) ...
```

All'interno del loop, i dati vengono caricati da `trainloader`, processati, la perdita viene calcolata, la backpropagation viene eseguita e i pesi vengono aggiornati.  La perdita e l'accuratezza vengono monitorate e stampate ogni `print_every` step.  Il modello viene valutato sul `validloader` ad ogni iterazione per monitorare le prestazioni sulla validation set.

**4. Grafico delle perdite:**

```python
plt.plot(train_losses, label='Training loss')
plt.plot(test_losses, label='Validation loss')
plt.legend(frameon=False)
plt.show();
```

Questo codice genera un grafico che mostra l'andamento delle perdite di addestramento e validazione durante l'addestramento.  ![png](7.Network_architectures_extended_62_0.png) mostra un esempio di questo grafico.


**5. Valutazione del modello:**

```python
criterion = nn.NLLLoss()
results = evaluate(modelPreTrained, testloader, criterion)
results.head()
```

Questo codice utilizza una funzione `evaluate` (non definita nel testo) per valutare il modello sul `testloader` e calcola la perdita e le metriche di accuratezza (`top1`, `top5`).  La tabella seguente mostra i risultati:

<div>
<style scoped>
.dataframe tbody tr th:only-of-type {
  vertical-align: middle;
}
.dataframe tbody tr th {
  vertical-align: top;
}
.dataframe thead th {
  text-align: right;
}
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>class</th>
<th>top1</th>
<th>top5</th>
<th>loss</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>BACKGROUND_Google</td>
<td>50.000000</td>
<td>75.000000</td>
<td>2.561424</td>
</tr>
<tr>
<th>1</th>
<td>Faces</td>
<td>22.222222</td>
<td>100.000000</td>
<td>1.169563</td>
</tr>
<tr>
<th>2</th>
<td>Faces_easy</td>
<td>77.777778</td>
<td>100.000000</td>
<td>0.849270</td>
</tr>
<tr>
<th>3</th>
<td>Leopards</td>
<td>66.666667</td>
<td>88.888889</td>
<td>0.875035</td>
</tr>
<tr>
<th>4</th>
<td>Motorbikes</td>
<td>100.000000</td>
<td>100.000000</td>
<td>0.010580</td>
</tr>
</tbody>
</table>
</div>

Il grafico seguente visualizza `top1` e `top5` accuracy per ogni classe:

![png](7.Network_architectures_extended_65_0.png)


**6. Visualizzazione delle predizioni:**

```python
display_prediction(testloader, 13, modelPreTrained, 5)
display_prediction(testloader, 43, modelPreTrained, 5)
display_prediction(testloader, 53, modelPreTrained, 5)
```

Questo codice utilizza una funzione `display_prediction` (non definita nel testo) per visualizzare le predizioni del modello su tre immagini specifiche (indice 13, 43 e 53 nel `testloader`), mostrando le 5 classi con la probabilità più alta.  Le immagini seguenti mostrano i risultati:

![png](7.Network_architectures_extended_67_0.png)
![png](7.Network_architectures_extended_67_1.png)
![png](7.Network_architectures_extended_67_2.png)


In sintesi, il codice addestra un modello di classificazione di immagini modificando il classificatore di un modello pre-addestrato, monitora le prestazioni durante l'addestramento e visualizza i risultati.  Mancano alcune definizioni di funzioni (`test`, `evaluate`, `display_prediction`), ma la struttura generale del codice e il flusso di lavoro sono chiari.


Questo testo descrive come visualizzare le mappe di attivazione di una rete neurale convoluzionale pre-addestrata (in questo caso, un modello VGG) utilizzando la libreria `viztools-pytorch`.  L'obiettivo è comprendere quali *feature* vengono estratte da ogni livello della rete.

**1. Installazione della libreria:**

Il testo inizia indicando come installare la libreria necessaria: `pip install git+https://github.com/andrijdavid/viztools-pytorch.git`

**2. Funzione `viz_act_val`:**

```python
def viz_act_val(image, model, layer, kernel):
    # ... (asserzioni per validare gli input) ...
    layers = defuse_model(model) # Ottiene i layers del modello
    # ... (inserisce un layer Flatten se necessario) ...
    # ... (pre-elabora l'immagine e la sposta sulla GPU se disponibile) ...
    for tag, module in layers.items():
        image = module(image) # Fa passare l'immagine attraverso i layers
        if tag == layer:
            assert kernel < image.size()[1], "'kernel' out of bound"
            activation = image[0, kernel].cpu().data.numpy() # Ottiene l'attivazione del kernel specificato
            activation -= activation.min() # Normalizza l'attivazione
            activation /= activation.max()
            return activation
```

Questa funzione è il cuore del processo di visualizzazione.  Prende in input:

* `image`: un array NumPy rappresentante l'immagine di input.
* `model`: il modello PyTorch pre-addestrato.
* `layer`: il nome del layer di cui si vogliono visualizzare le attivazioni (es. 'Conv2d-1').
* `kernel`: l'indice del kernel (filtro) all'interno del layer.

Restituisce:

* `activation`: un array NumPy contenente i valori di attivazione del kernel specificato, normalizzati tra 0 e 1.

La funzione esegue una forward pass dell'immagine attraverso il modello fino al layer specificato.  Quindi, estrae le attivazioni del kernel richiesto, le normalizza e le restituisce.  Le asserzioni garantiscono che gli input siano validi e che il kernel specificato esista nel layer.  La funzione gestisce anche la presenza di una GPU.


**3. Visualizzazione delle attivazioni:**

Il codice successivo mostra come utilizzare `viz_act_val` per visualizzare le mappe di attivazione di diversi layer.

**3.1. Visualizzazione del layer 'Conv2d-1':**

```python
# ... (caricamento dell'immagine) ...
layer_name = 'Conv2d-1'
filters = 64
result = []
for layerID in range(filters):
    activation_value = viz_act_val(image_array, modelPreTrained, layer_name, layerID)
    result.append(activation_value)
result_image = combine_images(result) # Combina le attivazioni di tutti i kernel
plt.imshow(result_image, cmap=plt.cm.RdBu_r) # Mostra l'immagine risultante
plt.show()
```

Questo blocco di codice itera attraverso tutti i 64 filtri del primo layer convoluzionale ('Conv2d-1'), chiama `viz_act_val` per ottenere le mappe di attivazione di ogni filtro, le combina in una singola immagine usando `combine_images` (funzione non mostrata nel codice ma presumibilmente parte di `viztools-pytorch`) e la visualizza.  `activation shape (224, 224)` indica la dimensione delle mappe di attivazione.

![png](7.Network_architectures_extended_71_1.png)
![png](7.Network_architectures_extended_71_2.png)  Queste immagini mostrano il risultato della visualizzazione del layer 'Conv2d-1'.


**3.2. Visualizzazione del layer 'ReLU-1':**

Un codice simile viene utilizzato per visualizzare le attivazioni del layer 'ReLU-1':

```python
layer_name = 'ReLU-1'
# ... (codice simile al precedente) ...
```

![png](7.Network_architectures_extended_72_2.png) Questa immagine mostra il risultato per il layer 'ReLU-1'.


**3.3. Visualizzazione di un singolo filtro:**

Infine, viene mostrato come visualizzare un singolo filtro (il nono):

```python
plt.imshow(result[8], cmap=plt.cm.RdBu_r)
plt.show()
```

![png](7.Network_architectures_extended_73_0.png) Questa immagine mostra l'attivazione del nono filtro.


**4. Struttura del modello VGG:**

```python
modelPreTrained
VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace=True) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ... ))
```

Questo snippet mostra una parte della struttura del modello VGG utilizzato, evidenziando i primi layer convoluzionali e di attivazione ReLU, e un layer di max pooling.


In sintesi, il codice fornisce un metodo per visualizzare le mappe di attivazione di una rete neurale convoluzionale, aiutando a comprendere il processo di apprendimento e le *feature* estratte ad ogni livello.  L'utilizzo di `viz_act_val` semplifica l'accesso alle attivazioni, mentre la visualizzazione tramite Matplotlib permette una comprensione intuitiva dei risultati.


