```python
from utils import *
import numpy.linalg as LA

img_src = '.'
```

# SIFT Detection and Descriptors

L'algoritmo SIFT si basa essenzialmente su 4 concetti: 
- Scale-space extrema detection
- Keypoint localization
- Orientation assignment
- Local descriptor creation


Proviamo ad illustrare i vari concetti sulla solita immagine


```python
imageFull = mp_image.imread(os.path.join(img_src,'mountain_view1.png'))

image = rgb2gray(imageFull)

maxv, minv = np.max(image), np.min(image)

image = (image - minv)/(maxv-minv)

print('Shape', image.shape)
img_show(image,cmap=cm.gray)
```

![png](output_3_1.png)

La scala di grigi è una media pesata sui 3 canali. Il descrittore che trovo su questa immagine continua ad essere valido sulla stessa immagine in scala a colori. 
    

## Scale-space

Cominciamo con l'illustrare la piramide gaussiana. 

Il concetto di scale-space può essere illustrato applicando il filtro gaussiano ad un'immagine per valori differenti di deviazione standard. 

Dalle proprietà della distribuzione gaussiana sappiamo che $6\sigma$ rappresenta il range che racchiude il 99% dei dati distribuiti secondo una gaussiana. Questo significa che, se immaginiamo una griglia di punti, tutti i punti che distano più di $3\sigma$ dal centro avranno valore 0. Un valore di $\sigma$ esprime quindi un filtro gaussiano di dimensione $s = 2\cdot \lceil 3\sigma\rceil +1$. 

Proviamo ora ad applicare alla stessa immagine un filtro gaussiano corrispondente a valori crescenti di devianza, su un fattore $2^k$ rispetto ad un valore iniziale di $\sigma$. 

```python
fig = plt.figure(figsize=(20, 20))

num_octave = 6

for i in range(num_octave):
    sigma = 2**(i-1)
    fig.add_subplot(1, num_octave, i+1) 
    plt.imshow(ndimage.gaussian_filter(image,sigma),cmap=cm.gray)
    s = 2*np.ceil(3*sigma)+1
    plt.title(f'$\sigma$: {sigma} (filter size: {s})')
    plt.axis('off')

plt.show()
```

![png](output_7_0.png)

### Scale-Space e Filtro Gaussiano

Lo **scale-space** mira a gestire l'invarianza rispetto a diverse scale, ad esempio riducendo la risoluzione. Tuttavia, questo processo è computazionalmente costoso. Per ovviare a questo problema, si può simulare la riduzione di risoluzione tramite un **filtro gaussiano**.

Il parametro del filtro gaussiano, **sigma**, varia in modo esponenziale, raddoppiando ad ogni passo. Questo processo è definito **ottava**: ad ogni ottava, la dimensione dell'immagine viene ridotta. Il numero di ottave da calcolare è un parametro di input.

Applicando il filtro gaussiano e riducendo la dimensione dell'immagine, si mantiene la stessa informazione anche dopo la riduzione. In pratica, l'applicazione del filtro simula la perdita di dettagli che si avrebbe se la scala dell'immagine decrescesse progressivamente.

Poiché il filtro gaussiano effettua uno smoothing dell'intensità, non è necessario mantenere tutti i pixel. Di conseguenza, possiamo eliminarli senza perdere informazioni significative. 

Ad esempio, possiamo plottare le varie figure dimezzando la dimensione ad ogni passaggio, ottenendo una rappresentazione visiva della riduzione di scala.


```python
rows,cols = image.shape

output = np.zeros((rows,cols+cols//2))

output[:rows,:cols] = image

num_octave = 6

cur_scale = image

horiz = True
rows = 0

for i in range(num_octave):
    sigma = 2**i
    cur_scale = ndimage.gaussian_filter(image,sigma)
    cur_scale = cur_scale[::2**(i+1),::2**(i+1)]
    rows_scaled, cols_scaled = cur_scale.shape
    output[rows:rows+rows_scaled,cols:cols+cols_scaled] = cur_scale
    rows += rows_scaled
        
        
plt.figure(figsize=(20, 20))

plt.imshow(output,cmap=cm.gray)
plt.axis('off')
plt.show()
```


    
![png](output_10_0.png)
    
## Scale-Space: Ottava e Riduzione di Dimensione

Per costruire il **scale-space**, si procede per **ottave**. La prima ottava contiene l'immagine a risoluzione reale. Successivamente, si applica un **filtro gaussiano** all'immagine.

Per ridurre la dimensione dell'immagine, si selezionano i pixel con uno **step** di incremento. Questo step aumenta progressivamente per le ottave successive. Ad esempio, per la prima ottava lo step è 1, per la seconda è 2, per la terza è 4 e così via.

In pratica, si applica il filtro gaussiano all'immagine originale, si dimezza la dimensione dell'immagine ottenendo la seconda ottava, si dimezza nuovamente la dimensione ottenendo la terza ottava e così via. Il numero di iterazioni è determinato da un parametro che dipende dalla dimensione dell'immagine.


Nella figura sopra, l'immagine grande è l'originale, mentre a destra troviamo le riduzioni. Si noti come, nelle riduzioni, si perdono progressivamente i dettagli mentre il focus sulle componenti principali dell'immagine rimane.

Possiamo ripetere le stesse operazioni in sequenza per costruire il scale-space completo.


```python
sz = 4

num_octave = 6

cur_scale = np.copy(image)
sigma = 1

for i in range(num_octave):
    cur_scale = ndimage.gaussian_filter(cur_scale,sigma)
    cur_scale = cur_scale[::2,::2]
    
    plt.figure(figsize=(sz, sz))
    plt.imshow(cur_scale,cmap=cm.gray)
    s = 2*np.ceil(3*2**(i-1))+1
    plt.title(f'$\sigma$: {2**(i-1)} - Size: {cur_scale.shape}')
    plt.axis('off')
    plt.show()
    
    sz = sz/2
    

```
![png](output_13_0.png)
![png](output_13_1.png)
    
![png](output_13_2.png)
![png](output_13_3.png)
    
![png](output_13_4.png)
![png](output_13_5.png)

Anche un piccolo quadratino (es. 7x9 pixel) può contenere informazioni utili, come la cima di una montagna. Il valore di sigma del filtro gaussiano tende ad essere elevato in questo contesto, poiché il filtro viene applicato a scale sempre più grandi.

Il modello logico utilizzato è la **piramide gaussiana**, una struttura gerarchica di immagini dove ogni livello rappresenta una diversa scala dell'immagine originale. La piramide gaussiana è costruita applicando il filtro gaussiano e riducendo la dimensione dell'immagine, ripetendo il processo per creare livelli con risoluzioni inferiori.

La piramide gaussiana è fondamentale per l'analisi dell'immagine a diverse scale, permettendo di estrarre informazioni utili da ciascuna scala.

La sequenza che abbiamo costruito è una piramide Gaussiana. Nella rappresentazione compatta: 


```python
rows,cols = image.shape

output = np.zeros((rows,cols+cols//2))

output[:rows,:cols] = image

num_octave = 6
sigma = 1

cur_scale = image

horiz = True
rows = 0

for i in range(num_octave):
    cur_scale = ndimage.gaussian_filter(cur_scale,sigma)
    cur_scale = cur_scale[::2,::2]
    rows_scaled, cols_scaled = cur_scale.shape
    output[rows:rows+rows_scaled,cols:cols+cols_scaled] = cur_scale
    rows += rows_scaled
        
        
plt.figure(figsize=(20, 20))

plt.imshow(output,cmap=cm.gray)
plt.axis('off')
plt.show()
```

    
![png](output_16_0.png)
    
## Scale-Space: Piramide Gaussiana e SIFT

### Piramide Gaussiana

La piramide gaussiana è una struttura gerarchica di immagini dove ogni livello rappresenta una diversa scala dell'immagine originale. La base della piramide si riduce progressivamente applicando filtri gaussiani ad ogni livello.

### Scale-Space

Lo Scale-Space rappresenta un'immagine come una famiglia di versioni smoothed della stessa immagine, con l'obiettivo di simulare la perdita di dettagli che si avrebbe riducendo la scala. L'unico parametro che controlla questa famiglia di versioni è la devianza $\sigma$ del filtro gaussiano di smoothing.

Applicando progressivamente $\sigma$, $2\sigma$, $4\sigma$ ... stiamo di fatto riducendo la scala di un fattore 1/4 ad ogni applicazione del filtro.

### SIFT e Scale-Space

SIFT (Scale-Invariant Feature Transform) suddivide lo scale-space in **ottave**, dove ogni ottava $i$ corrisponde all'applicazione del filtro $2^i\sigma$. In pratica, ogni ottava raddoppia il valore precedente.

Ogni ottava è suddivisa in un numero $s$ di **intervalli**:

* Se $s=1$, l'ottava sarà composta di due immagini (un intervallo).
* Se $s=2$, avremo tre immagini e così via.

Poiché l'ultima immagine dell'ottava sarà raddoppiata rispetto all'immagine iniziale, le immagini saranno ottenute applicando progressivamente il filtro gaussiano con devianza $k \sigma$, dove $k$ è la costante che garantisce che $k^s\sigma = 2\sigma$ (e quindi $k= 2^{1/s}$).

### Codifica dello Scale-Space

```python
def generate_octave(init_level, s, sigma): 
    octave = [init_level] 
    k = 2**(1/s) 
    for _ in range(s+2): 
        next_level = ndimage.gaussian_filter(octave[-1],k * sigma)
        octave.append(next_level) 
    return octave

def generate_gaussian_pyramid(im, num_octave, s, sigma): 
    pyr = [] 
    for _ in range(num_octave): 
        octave = generate_octave(im, s, sigma) 
        pyr.append(octave) 
        im = octave[-3][::2, ::2] 
    return pyr
```

Questi due metodi generano la piramide gaussiana:

* Prendono l'immagine iniziale, il numero di ottave e il numero di scale.
* Per ogni livello della piramide, calcolano un numero parametrico (S) di responsi ai filtri gaussiani ai diversi valori di sigma.

---

La funzione `generate_octave` costruisce delle immagini aggiuntive, per dei motivi che saranno chiari in seguito. Se proviamo a visualizzare la piramide otteniamo questo: 


```python
def plot_pyramid(p,sz,hspace=10,vspace=10):
    rows, cols = sz[0],sz[1]

    nrows = sum([x[0].shape[0] for x in p]) +  vspace*(num_octave-1)
    ncols = cols*(s+3)+hspace*(s+2)
    output_image = np.ones((nrows,ncols))

    r = 0
    for i in range(len(p)):
        c = 0
        for j in range(len(p[i])):
            w,h = p[i][j].shape
            output_image[r:r+w,c:c+h] = p[i][j]
            c += cols + hspace
        r += w + vspace
    
    return output_image

```

la piramide sarà fatta da basi con tanti sottolivelli


```python
num_octave = 4
s = 2
sigma = 1

p = generate_gaussian_pyramid(image,num_octave,s,sigma)

output_pyr = plot_pyramid(p,image.shape)

fig = plt.figure(figsize=(20, 20))

plt.imshow(output_pyr,cmap=cm.gray)
plt.axis('off')
plt.show()
```


    
![png](output_27_0.png)
    
## Scale-Space: Piramide Gaussiana e SIFT

### Costruzione dello Scale-Space

Lo scale-space appena mostrato è in due dimensioni:

* **Colonne:** Le immagini vengono convolute iterativamente con un filtro gaussiano di devianza $k\sigma$.
* **Righe:** Ogni riga inizia con la terz'ultima immagine della riga precedente, campionata al 50%.

### Relazioni tra le Immagini

E' interessante vedere le relazioni che esistono tra le varie immagini. Poiché partiamo con $\sigma=1$, possiamo costruire la seguente tabella:

$$
\begin{array}{|c|c|c|c|}
    \hline
    \sigma = 1,s=1  & \sigma = \sqrt{2},s=1 & \sigma = 2,s=1 & \sigma = 2\sqrt{2},s=1& \sigma = 4,s=1\\
    \sigma = 2,s=1/4 & \sigma = 2\sqrt{2},s=1/4 & \sigma = 4,s=1/4 & \sigma = 4\sqrt{2},s=1/4& \sigma = 8,s=1/4\\
    \sigma = 4,s=1/16 & \sigma = 4\sqrt{2},s=1/16 & \sigma = 8,s=1/16 & \sigma = 8\sqrt{2},s=1/16& \sigma = 16,s=1/16\\
    \sigma = 8,s=1/32  & \sigma = 8\sqrt{2},s=1/32 & \sigma = 16,s=1/32 & \sigma = 16\sqrt{2},s=1/32& \sigma = 32,s=1/32\\
    \hline
\end{array}
$$

In questa tabella vengono evidenziati i parametri di sigma e scale usati per ottenere quel risultato. Si nota che:

* La dimensione è dimezzata (relazione fattore 2).
* Sigma varia per un fattore k.
* Ci sono valori che sono ripetuti.

A parità di scale, dimezzando la risoluzione di un parametro gamma= sqrt2, si ottengono valori ripetuti. Questa proprietà è utile per individuare i punti di interesse.

### Individuazione dei Punti di Interesse

L'obiettivo è individuare i punti di interesse all'interno dell'immagine che siano invarianti rispetto alla scala. Per farlo, cerchiamo tutti i possibili valori delle scale possibili, alle diverse risoluzioni, e cerchiamo i punti, rimappandoli poi nell'immagine originale.

Per trovare questi punti, dobbiamo vedere come varia il gradiente e dunque l'intensità di quei pixel. Dobbiamo calcolare la variazione dei pixel in un certo intorno a diverse scale, con il laplaciano della gaussiana.

## Extrema detection

Data una sequenza di ottave, un'idea semplice per trovare gli estremi potrebbe essere quella di applicare il laplaciano "smoothed" (ovvero il laplaciano della gaussiana, LoG). La formula per LoG è data da

$$
\nabla^2 G_\sigma (x,y) = \left(\frac{x^2+y^2-2\sigma^2}{\sigma^4}\right)e^{-\frac{x^2+y^2}{2\sigma^2}}
$$

Se proviamo ad applicarlo all'immagine originale otteniamo quel che segue.


```python
fig = plt.figure(figsize=(20, 20))

num_octave = 4

for i in range(num_octave):
    sigma = 2**(i-1)
    fig.add_subplot(1, num_octave, i+1) 
    plt.imshow(ndimage.gaussian_laplace(image,sigma),cmap=cm.gray)
    s = 2*np.ceil(3*sigma)+1
    plt.title(f'$\sigma$: {sigma} (filter size: {s})')
    plt.axis('off')

plt.show()
```


    
![png](output_33_0.png)
    

## Applicazione del Filtro LoG per l'Individuazione di Keypoint

Il filtro LoG (Laplacian of Gaussian) viene applicato alle immagini per calcolare il gradiente applicato a un filtro gaussiano. Il risultato è un'immagine in cui i punti più chiari indicano una maggiore variazione di intensità nell'immagine originale, mentre le zone scure corrispondono a un gradiente piatto. 

**Interpretazione dei Risultati:**

- **Punti Chiari:** Indicano una forte variazione di intensità nell'immagine originale, suggerendo la presenza di un keypoint.
- **Zone Scure:** Indicano un gradiente piatto, ovvero una variazione di intensità minima.

**Problema della Scala:**

Aumentando la scala (devianza $\sigma$) del filtro LoG, il responso del filtro tende a diluirsi, rendendo difficile l'individuazione di zone significative.

**Soluzione:**

Per ovviare a questo problema, è possibile pesare il filtro LoG con un fattore proporzionale alla scala:

$$
\sigma^2\nabla^2G_\sigma
$$

**Vantaggi della Pesatura:**

- **Aumento dell'Intensità:** La moltiplicazione per $\sigma^2$ aumenta l'intensità dei pixel, rendendo più evidenti le zone di interesse.
- **Miglioramento della Visibilità:** La pesatura con $\sigma^2$ aiuta a mantenere la visibilità delle zone significative anche a scale maggiori.

**Conclusione:**

L'applicazione del filtro LoG pesato con $\sigma^2$ permette di individuare zone da "attenzionare" per l'individuazione dei keypoint, anche a scale maggiori. Questo approccio aiuta a migliorare la precisione e la robustezza dell'algoritmo di rilevamento dei keypoint.


Cerchiamo di capire questa cosa con un esempio


```python
a = 100
b = 200

sig = np.concatenate((np.zeros(a),np.ones(b),np.zeros(a)))

x = np.linspace(-20,20,a*2 + b)


plt.plot(x,sig)

plt.show()
```


    
![png](output_38_0.png)
    


applicazione a un segnale ad impulso


```python
from scipy import signal

def LoG(x,sigma):
    return ((x**2 - 2*sigma**2)/(sigma**4))*np.exp(-x**2/(2*sigma**2))


fig = plt.figure(figsize=(20, 20))

k=1

y_min, y_max = -60, 5
scales = 4

for i in range(scales):
    sigma = 2**(i-1)

    w = np.linspace(-10,10,100)
    lg = LoG(w,sigma)

    y = signal.convolve(sig, lg, mode='same')
    z = signal.convolve(sig, (sigma**2)*lg, mode='same')

    
    fig.add_subplot(scales, 4, k) 
    plt.ylim([y_min, y_max])
    plt.plot(x,y)
    plt.title(f'conv(sig,LoG) $\sigma$={sigma}')
    
    fig.add_subplot(scales, 4, k+1)
    plt.ylim([y_min, y_max])
    plt.plot(w,lg)
    plt.title(f'LoG $\sigma$={sigma}')
    
    fig.add_subplot(scales, 4, k+2) 
    plt.ylim([y_min, y_max])
    plt.plot(x,z)
    plt.title(f'conv(sig,$\sigma^2$*LoG) $\sigma$={sigma}')
    
    fig.add_subplot(scales, 4, k+3)
    plt.ylim([y_min, y_max])
    plt.plot(w,(sigma**2)*lg)
    plt.title(f'$\sigma^2$*LoG $\sigma$={sigma}')

    k += 4
    

plt.show()
```
    
![[6b_Features_SIFT-20241008154143102.png]]
**Configurazione:**

* **Assi:** Le colonne rappresentano i diversi valori di sigma. Le righe rappresentano la convoluzione del segnale con il laplaciano, la convoluzione con sigma^2 e il filtro applicato.
* **Obiettivo:** Identificare i punti in cui si verificano variazioni nel segnale (da 0 a 1 e da 1 a 0).

**Comportamento del Filtro:**

* **Sigma basso:** Il filtro laplaciano non pesato produce un responso definito, evidenziando le variazioni nel segnale.
* **Sigma alto:** All'aumentare di sigma, il responso del filtro si appiattisce. Per valori di sigma molto alti, il responso diventa una linea piatta perché il filtro stesso assume una forma piatta.

**Spiegazione:**

L'appianamento del responso del filtro con l'aumento di sigma è dovuto al fatto che il filtro laplaciano diventa sempre più "sfumato" con l'aumentare della deviazione standard (sigma). Questo effetto di sfumatura tende a "cancellare" le variazioni brusche nel segnale, rendendo il responso del filtro più uniforme.


```python
x = np.linspace(-5,5,1000)

fig = plt.figure(figsize=(10, 10))

k = 1
for i in range(num_octave):
    sigma = 2**(i-1)
    y = LoG(x,sigma) 
    #y2= y/sigma**2
    y2= y*sigma**2
    
    y_min, y_max = min(y+y2), max(y+y2)
    #y_min, y_max = None, None

    
    fig.add_subplot(num_octave, 2, k) 
    plt.plot(x,y,label=f'$LoG_\sigma, \sigma={sigma}$')
    plt.ylim([y_min, y_max])
    plt.legend()

    fig.add_subplot(num_octave, 2, k+1) 
    plt.plot(x,y2,label=f'$sigma^2LoG_\sigma, \sigma={sigma}$')
    plt.ylim([y_min, y_max])
    plt.legend()
    k += 2

plt.show()
```


![[6b_Features_SIFT-20241008154125724.png]]


qui cambiamo le scale dei grafici
Si noti come, nel grafico sopra, le scale vengono mantenute costanti. Applicato alle immagini, otteniamo:


```python
fig = plt.figure(figsize=(20, 20))

num_octave = 4

for i in range(num_octave):
    sigma = 2**(i-1)
    fig.add_subplot(1, num_octave, i+1) 
    plt.imshow(sigma**2*ndimage.gaussian_laplace(image,sigma),cmap=cm.gray)
    s = 2*np.ceil(3*sigma)+1
    plt.title(f'$\sigma$: {sigma} (filter size: {s})')
    plt.axis('off')

plt.show()
```


    ![[6b_Features_SIFT-20241008154150829.png]]

    


## Applicazione del Filtro Laplaciano per la Segmentazione

L'utilizzo del filtro laplaciano nella segmentazione di immagini offre diversi vantaggi rispetto ad altri metodi.

**Vantaggi:**

* **Migliore distinzione:** L'applicazione del filtro laplaciano permette di distinguere in modo più netto le zone di interesse (ad alta variazione) dalle zone a bassa variazione.
* **Efficienza computazionale:** Il filtro laplaciano può essere approssimato come la differenza di due gaussiane con valori di sigma diversi di un fattore k. Questo permette di ottenere l'applicazione del filtro calcolando la differenza dei responsi di due filtri gaussiani con sigma e 2sigma.
* **Riduzione del calcolo:** Non è necessario applicare il filtro laplaciano direttamente (operatore esponenziale).
* **Riutilizzo dei filtri gaussiani:** I filtri gaussiani utilizzati per l'approssimazione del laplaciano sono già stati calcolati durante la ricerca delle diverse scale dell'immagine, quindi possono essere riutilizzati.

**Relazione tra Gaussiane e Laplaciano:**

Un aspetto interessante è che l'applicazione del laplaciano può essere ottenuta direttamente, calcolando la differenza tra le immagini in una ottava. Infatti, si può dimostrare (sia graficamente che analiticamente) che:

$$G_{k\sigma}(x,y)-G_{\sigma}(x,y) \approx (k-1)\sigma^2\nabla^2G_\sigma(x,y)$$

dove:

* $G_{k\sigma}(x,y)$ è la gaussiana con deviazione standard $k\sigma$.
* $G_{\sigma}(x,y)$ è la gaussiana con deviazione standard $\sigma$.
* $\nabla^2$ è l'operatore laplaciano.

Questa relazione dimostra che la differenza tra due gaussiane con deviazioni standard diverse è approssimativamente proporzionale al laplaciano della gaussiana con la deviazione standard minore.

```python
def G(x,sigma):
    return (1/(2*np.pi*sigma**2))*np.exp(-x**2/(2*sigma**2))

x = np.linspace(-5,5,1000)

sigma = 1
k = np.sqrt(2)

y = LoG(x,sigma)

z = (G(x,k*sigma) - G(x,sigma))

fig = plt.figure(figsize=(10, 4))
fig.add_subplot(1, 2, 1) 
plt.plot(x,y,label='$\sigma^2LoG_\sigma$')
plt.legend()

fig.add_subplot(1, 2, 2) 
plt.plot(x,z,label='$G_{k\sigma} - G_{\sigma}$')
plt.legend()


plt.show()

```

![[6b_Features_SIFT-20241008154204741.png]]
    

Rappresenta la shape che hanno le due operazioni, la prima è col filtro vero, la seconda con l'applicazione dei due filtri gaussiani. A variare sono solo le scale, ma a noi interessa solo dove si trova il picco. 
In termini di shape è una buona approssimazione

A questo punto possiamo ottenere le ottave di DoG: 


```python
def generate_DoG_octave(gaussian_octave,use_concat): 
    octave = [] 
    for i in range(1, len(gaussian_octave)):   
        octave.append(gaussian_octave[i] - gaussian_octave[i-1]) 
    if use_concat:
        return np.concatenate([o[:,:,np.newaxis] for o in octave], axis=2) 
    else:
        return octave

def generate_DoG_pyramid(gaussian_pyramid,use_concat=False): 
    pyr = [] 
    for gaussian_octave in gaussian_pyramid: 
        pyr.append(generate_DoG_octave(gaussian_octave,use_concat)) 
    return pyr
```


```python
num_octave = 4
s = 2
sigma = 1

p = generate_gaussian_pyramid(image,num_octave,s,sigma)
d = generate_DoG_pyramid(p)

output_pyr = plot_pyramid(d,image.shape)

fig = plt.figure(figsize=(20, 20))

plt.imshow(output_pyr,cmap=cm.gray)
plt.axis('off')
plt.show()
```

![[6b_Features_SIFT-20241008154211234.png]]


A questo punto, avendo costruito la piramide con le varie scale, possiamo individuare i keypoints candidati.

Un keypoint si considera come candidato se è il centro di una graglia 3x3x3 e rappresenta il minimo o il massimo tra i vicini.

La ricerca avviene per ogni piramide e iterando sui diversi livelli


```python
num_octave = 4
s = 2
sigma = 1

p = generate_gaussian_pyramid(image,num_octave,s,sigma)
d = generate_DoG_pyramid(p, True)

DoG_shapes = [x.shape for x in d]

DoG_shapes
```

    [(415, 572, 4), (208, 286, 4), (104, 143, 4), (52, 72, 4)]

## Identificazione dei Punti Candidati con il Filtro Laplaciano

L'obiettivo è identificare i punti candidati per la detezione di caratteristiche, in corrispondenza dei picchi del filtro laplaciano applicato all'immagine.

**Metodo:**

1. **Piramide Gaussiana:** Si utilizza una piramide gaussiana, dove ogni livello rappresenta una scala diversa dell'immagine. Le scale sono costruite con valori di sigma crescenti.
2. **Intorno 3D:** Per ogni punto dell'immagine, si considera un intorno 3D composto da 3 livelli di scala adiacenti $(sigma, k \cdot sigma, k^2 \cdot sigma).$
3. **Calcolo del responso del filtro:** Si calcola il responso del filtro laplaciano per ogni punto dell'intorno 3D, calcolando la differenza tra i responsi dei due filtri gaussiani successivi.
4. **Confronto con i valori adiacenti:** Il valore del responso del filtro per il punto centrale dell'intorno 3D viene confrontato con i valori dei 26 punti adiacenti.
5. **Identificazione dei picchi:** Se il valore del punto centrale è il massimo o il minimo tra i 27 valori dell'intorno, il punto viene considerato un punto candidato.

**Esempio:**

Per la prima DoG (Difference of Gaussians), il numero di punti candidati dipende dalla dimensione dell'immagine e dallo step di scansione.

**Metodo *get_candidate_keypoints***

Il metodo *get_candidate_keypoints* itera su tutti i punti dell'immagine con uno step di $\frac{w}{2}$, dove *w* è la larghezza dell'immagine. Per ogni punto, costruisce una patch 3x3x3, che rappresenta l'intorno 3D.

**Posizione del centro della patch:**

Poiché la patch 3x3x3 contiene 27 elementi, il centro della patch si trova in posizione $\frac{27}{2}$ (divisione intera). 


```python
def get_candidate_keypoints(D, w=16): 
    candidates = [] 
    
    # add new levels for z iterations
    D[:,:,0] = 0 
    D[:,:,-1] = 0
    
    # iter on x
    for i in range(w//2+1, D.shape[0] - w//2-1): 
        # iter on y
        for j in range(w//2+1, D.shape[1]-w//2-1): 
            # iter on z
            for k in range(1, D.shape[2]-1): 
                patch = D[i-1:i+2, j-1:j+2, k-1:k+2] 
                if np.argmax(patch) == 13 or np.argmin(patch) == 13: 
                    candidates.append([i, j, k]) 
    return candidates

first_DoG = d[0]
candidates = get_candidate_keypoints(first_DoG)

candidates_array = np.array(candidates)

print(f'{candidates_array}\n Shape: {candidates_array.shape}')
```

## Selezione dei Punti Candidati

Dopo aver applicato il filtro laplaciano e identificato i punti candidati, è necessario selezionare i punti più significativi.

**Matrice dei Punti Candidati:**

La matrice dei punti candidati ha la seguente struttura:

```
     [[  9  12   1]
     [  9  56   1]
     [  9  61   1]
     ...
     [405 494   1]
     [405 518   1]
     [405 533   1]]
     Shape: (7877, 3)
```

* Ogni riga rappresenta un punto candidato.
* La prima colonna indica la coordinata x del punto.
* La seconda colonna indica la coordinata y del punto.
* La terza colonna indica il livello di scala (sigma) a cui il punto è stato trovato.

**Selezione dei Punti:**

Il codice `if np.argmax(patch) == 13` verifica se il punto centrale della patch 3x3x3 (posizione 13) è il massimo o il minimo tra i 27 valori della patch. Questo processo identifica i punti candidati che sono picchi locali del responso del filtro.

**Numero di Punti Candidati:**

In questo caso, sono stati trovati 7877 punti candidati. Questo numero è elevato e indica che molti punti dell'immagine sono stati identificati come potenziali punti di interesse.

**Visualizzazione dei Punti sull'Ottava:**

Per visualizzare i punti candidati sull'ottava, è necessario proiettare i punti trovati a diverse scale sulla stessa immagine. Questo può essere fatto interpolando i punti trovati a diverse scale sull'immagine originale.




```python
for i, computed_DoG in enumerate(d):
    candidates_i = get_candidate_keypoints(computed_DoG)
    
    for k in range(1, computed_DoG.shape[2]-1):
        points = [x for x in candidates_i if x[-1] == k]

        points_image = np.ones_like(p[i][k])
        
        for x, y, _ in points:
            points_image[x, y] = 0

        fig, ax = plt.subplots(1, 2, figsize=(8, 6))
        ax[0].imshow(p[i][k], cmap='gray')
        ax[0].axis('off')
        ax[0].set_title(f'octave i-th {i}')

        ax[1].imshow(points_image, cmap='gray')
        ax[1].axis('off')
        ax[1].set_title(f'DoG k-th {k}')

plt.show()    
```

![[6b_Features_SIFT-20241008154608234.png]]
![png](output_59_4.png)
![png](output_59_5.png)
![png](output_59_6.png)
![png](output_59_7.png)
## Trovare il Pixel Corrispondente a Diverse Scale

### Panoramica
Il processo prevede l'utilizzo di una piramide gaussiana per rappresentare l'immagine a diverse scale. La piramide gaussiana è una sequenza di immagini, ciascuna delle quali è una versione ridotta della precedente. La riduzione di scala viene ottenuta applicando un filtro gaussiano all'immagine.

### Trovare il Pixel Corrispondente
Per trovare il pixel corrispondente a diverse scale, si utilizza un'approssimazione di Taylor. L'approssimazione di Taylor è un metodo per approssimare una funzione in un punto usando i suoi derivati. In questo caso, la funzione è la piramide gaussiana e il punto è il pixel che si desidera trovare.

### Matrice Hessiana e Jacobiano
Per calcolare l'approssimazione di Taylor, si utilizzano la matrice Hessiana e il Jacobiano. La matrice Hessiana è una matrice di derivate seconde della funzione, mentre il Jacobiano è una matrice di derivate prime della funzione.

In questo caso, la matrice Hessiana è una matrice 3D perché la piramide gaussiana è una funzione 3D (x, y, s). I pedici x, y, s indicano le coordinate spaziali e la scala.

### Equazione per Trovare il Pixel Corrispondente
Per trovare il pixel corrispondente, si imposta l'approssimazione di Taylor a 0. Questo porta a un'equazione che può essere risolta per trovare le coordinate del pixel corrispondente.


# Keypoint Localization

Il processo di selezione si compone di tre step:

1. per ogni keypoint candidato si calcola la posizione del subpixel
2. fissata una threshold, si scartano i keypoint con un valore del subpixel inferiore alla soglia
3. si eliminano i keypoint ai bordi (e agli angoli)


Perché si introduce il concetto di subpixel? Cosa sono i keypoint candidati che sono stati individuati? E dove sono stati individuati?

Ricordate che stiamo analizzando la piramide di DoG? Che è stata ottenuta con valori crescenti di $\sigma$?

Quindi si utilizza un processo di raffinamento per individuare la posizione migliore, cioé il pixel che meglio rappresenta la vera posizione di un keypoint.
E contemporaneamente si eliminano i punti con un basso valore di contrasto (la soglia!) e quelli che sono stati individuati ai bordi dell'immagine

## Subpixel Localization

Si utilizza un metodo che si basa sull'espansione di Taylor del secondo ordine per calcolare l'offset del subpixel rispetto al punto cadidato. Ovvero:

$$
z_0 = [x_0, y_0, \sigma]^T  \\
z = [\delta_x, \delta_y, \delta_\sigma]^T  \\
D(z_0 + z) \approx D(z_0) + ({\frac{\partial D}{\partial z}}|_{z_0})^T z + \frac {1}{2} z^T ({\frac{\partial^2 D}{\partial z^2}}|_{z_0}) z
$$


Derivando rispetto a *z* e eguagliando a 0, otteniamo

$$
\widehat{z} = - ({\frac{\partial^2 D}{\partial z^2}}|_{z_0}) ({\frac{\partial D}{\partial z}}|_{z_0})
$$

e $\widehat{z}$ è proprio l'offset che stiamo cercando.

Poiché consideriamo i punti vicini per calcolare le derivate, di fatto dobbiamo risolvere un sistema di equazioni 3x3.

Per risolvere questo sistema costruiamo il vettore Jacobiano con le derivate parziali prime e la matrice Hessiana con le derivate parziali seconde e ne facciamo il prodotto

$$
J = \begin{bmatrix}
D_x \\
D_y \\
D_s
\end{bmatrix}
$$

e

$$
H = \begin{bmatrix}
D_{xx} & D_{xy} & D_{xs} \\
D_{xy} & D_{yy} & D_{ys} \\
D_{xs} & D_{ys} & D_{ss} 
\end{bmatrix}
$$



```python
def localize_keypoint(D, x, y, s):
	dx = (D[y,x+1,s]-D[y,x-1,s])/2.
	dy = (D[y+1,x,s]-D[y-1,x,s])/2.
	ds = (D[y,x,s+1]-D[y,x,s-1])/2.

	dxx = D[y,x+1,s]-2*D[y,x,s]+D[y,x-1,s]
	dxy = ((D[y+1,x+1,s]-D[y+1,x-1,s]) - (D[y-1,x+1,s]-D[y-1,x-1,s]))/4.
	dxs = ((D[y,x+1,s+1]-D[y,x-1,s+1]) - (D[y,x+1,s-1]-D[y,x-1,s-1]))/4.
	dyy = D[y+1,x,s]-2*D[y,x,s]+D[y-1,x,s]
	dys = ((D[y+1,x,s+1]-D[y-1,x,s+1]) - (D[y+1,x,s-1]-D[y-1,x,s-1]))/4.
	dss = D[y,x,s+1]-2*D[y,x,s]+D[y,x,s-1]

	J = np.array([dx, dy, ds])
	HD = np.array([
		[dxx, dxy, dxs],
		[dxy, dyy, dys],
		[dxs, dys, dss]])
	
	offset = -LA.inv(HD).dot(J)	
	return offset, J, HD[:2,:2], x, y, s


first_kp = candidates[0]
print ('Keypoint', first_kp)

offset, J, HD, x, y, s = localize_keypoint(first_DoG, first_kp[0], first_kp[1], first_kp[2])

print('Offset', offset)
print('Jacobian', J)
print('Hessian\n', HD)

print('Keypoint Orig', x, y, s)
```


## Calcolo dell'Offset per Keypoint

Le matrici utilizzate contengono derivate parziali. Questo contesto è discreto, quindi le derivate indicano la variazione di intensità tra due pixel consecutivi. Per calcolare l'offset di un keypoint, si calcolano tutte le derivate parziali.

**LA** è una libreria di NumPy che, dato un keypoint, calcola le matrici e l'offset. Questo offset, sommato alle coordinate del keypoint, permette di ottenere le coordinate del punto originale.

**In sintesi:**

1. Si calcolano le derivate parziali per ogni pixel.
2. Si utilizzano le derivate parziali per calcolare le matrici.
3. Si utilizza la libreria LA per calcolare l'offset del keypoint.
4. Si sommano le coordinate del keypoint all'offset per ottenere le coordinate del punto originale.

## Sub pixel inferiori ad una soglia

Il subpixel ottenuto sommando l'offset alle coordinate del punto candidato viene scartato se il valore di contrasto è inferiore ad una soglia. I punti con un contrasto basso non sono significativi per caratterizzare l'immagine, per questo motivo vengono scartati

Se $\widehat{x}$ è l'offset individuato ed *x* è il punto candidato, allora *x* viene scartato se la quantità seguente è inferiore alla soglia

$$
D(\widehat{x}) = D + \frac {1}{2} \frac {\partial D^T} {\partial x} \widehat{x}
$$

> Nel paper di SIFT viene utilizzato il valore 0.03


```python
# esempio di valutazione del contrasto
t_c = .03

contrast = first_DoG[y,x,s] + .5*J.dot(offset) 
if abs(contrast) < t_c:
    print('point is discarded')
```

    point is discarded
    

## Filtraggio dei Punti

Durante il processo di analisi, è necessario filtrare alcuni punti. In particolare, i punti che corrispondono ad angoli non sono utili perché non è possibile calcolare l'orientamento del gradiente. L'orientamento del gradiente è fondamentale per calcolare l'invarianza.

Per determinare quali punti scartare, si utilizza un valore di soglia calcolato con la relazione D(X). Questo metodo è computazionalmente efficiente per calcolare il valore di contrasto. Tutti i punti con un valore di contrasto inferiore alla soglia vengono scartati.

## Edge Subpixel: Riconoscimento e Scarto

I keypoint che si trovano ai bordi dell'immagine devono essere scartati perché è difficile determinare l'orientamento del gradiente in questi punti.

Per identificare questi keypoint, si sfrutta una proprietà della matrice Hessiana **HD**:

* Se gli autovalori di HD sono entrambi grandi, il keypoint è un angolo.
* Se solo uno degli autovalori è grande, il keypoint è un bordo.

La matrice *HD*, calcolata in precedenza e contenente le derivate di ordine 2, viene riutilizzata per questo scopo.

Il rapporto tra gli autovalori di HD fornisce informazioni sulla curvatura di D. Il rapporto:

$$
\frac {(r+1)^2} {r}
$$

dove *r* rappresenta il rapporto degli autovalori, raggiunge il minimo quando gli autovalori sono uguali e aumenta al crescere di *r*.

> Nel paper di SIFT, il valore limite per questo rapporto è 10.

I pixel ai bordi sono quelli per cui la relazione precedente è vera.

Calcolando gli autovalori della matrice HD, si possono identificare quattro situazioni:

* **Angolo:** lambda1 ≈ lambda2
* **Piatto:** lambda1 ≈ lambda2
* **Bordo orizzontale:** lambda2 >> lambda1
* **Bordo verticale:** lambda1 >> lambda2

Queste relazioni vengono utilizzate per scartare i punti indesiderati.

**Nota:**

* Un punto piatto indica che la variazione di intensità è costante su uno degli assi.
* Un punto angolo indica che la variazione di intensità è presente su entrambi gli assi, come un picco in un dominio 2D.
* Se gli autovalori superano una certa soglia, il keypoint viene scartato.

```python
R_th = (10+1)**2/10

w, v = LA.eig(HD)
r = w[1]/w[0]
R = (r+1)**2 / r

if R > R_th:
    print('point is discarded')
```

    point is discarded
    

Quindi il pruning dei keypoint candidati avviene eseguendo i tre step descritti in precedenza.


```python
def find_keypoints_for_DoG_octave(D, R_th, t_c, w):
    candidates = get_candidate_keypoints(D, w)
    
    keypoints = []

    for i, cand in enumerate(candidates):
        y, x, s = cand[0], cand[1], cand[2]
        offset, J, H, x, y, s = localize_keypoint(D, x, y, s)
        
        # throw out low contrast points
        contrast = D[y,x,s] + .5 * J.dot(offset)
        if abs(contrast) < t_c:
            continue

        # # throw out edge points
        w, v = LA.eig(H)
        r = w[1]/w[0]
        R = (r+1)**2 / r
        if R > R_th: 
            continue

        # compute KP, but throw out boundary points
        kp = np.array([x, y, s]) + offset
        if kp[1] >= D.shape[0] or kp[0] >= D.shape[1]: 
            continue 

        keypoints.append(kp)

    print(f'#candidates = {len(candidates)}, #keypoints = {len(keypoints)}')
    
    return np.array(keypoints)


R_th = 10
t_c = .03
w_patch_size = 16
keypoints_first_DoG = find_keypoints_for_DoG_octave(first_DoG, R_th, t_c, w_patch_size)

print(f'{keypoints_first_DoG}\n Shape: {keypoints_first_DoG.shape}')
```

    #candidates = 7877, #keypoints = 1486
    [[408.75938434 154.24056872   1.2234741 ]
     [367.57668088 154.90026712   1.19036504]
     [424.83006557 158.92296556   1.1530998 ]
     ...
     [346.87703402 405.07882543   1.11209332]
     [367.77683613 404.8731439    1.14202255]
     [481.60012561 404.81703693   1.1847661 ]]
     Shape: (1486, 3)
    


```python
# cambiando le soglie 
R_th_2 = 20
t_c_2 = .001
w_patch_size = 16
_ = find_keypoints_for_DoG_octave(first_DoG, R_th_2, t_c_2, w_patch_size)
```

    #candidates = 7877, #keypoints = 5738
    

### Final step

A questo punto calcoliamo i keypoint per tutte le ottave


```python
def get_keypoints(DoG_pyr, R_th, t_c, w):
    kps = []
    for D in DoG_pyr:
        kps.append(find_keypoints_for_DoG_octave(D, R_th, t_c, w))
    return kps

keypoints_pyr = get_keypoints(d, R_th, t_c, w_patch_size)

len(keypoints_pyr) == len(d)
```

    #candidates = 7877, #keypoints = 1486
    #candidates = 1695, #keypoints = 115
    #candidates = 317, #keypoints = 22
    #candidates = 42, #keypoints = 2
    
    True

Troviamo i punti, in termini di coordinate. Dobbiamo associarci un'informazione descrittiva che dovrà avere in se informazioni sulla rotazione di quella patch
## Calcolo dell'Orientamento

Fino a questo punto, i keypoint calcolati sono invarianti rispetto alla scala e alla posizione. Per renderli invarianti anche alla rotazione, è necessario assegnare un orientamento a ciascun keypoint.

L'orientamento viene determinato considerando la direzione del gradiente dominante nell'intorno del keypoint.

**Procedura:**

1. **Costruzione della patch:** Per ogni keypoint, si costruisce una patch di dimensione proporzionale alla scala del keypoint.
2. **Calcolo dell'istogramma dei gradienti:** Si calcola un istogramma dei gradienti per ogni pixel nella patch.
    * Per ogni pixel, si considera il valore pesato *Wg* del gradiente e si assegna al bin corrispondente all'orientamento del gradiente.
    * Il valore *Wg* è la magnitudine del gradiente pesata dalla dimensione del filtro Gaussiano utilizzato per creare l'ottava.
3. **Assegnazione dell'orientamento:** L'orientamento del keypoint è pari al bin con valore massimo nell'istogramma.

L'istogramma dei gradienti ha 36 bin, corrispondenti a step di 10° (360° // 36).

**In questo modo, l'orientamento del keypoint tiene in considerazione anche il livello di dettaglio dell'ottava ottenuto dall'applicazione del filtro Gaussiano.**

**Esempio:**

Si considera una matrice di punti vicini al keypoint candidato. Tipicamente, la matrice è di dimensioni pari a una potenza di 2, ad esempio 16x16. Questa matrice viene suddivisa in celle 4x4. Per ogni cella, si calcola l'orientamento del gradiente dei pixel al suo interno. Si ottiene quindi un istogramma di 16 gradienti, descritti in termini di angolo e magnitudine. Questo istogramma rappresenta il descrittore della patch 4x4. Concatenando tutti gli istogrammi delle celle, si ottiene un descrittore finale, che è un array di 128 elementi. Questo array rappresenta la feature del keypoint.

```python
def gaussian_filter(sigma):
	size = 2*np.ceil(3*sigma)+1
	x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]
	g = np.exp(-((x**2 + y**2)/(2.0*sigma**2))) / (2*np.pi*sigma**2)
	return g/g.sum()


def cart_to_polar_grad(dx, dy):
    m = np.sqrt(dx**2 + dy**2)
    theta = (np.arctan2(dy, dx)+np.pi) * 180/np.pi
    return m, theta


def get_grad(L, x, y):
    dy = L[min(L.shape[0]-1, y+1),x] - L[max(0, y-1),x]
    dx = L[y,min(L.shape[1]-1, x+1)] - L[y,max(0, x-1)]
    return cart_to_polar_grad(dx, dy)


def quantize_orientation(theta, num_bins):
    bin_width = 360//num_bins
    return int(np.floor(theta)//bin_width)


def fit_parabola(hist, binno, bin_width):
    centerval = binno*bin_width + bin_width/2.

    if binno == len(hist)-1: rightval = 360 + bin_width/2.
    else: rightval = (binno+1)*bin_width + bin_width/2.

    if binno == 0: leftval = -bin_width/2.
    else: leftval = (binno-1)*bin_width + bin_width/2.
    
    A = np.array([
        [centerval**2, centerval, 1],
        [rightval**2, rightval, 1],
        [leftval**2, leftval, 1]])
    b = np.array([
        hist[binno],
        hist[(binno+1)%len(hist)], 
        hist[(binno-1)%len(hist)]])

    x = LA.lstsq(A, b, rcond=None)[0]
    if x[0] == 0: x[0] = 1e-6
    return -x[1]/(2*x[0])


def assign_orientation(kps, octave, num_bins=36):
    new_kps = []
    bin_width = 360//num_bins

    for kp in kps:
        cx, cy, s = int(kp[0]), int(kp[1]), int(kp[2])
        s = np.clip(s, 0, octave.shape[2]-1)

        sigma = kp[2]*1.5
        w = int(2*np.ceil(sigma)+1)
        kernel = gaussian_filter(sigma)

        L = octave[...,s]
        hist = np.zeros(num_bins, dtype=np.float32)

        for oy in range(-w, w+1):
            for ox in range(-w, w+1):
                x, y = cx+ox, cy+oy
                
                if x < 0 or x > octave.shape[1]-1: 
                    continue
                elif y < 0 or y > octave.shape[0]-1: 
                    continue
                
                m, theta = get_grad(L, x, y)
                weight = kernel[oy+w, ox+w] * m

                bin = quantize_orientation(theta, num_bins)
                hist[bin] += weight

        max_bin = np.argmax(hist)
        new_kps.append([kp[0], kp[1], kp[2], fit_parabola(hist, max_bin, bin_width)])

        max_val = np.max(hist)
        for binno, val in enumerate(hist):
            if binno == max_bin: 
                continue

            if .8 * max_val <= val:
                new_kps.append([kp[0], kp[1], kp[2], fit_parabola(hist, binno, bin_width)])

    return np.array(new_kps)

# compute orientations for first DoG
keypoints_with_orientation_firstDoG = assign_orientation(keypoints_pyr[0], first_DoG)

print(f'before {len(keypoints_pyr[0])} and {len(keypoints_with_orientation_firstDoG)} after orientation')
```

## Descrizione dei metodi
| Metodo                                       | Descrizione                                                                                       |
| -------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| gaussian_filter(sigma)                       | Genera un filtro gaussiano 2D con deviazione standard sigma.                                      |
| cart_to_polar_grad(dx, dy)                   | Converte le derivate cartesiane dx e dy in magnitudine e angolo del gradiente.                    |
| get_grad(L, x, y)                            | Calcola le derivate cartesiane del gradiente in un punto (x, y) dell'immagine L.                  |
| quantize_orientation(theta, num_bins)        | Quantizza l'angolo del gradiente theta in uno dei num_bins bin.                                   |
| fit_parabola(hist, binno, bin_width)         | Adatta una parabola all'istogramma dei gradienti per trovare l'orientamento preciso del keypoint. |
| assign_orientation(kps, octave, num_bins=36) | Assegna un orientamento a ciascun keypoint in un'ottava dell'immagine.                            |


**Osservazioni**

Per ogni istogramma, si considera l'orientamento del bin max. Ma si considerano anche i bin con valori superiori all'80% del max

Nel definire il valore di orientamento, si utilizza una funzione di interpolazione parabolare. Cioè per definire il valore esatto si utillizza la funzione di una parabola consideranto anche i valori vicini al max 

Calcoliamo l'orientamento per tutti i keypoint


```python
keypoints_with_orientation = []

for i, DoG_octave in enumerate(d):
    kp_or = assign_orientation(keypoints_pyr[i], DoG_octave)
    
    keypoints_with_orientation.append(kp_or)
    
print('computed orientations for all octaves')    
```

    computed orientations for all octaves
    

# Local descriptor creation

Lo step finale dell'algoritmo SIFT è il calcolo delle features finali (local descriptors).

Per ogni keypoint SIFT restituisce un vettore di 128 elementi. Come vengono calcolati?

Per ogni keypoint:

1. Si calcola una patch 16x16
2. La patch si suddivide in 16 regioni 4x4
3. Il gradiente (considerando le coordinate polari) per ogni regione è inserito in un istogramma con 8 bin
4. Gli istogrammi di tutte le regioni sono concatenati ottenendo un vettore di 4x4x8=128 elementi
5. Infine, il vettore finale è normalizzato, filtrato per una threshold e rinormalizzato. In questo modo il risultato finale è meno influenzato da piccoli cambiamenti sulla luminosità.



```python
def get_patch_grads(p):
    r1 = np.zeros_like(p)
    r1[-1] = p[-1]
    r1[:-1] = p[1:]

    r2 = np.zeros_like(p)
    r2[0] = p[0]
    r2[1:] = p[:-1]

    dy = r1-r2

    r1[:,-1] = p[:,-1]
    r1[:,:-1] = p[:,1:]

    r2[:,0] = p[:,0]
    r2[:,1:] = p[:,:-1]

    dx = r1-r2

    return dx, dy

def get_histogram_for_subregion(m, theta, num_bin, reference_angle, bin_width, subregion_w):
    hist = np.zeros(num_bin, dtype=np.float32)
    c = subregion_w/2 - .5

    for i, (mag, angle) in enumerate(zip(m, theta)):
        angle = (angle-reference_angle) % 360
        binno = quantize_orientation(angle, num_bin)
        vote = mag

        # binno*bin_width is the start angle of the histogram bin
        # binno*bin_width+bin_width/2 is the center of the histogram bin
        # angle - " is the distance from the angle to the center of the bin 
        hist_interp_weight = 1 - abs(angle - (binno*bin_width + bin_width/2))/(bin_width/2)
        vote *= max(hist_interp_weight, 1e-6)

        gy, gx = np.unravel_index(i, (subregion_w, subregion_w))
        x_interp_weight = max(1 - abs(gx - c)/c, 1e-6)
        y_interp_weight = max(1 - abs(gy - c)/c, 1e-6)
        vote *= x_interp_weight * y_interp_weight

        hist[binno] += vote

    return hist

def get_local_descriptors(kps, octave, w=16, num_subregion=4, num_bin=8):
    descs = []
    bin_width = 360//num_bin

    for kp in kps:
        cx, cy, s = int(kp[0]), int(kp[1]), int(kp[2])
        s = np.clip(s, 0, octave.shape[2]-1)
        kernel = gaussian_filter(w/6) # gaussian_filter multiplies sigma by 3
        L = octave[...,s]

        t, l = max(0, cy-w//2), max(0, cx-w//2)
        b, r = min(L.shape[0], cy+w//2+1), min(L.shape[1], cx+w//2+1)
        patch = L[t:b, l:r]

        dx, dy = get_patch_grads(patch)

        if dx.shape[0] < w+1:
            if t == 0: kernel = kernel[kernel.shape[0]-dx.shape[0]:]
            else: kernel = kernel[:dx.shape[0]]
        if dx.shape[1] < w+1:
            if l == 0: kernel = kernel[kernel.shape[1]-dx.shape[1]:]
            else: kernel = kernel[:dx.shape[1]]

        if dy.shape[0] < w+1:
            if t == 0: kernel = kernel[kernel.shape[0]-dy.shape[0]:]
            else: kernel = kernel[:dy.shape[0]]
        if dy.shape[1] < w+1:
            if l == 0: kernel = kernel[kernel.shape[1]-dy.shape[1]:]
            else: kernel = kernel[:dy.shape[1]]

        m, theta = cart_to_polar_grad(dx, dy)
        dx, dy = dx*kernel, dy*kernel

        subregion_w = w//num_subregion
        featvec = np.zeros(num_bin * num_subregion**2, dtype=np.float32)

        for i in range(0, subregion_w):
            for j in range(0, subregion_w):
                t, l = i*subregion_w, j*subregion_w
                b, r = min(L.shape[0], (i+1)*subregion_w), min(L.shape[1], (j+1)*subregion_w)

                hist = get_histogram_for_subregion(m[t:b, l:r].ravel(), 
                                                theta[t:b, l:r].ravel(), 
                                                num_bin, 
                                                kp[3], 
                                                bin_width,
                                                subregion_w)
                featvec[i*subregion_w*num_bin + j*num_bin:i*subregion_w*num_bin + (j+1)*num_bin] = hist.flatten()

        featvec /= max(1e-6, LA.norm(featvec))
        featvec[featvec>0.2] = 0.2
        featvec /= max(1e-6, LA.norm(featvec))
        descs.append(featvec)

    return np.array(descs)
```

## Assegnazione dell'Orientamento ai Keypoint

L'assegnazione dell'orientamento ai keypoint è un passo fondamentale nell'estrazione di feature SIFT. Questo processo determina la direzione dominante del gradiente in un intorno del keypoint, fornendo informazioni sulla struttura locale dell'immagine.

**Procedura:**

1. **Calcolo dell'istogramma dei gradienti:**
   - Per ogni keypoint, viene definita una finestra di dimensione `w` (tipicamente 16 pixel) centrata sul keypoint.
   - All'interno di questa finestra, vengono calcolati i gradienti di ogni pixel utilizzando un filtro di Sobel o un altro metodo appropriato.
   - Per ogni pixel, viene calcolato l'angolo del gradiente `theta` e la sua magnitudine `m`.
   - L'angolo `theta` viene quantizzato in `num_bins` bin (tipicamente 36).
   - Viene creato un istogramma dei gradienti, dove ogni bin rappresenta un intervallo di angoli.
   - Per ogni pixel, il valore `m` viene aggiunto al bin corrispondente all'angolo `theta`.
   - Il valore `m` viene pesato da un fattore `weight`, che è proporzionale alla magnitudine del gradiente.

2. **Determinazione dell'orientamento dominante:**
   - Il bin con il valore massimo nell'istogramma rappresenta l'orientamento dominante del keypoint.
   - Per migliorare la precisione, viene applicata una funzione di smoothing parabolico (fit_parabola) all'istogramma.
   - La funzione parabolico interpola i valori dell'istogramma e determina l'orientamento preciso del keypoint.

3. **Creazione del descrittore:**
   - Il descrittore del keypoint è un vettore che rappresenta la distribuzione dei gradienti nell'intorno del keypoint.
   - Il descrittore è composto da `num_bins` elementi, dove ogni elemento rappresenta la somma dei valori `m` pesati per ogni bin dell'istogramma.

4. **Gestione di orientamenti multipli:**
   - Per aumentare la robustezza del descrittore, vengono considerati anche i bin vicini al bin massimo.
   - Se un bin ha un valore superiore all'80% del valore massimo, viene considerato un orientamento valido.
   - Questo processo genera più descrittori per ogni keypoint, ognuno con un orientamento leggermente diverso.


Calcolo dei descrittori per tutti i keypoint individuati al passo precedente


```python
feature_descriptors = []

for i, DoG_octave in enumerate(d):
    local_desc = get_local_descriptors(keypoints_with_orientation[i], DoG_octave)
    
    feature_descriptors.append(local_desc)
    
    
print('computed all local descriptors')    
```

    computed all local descriptors
    
### Estrazione di Features

1. **Calcolo del Gradiente:** Viene calcolato il gradiente dell'immagine in ogni pixel, ottenendo informazioni sulla direzione e la magnitudo del cambiamento di luminosità.
2. **Determinazione dei Key Points:** I key points sono punti di interesse nell'immagine, caratterizzati da un alto valore di gradiente. Vengono selezionati i punti con un valore di gradiente superiore all'80% del valore massimo.
3. **Descrizione dei Key Points:** Per ogni key point, viene calcolato un descrittore che rappresenta le caratteristiche locali dell'immagine in quel punto. Il descrittore è un vettore di 128 bit, che rappresenta la distribuzione dei gradienti in una regione di 4x4 pixel attorno al key point.
4. **Costruzione del Vettore di Features:** Per ogni immagine, viene costruito un vettore di features, concatenando i descrittori di tutti i key points. Questo vettore rappresenta l'immagine in modo compatto, catturando le informazioni più importanti per il riconoscimento di oggetti.

### Matching di Features

1. **Calcolo della Distanza:** Viene calcolata la distanza tra i vettori di features di due immagini, utilizzando una metrica di distanza appropriata.
2. **Trovare le Corrispondenze:** Per ogni key point nell'immagine A, viene trovato il key point nell'immagine B con la distanza minima. Questo processo viene ripetuto per tutti i key points nell'immagine A, ottenendo una lista di coppie di key points corrispondenti.
3. **Filtraggio delle Corrispondenze:** Le coppie di key points corrispondenti vengono filtrate per rimuovere le corrispondenze errate, utilizzando un algoritmo di filtraggio appropriato.

### Trasformazione Geometrica

1. **Calcolo della Matrice di Rototraslazione:** Una volta trovate le corrispondenze tra i key points, viene calcolata la matrice di rototraslazione che permette di sovrapporre le due immagini.
2. **Applicazione della Trasformazione:** La matrice di rototraslazione viene applicata all'immagine di partenza, ottenendo un'immagine trasformata che è allineata con l'immagine di riferimento.

