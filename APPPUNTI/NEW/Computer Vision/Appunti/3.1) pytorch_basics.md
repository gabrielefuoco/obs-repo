# Pytorch - Getting started

```python
import torch
import numpy as np

import matplotlib.pyplot as plt
%matplotlib inline
```


```python
# init tensor with random value
x = torch.rand(5, 3)
print(x)
```

    tensor([[0.4236, 0.5705, 0.8534],
            [0.6901, 0.7609, 0.6029],
            [0.4556, 0.7350, 0.0717],
            [0.6287, 0.2511, 0.3292],
            [0.1664, 0.0419, 0.8420]])
    

Ritorna un tensore in cui ogni valore è il risultato di un operazione random


```python
print(x.type()) # <class 'torch.FloatTensor'>
print(x.dtype) # float32

y = np.zeros((4, 4))
print(y.dtype) # float64
```

    torch.FloatTensor
    torch.float32
    float64
    

## Metodi di Base dei Tensori

I metodi di base per i tensori derivano da NumPy e offrono funzionalità essenziali per la manipolazione e l'analisi dei dati. Ecco alcuni esempi:

**`zeros`:** Questo metodo inizializza un tensore con tutti gli elementi impostati a 0.

**`dtype`:** Questo metodo restituisce il tipo di base degli elementi del tensore.

**Creazione di Tensori con Tipo di Base Specifico:** È possibile creare tensori con un tipo di base specifico durante la loro inizializzazione. Ad esempio, per creare un tensore di interi a 32 bit.

**Conversione del Tipo di Base:** È anche possibile convertire un tensore esistente in un altro tipo di base. Ad esempio, per convertire un tensore di numeri a virgola mobile in un tensore di interi.

```python
# tensor of 0s, the ones() method init with value of 1
x = torch.zeros(5, 3, dtype=torch.long)
print(x)
```

    tensor([[0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0]])
    


```python
# build tensor from multi-dimensional list
x = torch.tensor([[5, 3, 1],[3,5,2], [4,4,4],[1,2,3],[0,0,1]])
print(x)
```

    tensor([[5, 3, 1],
            [3, 5, 2],
            [4, 4, 4],
            [1, 2, 3],
            [0, 0, 1]])
    

Possiamo inizializzare un tensore con una lista di numeri: questa definirà inoltre la shape (dimensionalità)


```python
# shape of the tensor
print(x.size())
print(x.shape)
```

    torch.Size([5, 3])
    torch.Size([5, 3])
    
In questo esempio, il tensore `tensor` ha dimensioni `[5, 3]`, quindi sia `tensor.size()` che `tensor.shape` restituiscono `torch.Size([5, 3])`.


## Dimensione e Forma dei Tensori

I metodi `size` e `shape` forniscono informazioni sulla dimensione e la forma di un tensore.

**`size`:** Questo metodo restituisce un oggetto simile a un array che contiene le dimensioni di ogni dimensione del tensore. Ad esempio, per un tensore con dimensioni `[5, 3]`, il metodo `size` restituirà `[5, 3]`.

**`shape`:** Questo metodo è equivalente a `size` e restituisce lo stesso risultato.

**Interpretazione:**

Un tensore con dimensioni `[5, 3]` rappresenta una matrice con 5 righe e 3 colonne. La dimensione del tensore è 2, indicando che ha due dimensioni. La prima dimensione ha valore 5 e la seconda dimensione ha valore 3.

```


#### From numpy


```python
import numpy as np

# Create a numpy array.
x = np.array([[1, 2], [3, 4]])


# Convert the numpy array to a torch tensor.
y = torch.from_numpy(x)

# Convert the torch tensor to a numpy array.
z = y.numpy()

print(type(z))
```

    <class 'numpy.ndarray'>
    

from_numpy: vengono preservate le shape degli array numpy

## Operazioni di base

```python
x = torch.rand([5,3])

y = torch.rand([5,3])

z = x + y

print(z)
```

    tensor([[1.1409, 1.3387, 0.2749],
            [0.8386, 1.5641, 0.7540],
            [0.8538, 0.9361, 0.9627],
            [0.7989, 1.5444, 0.5865],
            [0.3518, 1.0559, 1.3198]])
    

In Python è possibile ridefinire gli operatori tramite i metodi speciali (magic methods) come `__add__`, `__mul__`, ecc. Questo è possibile anche in PyTorch.

La somma di due tensori non è univoca per strutture multidimensionali. Esistono diversi metodi. In questo caso, l'operatore `+` applica un'operazione elemento per elemento.

Ad esempio, `x * x` restituisce un tensore con la stessa forma di `x`, ma non è il vero prodotto tra due matrici. Per il prodotto matriciale, è necessario utilizzare `bmm`, che riceve due tensori e restituisce il tensore risultato. L'operatore corrispondente è `@`.

## Reshape

In PyTorch, il metodo `view` permette di cambiare le dimensioni di un tensore. Un tensore contiene una sequenza di valori e le dimensioni sono solo una loro organizzazione logica. Cambiare le dimensioni non determina un cambiamento nella struttura dati, quindi per tensori di "grandi" dimensioni, cambiare la forma del tensore non ha un impatto sulle prestazioni, a differenza della copia.

```python
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
print(x.shape, y.shape, z.shape)

```

    torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
    

`view` permette di cambiare la forma di un tensore.

Inizializziamo un vettore 4x4.

Chiamiamo `view` con forma 16.

La libreria restituisce due tensori con forma 4x4 il primo e 16 il secondo.

`(-1, 8)` significa: crea un tensore a due dimensioni: la prima 8, la seconda (-1) calcola in automatico la forma.

Un tensore è un numero di celle di memoria contigua.

La memoria è un array continuo, ad ognuna delle quali si accede tramite indirizzo.

`reshape` è una struttura logica, con `view` la alteriamo.

Non possiamo però cambiare il numero totale di celle.

Il numero totale ottenuto dal prodotto delle dimensioni non può cambiare.

La concatenazione effettua la concatenazione su una dimensione scelta, data dalla somma (sulle dimensioni) dei due.

Se `view` modifica solo la struttura logica, è un'operazione veloce. La concatenazione e altri reshape che implicano un cambiamento nella struttura di memoria sono più lenti.


## Sfruttare le GPU

Rispetto a NumPy, il vantaggio principale di PyTorch è la possibilità di eseguire il codice su dispositivi diversi, come CPU o GPU. Ogni framework ha la sua semantica per gestire questa funzionalità.

Ogni tensore risiede nella memoria di uno di questi dispositivi. Il metodo `to` permette di spostarlo dalla memoria di uno all'altro. Tutte le operazioni possono essere eseguite solo quando i tensori risiedono nella memoria dello stesso dispositivo.

Il metodo `.device` permette di istanziare un identificatore del dispositivo.

* `.to(device)`: sposta il tensore sul dispositivo specificato.
* `.cuda`: copia direttamente sulla GPU Nvidia.
* `.cpu`: copia sulla memoria centrale.

Un'altra sintassi è quella di fornire le etichette del dispositivo.

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

Questo codice verifica se una GPU è disponibile e, in caso affermativo, imposta il dispositivo su "cuda", altrimenti su "cpu".


```python
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA device object
else:
    device = torch.device("cpu")          # a cpu device 

x = torch.randn(5, 3)
y = torch.ones([5,3], device=device)  # directly create a tensor on device
x = x.to(device)                       # or just use strings ``.to("cuda")``
z = x + y

print(z)
print(z.to("cpu"))    
    
if torch.cuda.is_available():
    print('move y from cpu to cuda')
    y = torch.ones([5,3])
    y = y.cuda()
```

    tensor([[ 0.3988,  0.0223,  1.4815],
            [-0.8617,  1.2693,  1.1544],
            [ 0.9233, -0.1953,  0.2897],
            [ 0.3232,  1.2209,  1.3106],
            [ 1.0360, -0.1011,  2.3795]])
    tensor([[ 0.3988,  0.0223,  1.4815],
            [-0.8617,  1.2693,  1.1544],
            [ 0.9233, -0.1953,  0.2897],
            [ 0.3232,  1.2209,  1.3106],
            [ 1.0360, -0.1011,  2.3795]])
    

## Differenziazione automatica

```python
x = torch.ones(2, 2, requires_grad=True)

print(x)
```

    tensor([[1., 1.],
            [1., 1.]], requires_grad=True)
    

Il metodo più frequente per addestrare modelli è quello della back-propagation. Questo metodo stima la loss (errore) tra il valore atteso e quello della rete.

Matematicamente, la back-propagation si basa sul calcolo delle derivate parziali dell'errore rispetto ai parametri del modello per stimare il peso di ogni parametro. PyTorch calcola automaticamente queste derivate parziali.

PyTorch mappa ogni operazione in memoria come se fosse un grafo. Ad ogni nodo del grafo, viene memorizzato come calcolare l'inverso di quell'operazione. Quando si deve ottimizzare il modello, si ripercorre a ritroso questo grafo (differenziazione automatica).

Ad esempio, consideriamo la funzione:

$$
\frac{1}{n}(3(x+2)^2)
$$

Al tensore `x` viene aggiunto il parametro `requires_grad=True`. Non tutti i tensori devono calcolare le derivate parziali, quindi questo metodo lo esplicita. Memorizziamo quindi i dati del tensore più quelli di `requires_grad`.

Se vogliamo esplicitamente evitare che venga calcolato il gradiente per quel tensore, possiamo esplicitarlo impostando `requires_grad` a `False`.

```python
x = torch.ones(2, 2, requires_grad=False)
```

In questo modo, PyTorch non calcolerà il gradiente per il tensore `x`.




```python
# tensors derived from other tensors track functions. These functions are the basis for computing the gradient
y = x + 2

print(y)
```

    tensor([[3., 3.],
            [3., 3.]], grad_fn=<AddBackward0>)
    


```python
z = torch.pow(y,2)

t = 3 * z

out = z.mean()

print(f'z = {z}')
print(f't = {t}')
print(f'out = {out}')

print(f'\n\ngrad z = {z.grad_fn}')
print(f'grad t = {t.grad_fn}')
print(f'grad out = {out.grad_fn}')

```

    z = tensor([[9., 9.],
            [9., 9.]], grad_fn=<PowBackward0>)
    t = tensor([[27., 27.],
            [27., 27.]], grad_fn=<MulBackward0>)
    out = 9.0
    
    
    grad z = <PowBackward0 object at 0x1327d58e0>
    grad t = <MulBackward0 object at 0x1327d5a30>
    grad out = <MeanBackward0 object at 0x13282dd00>
    
## Gradienti

Ad ogni tensore è associata una funzione `grad` che indica l'operazione da eseguire per effettuare la backpropagation.

#### Come si usano?
Il gradiente viene conservato in una variabile chiamata `grad`. Per calcolarlo a ritroso, c'è un metodo `backward` che lo fa.

```python
x = torch.ones(2, 2, requires_grad=True)
y = x + 2
z = y * y * 3
out = z.mean()

out.backward()

print(x.grad)
```

Questo codice calcola il gradiente di `x` rispetto a `out`. Il risultato sarà:

```
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
```

Questo significa che il gradiente di `x` rispetto a `out` è 4.5 per ogni elemento di `x`.

**Nota:** Il metodo `backward` può essere chiamato solo una volta per un grafo computazionale. Se si chiama `backward` più volte, il gradiente verrà sovrascritto.


```python
print(f'OLD grad value is {x.grad}')
out.backward()
```

    OLD grad value is None
    

print gradients $d(out)/dx$

il primo nodo è solo un accumulatore



```python
print(x.grad)
```

    tensor([[1.5000, 1.5000],
            [1.5000, 1.5000]])
    

Il calcolo del gradiente tramite il metodo *backward* funziona grazie al concetto di computation graph. Pytorch crea un grafo delle operazioni e naviga a ritroso su questo grafo per effettuare il calcolo.

Ad esempio, la variabile out definita in precedenza costruisce il seguente grafo.

    x = zeros(2, 2)
    y = x + 2
    z = y ^ 2
    out = mean(z)

Si utilizza il package torchviz per creare il Computational Graph a partire dal tensore


```python
import torchviz

torchviz.make_dot(out)
```




    
![svg](output_35_0.svg)
    



# Neural networks in pytorch

## Basic Networks

Esempio di un semplice task di regressione lineare


```python
import torch
import torch.nn as nn

```


```python
# Build DATASET

input_size = 1
output_size = 1

# Toy dataset
x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], 
                    [9.779], [6.182], [7.59], [2.167], [7.042], 
                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)

y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], 
                    [3.366], [2.596], [2.53], [1.221], [2.827], 
                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)

plt.plot(x_train, y_train, 'ro')
plt.show()
```


    
![png](output_39_0.png)
    
## Definizione del dataset

Definiamo un dataset come due liste di punti. `x_train` rappresenta le coordinate dei punti in rosso nel grafico, mentre `y_train` rappresenta le coordinate dei punti in blu.

### Definizione del modello

In PyTorch, la definizione di un modello lineare è realizzata tramite la classe `torch.nn.Linear`. Questa classe contiene gli oggetti di base per definire una retta.

### Funzione di errore

Come funzione di errore, utilizziamo la Mean Squared Error (MSE).

### Aggiornamento dei pesi

Calcoliamo i gradienti per aggiornare i pesi del modello. La strategia per l'aggiornamento dei pesi è definita dall'algoritmo di ottimizzazione scelto. Un esempio di algoritmo di ottimizzazione è il Stochastic Gradient Descent (SGD).

### Costruzione della rete

```python
# Linear regression model
model = nn.Linear(input_size, output_size)

# Loss and optimizer
criterion = nn.MSELoss()

learning_rate = 0.001
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  

```

Nel modulo **nn** sono presenti le implementazioni dei principali layer.

Il layer *Linear* è implementa la funzione

$$ 
y = A^T x + b
$$


## Addestramento del modello
```python
num_epochs = 60

# Convert numpy arrays to torch tensors
inputs = torch.from_numpy(x_train)
targets = torch.from_numpy(y_train)

for epoch in range(num_epochs):
    # Forward pass
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    
    # Backward and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 5 == 0:
        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))
        
```

    Epoch [5/60], Loss: 12.9528
    Epoch [10/60], Loss: 5.3991
    Epoch [15/60], Loss: 2.3389
    Epoch [20/60], Loss: 1.0990
    Epoch [25/60], Loss: 0.5965
    Epoch [30/60], Loss: 0.3929
    Epoch [35/60], Loss: 0.3102
    Epoch [40/60], Loss: 0.2766
    Epoch [45/60], Loss: 0.2629
    Epoch [50/60], Loss: 0.2572
    Epoch [55/60], Loss: 0.2547
    Epoch [60/60], Loss: 0.2536


Questo snippet di codice addestra una rete neurale.

**1. Preparazione dei dati:**

- I dati di input (`x_train`) e output (`y_train`) vengono convertiti in tensori PyTorch.

**2. Ciclo di addestramento:**

- Il ciclo itera per un numero definito di epoche (`num_epochs`).
- Ad ogni epoca:
    - **Forward pass:**
        - Il modello viene applicato ai dati di input (`inputs`) per ottenere le uscite predette (`outputs`).
        - La funzione di perdita (`criterion`) viene calcolata confrontando le uscite predette con i valori target (`targets`).
    - **Backward pass e ottimizzazione:**
        - I gradienti vengono azzerati (`optimizer.zero_grad()`).
        - I gradienti vengono calcolati per tutti i tensori che rappresentano il modello (`loss.backward()`).
        - I pesi del modello vengono aggiornati utilizzando l'algoritmo di ottimizzazione (`optimizer.step()`).
    - **Stampa della perdita:**
        - La perdita viene stampata ogni 5 epoche per monitorare l'andamento dell'addestramento.

**3. Valutazione dell'addestramento:**

- La perdita viene valutata ad ogni epoca per monitorare l'andamento dell'addestramento. Una diminuzione della perdita indica che il modello sta imparando.

**Spiegazione dettagliata:**

- **Forward pass:** Il modello viene applicato ai dati di input per ottenere le uscite predette.
- **Backward pass:** I gradienti vengono calcolati per tutti i tensori che rappresentano il modello. I gradienti indicano la direzione in cui i pesi del modello devono essere modificati per ridurre la perdita.
- **Ottimizzazione:** L'algoritmo di ottimizzazione utilizza i gradienti per aggiornare i pesi del modello. L'obiettivo è ridurre la perdita e migliorare le prestazioni del modello.

**N.B.:**

- Ad ogni iterazione, prima di aggiornare i pesi dei layer, il valore del gradiente deve essere azzerato. Il metodo `zero_grad` dell'ottimizzatore effettua l'operazione per tutti i tensori presenti nel grafo computazionale.


```python
# Plot the graph
predicted = model(torch.from_numpy(x_train)).detach().numpy()
plt.plot(x_train, y_train, 'ro', label='Original data')
plt.plot(x_train, predicted, label='Fitted line')
plt.legend()
plt.show()
```


    
![png](output_47_0.png)
    



```python
# compare predictions
for x,y,hat_y in zip(x_train,y_train,predicted):
    print(f'input = {x[0]:.3f} \t->\t output = {hat_y[0]:.3f} \t (y = {y[0]:.3f})')
```

    input = 3.300 	->	 output = 1.173 	 (y = 1.700)
    input = 4.400 	->	 output = 1.582 	 (y = 2.760)
    input = 5.500 	->	 output = 1.991 	 (y = 2.090)
    input = 6.710 	->	 output = 2.440 	 (y = 3.190)
    input = 6.930 	->	 output = 2.522 	 (y = 1.694)
    input = 4.168 	->	 output = 1.496 	 (y = 1.573)
    input = 9.779 	->	 output = 3.581 	 (y = 3.366)
    input = 6.182 	->	 output = 2.244 	 (y = 2.596)
    input = 7.590 	->	 output = 2.767 	 (y = 2.530)
    input = 2.167 	->	 output = 0.752 	 (y = 1.221)
    input = 7.042 	->	 output = 2.564 	 (y = 2.827)
    input = 10.791 	->	 output = 3.957 	 (y = 3.465)
    input = 5.313 	->	 output = 1.921 	 (y = 1.650)
    input = 7.997 	->	 output = 2.919 	 (y = 2.904)
    input = 3.100 	->	 output = 1.099 	 (y = 1.300)
    

## Logistic regression 

L'esempio seguente utilizza i dati visti in precedenza per costruire un modello di classificazione.

L'idea è addestrare un modello che approssimi questa regola di classificazione:

if $x \geq 6$ 
then class = 1
else class = 0

L'iperpiano $x - 6 = 0$ definisce suddivide il dominio in classe 0 e classe 1.


```python
x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], 
                    [9.779], [6.182], [7.59], [2.167], [7.042], 
                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)

y_train = np.array([
    [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0]
],dtype=np.int)
```


```python
model = nn.Linear(input_size, output_size)

# Loss and optimizer
criterion = nn.BCELoss()

learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  

num_epochs = 20000
log_freq = num_epochs / 10

# Convert numpy arrays to torch tensors
inputs = torch.from_numpy(x_train).float()
targets = torch.from_numpy(y_train).float()

for epoch in range(num_epochs):
    # Forward pass
    outputs = torch.sigmoid(model(inputs))
    loss = criterion(outputs, targets)
    
    # Backward and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % log_freq == 0:
        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))
        
```

    Epoch [2000/20000], Loss: 0.7319
    Epoch [4000/20000], Loss: 0.5256
    Epoch [6000/20000], Loss: 0.3777
    Epoch [8000/20000], Loss: 0.2834
    Epoch [10000/20000], Loss: 0.2209
    Epoch [12000/20000], Loss: 0.1771
    Epoch [14000/20000], Loss: 0.1451
    Epoch [16000/20000], Loss: 0.1208
    Epoch [18000/20000], Loss: 0.1019
    Epoch [20000/20000], Loss: 0.0869
    
### Funzione di Attivazione Sigmoide
Si aggiunge una funzione di attivazione che mappa l'output definito sui reali tra 0 e 1. La funzione sigmoide, usata nell'esempio, assegna il valore 1 agli input superiori ad una certa soglia, 0 agli input inferiori e un valore intermedio per gli input compresi tra le due soglie.

In sostanza, il valore restituito dalla funzione sigmoide rappresenta una sorta di probabilità che l'input sia vicino alla classe 1. 


```python
# boundary line x >= 6
line_y = np.linspace(0, 1., 1000)
line_x = [6] * len(line_y)

predicted = torch.sigmoid(model(torch.from_numpy(x_train))).detach().numpy()

plt.plot(x_train, predicted,'o',label='probabilities')
plt.plot(line_x, line_y, 'r')
plt.show()
```


    
![png](output_54_0.png)
    



```python
# Predictions
for x,y,hat_y in zip(x_train,y_train,predicted):
    print(f'input = {x[0]:.3f} \t->\t Probability of 1 = {hat_y[0]:.3f} \t (y = {y[0]:.3f})')    
```

    input = 3.300 	->	 Probability of 1 = 0.003 	 (y = 0.000)
    input = 4.400 	->	 Probability of 1 = 0.034 	 (y = 0.000)
    input = 5.500 	->	 Probability of 1 = 0.330 	 (y = 0.000)
    input = 6.710 	->	 Probability of 1 = 0.900 	 (y = 1.000)
    input = 6.930 	->	 Probability of 1 = 0.938 	 (y = 1.000)
    input = 4.168 	->	 Probability of 1 = 0.020 	 (y = 0.000)
    input = 9.779 	->	 Probability of 1 = 1.000 	 (y = 1.000)
    input = 6.182 	->	 Probability of 1 = 0.717 	 (y = 1.000)
    input = 7.590 	->	 Probability of 1 = 0.987 	 (y = 1.000)
    input = 2.167 	->	 Probability of 1 = 0.000 	 (y = 0.000)
    input = 7.042 	->	 Probability of 1 = 0.952 	 (y = 1.000)
    input = 10.791 	->	 Probability of 1 = 1.000 	 (y = 1.000)
    input = 5.313 	->	 Probability of 1 = 0.239 	 (y = 0.000)
    input = 7.997 	->	 Probability of 1 = 0.995 	 (y = 1.000)
    input = 3.100 	->	 Probability of 1 = 0.002 	 (y = 0.000)
    

# Classificazione di MNIST

Utilizzato per classificare le immagini.
In questo caso il codice carica in memoria il dataset mnist: dataset in cui si vuole riconoscere un numero scritto a mano, in scala di grigi.
La classe in output dovrà essere una delle 10 classi possibili


```python
import torchvision
import torchvision.transforms as transforms


batch_size = 64

# MNIST dataset 
train_dataset = torchvision.datasets.MNIST(root='data', 
                                           train=True, 
                                           transform=transforms.ToTensor(),  
                                           download=True)

test_dataset = torchvision.datasets.MNIST(root='data', 
                                          train=False, 
                                          transform=transforms.ToTensor())

# Data loader
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)

```


```python
image, label = train_dataset[0]

print(image.shape)
```

    torch.Size([1, 28, 28])
    


```python

for i in range(9):
    plt.subplot(3,3,i+1)
    plt.tight_layout()
    image, label = train_dataset[i]
    plt.imshow(image[0],cmap='gray', interpolation='none')
    plt.title("Class {}".format(label))
    plt.axis('off')
```


![png](output_60_1.png)
    


## Costruzione di una NN Custom

L'obiettivo è costruire un modello composto da due livelli lineari.

Si costruisce un oggetto che estende la classe `nn.Module`, si implementa il metodo `__init__` con l'inizializzazione della rete e si re-implementa il metodo `forward` per effettuare l'operazione implementata dalla rete.

La rete è composta da due layer lineari, ovvero `y=ax+b` ripetuta due volte. All'interno di questa sequenza vi è una funzione di attivazione (senza parametri addestranti). Si utilizza la ReLU (max tra 0 e input), che converte i valori negativi a 0. La ReLU ci permette di apprendere funzioni non lineari.

La classe superiore è `Module`, richiamata tramite `super`. Le shape (dimensioni input) sono:

- `(input size, hidden size)` per il primo layer
- `(hidden size, output size)` per il secondo layer (l'output ha la stessa dimensione dell'input).


```python
class SimpleFullyConnectedNet(nn.Module):
    def __init__(self,input_size, hidden_size, num_classes):
        super(SimpleFullyConnectedNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size) 
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)          
        
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

```


```python
# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

input_size = 28*28
hidden_size = 300
num_classes = 10
batch_size = 100

full_model = SimpleFullyConnectedNet(input_size, hidden_size, num_classes).to(device)
```

Si suddivide il dataset in batch, in questo caso di dimensione 100.
In questo caso facciamo i calcoli sul dispositivo (se c'è la gpu usa quella, altrimenti usa la cpu)

#### TRAIN


```python
# Loss and optimizer
criterion = nn.CrossEntropyLoss()
learning_rate = 0.0005
optimizer = torch.optim.Adam(full_model.parameters(), lr=learning_rate)  


num_epochs = 3

train_losses = []
train_counter = []
test_losses = []
test_counter = [i*len(train_loader.dataset) for i in range(num_epochs + 1)]
```

Primo parametro: param del modello da addestrare


```python
# The number of steps for each epoch, defined by the number of instances divided by the batch size. 
total_step = len(train_loader)

def train(epoch,model,criterion,optimizer,reshape=True):
    for batch_idx, (images, labels) in enumerate(train_loader):  
        # Move tensors to the configured device
        if reshape:
            images = images.reshape(-1, 28*28)
        images = images.to(device)
        labels = labels.to(device)
        
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (batch_idx+1) % 100 == 0:
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' 
                   .format(epoch, num_epochs, batch_idx+1, total_step, loss.item()))
        
        train_losses.append(loss.item())
        train_counter.append(
        (batch_idx*batch_size) + ((epoch-1)*len(train_loader.dataset)))
        
def test(model,criterion,reshape=True):
    test_loss = 0
    correct = 0
    
    with torch.no_grad():
        for images, labels in test_loader:
            if reshape:
                images = images.reshape(-1, 28*28)
                
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            
            correct += (predicted == labels).sum().item()
            
            loss = criterion(outputs,labels,)
            
            test_loss += loss.item()
            
    test_loss /= len(test_loader.dataset)
    test_losses.append(test_loss)
    
    print('\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
```

Il test di un modello di rete neurale consiste nell'utilizzare un dataset di test separato dal dataset di training per valutare le prestazioni del modello su dati mai visti prima.

Durante il test, si utilizza `torch.no_grad()` per disabilitare il calcolo dei gradienti. Questo perché durante il test non si desidera modificare i parametri della rete.

Per determinare l'etichetta di classe prevista dal modello, si utilizza `max(output.data, 1)`. Questo comando restituisce l'indice del valore massimo tra gli output del modello, che corrisponde all'etichetta di classe prevista. Il parametro `1` indica che il massimo deve essere calcolato lungo la seconda dimensione del tensore `output.data`.


```python
test(full_model,criterion)
for epoch in range(1,num_epochs+1):
    train(epoch,full_model,criterion,optimizer)
    test(full_model,criterion)
```

    
    Test set: Avg. loss: 0.0362, Accuracy: 1131/10000 (11%)
    
    Epoch [1/3], Step [100/938], Loss: 0.4702
    Epoch [1/3], Step [200/938], Loss: 0.3004
    Epoch [1/3], Step [300/938], Loss: 0.3555
    Epoch [1/3], Step [400/938], Loss: 0.4425
    Epoch [1/3], Step [500/938], Loss: 0.1958
    Epoch [1/3], Step [600/938], Loss: 0.1273
    Epoch [1/3], Step [700/938], Loss: 0.3757
    Epoch [1/3], Step [800/938], Loss: 0.3388
    Epoch [1/3], Step [900/938], Loss: 0.1628
    
    Test set: Avg. loss: 0.0030, Accuracy: 9450/10000 (94%)
    
    Epoch [2/3], Step [100/938], Loss: 0.1260
    Epoch [2/3], Step [200/938], Loss: 0.1677
    Epoch [2/3], Step [300/938], Loss: 0.0564
    Epoch [2/3], Step [400/938], Loss: 0.1094
    Epoch [2/3], Step [500/938], Loss: 0.1329
    Epoch [2/3], Step [600/938], Loss: 0.1696
    Epoch [2/3], Step [700/938], Loss: 0.2134
    Epoch [2/3], Step [800/938], Loss: 0.0724
    Epoch [2/3], Step [900/938], Loss: 0.0647
    
    Test set: Avg. loss: 0.0021, Accuracy: 9598/10000 (96%)
    
    Epoch [3/3], Step [100/938], Loss: 0.1024
    Epoch [3/3], Step [200/938], Loss: 0.1237
    Epoch [3/3], Step [300/938], Loss: 0.1879
    Epoch [3/3], Step [400/938], Loss: 0.0411
    Epoch [3/3], Step [500/938], Loss: 0.0819
    Epoch [3/3], Step [600/938], Loss: 0.1073
    Epoch [3/3], Step [700/938], Loss: 0.1081
    Epoch [3/3], Step [800/938], Loss: 0.0274
    Epoch [3/3], Step [900/938], Loss: 0.1477
    
    Test set: Avg. loss: 0.0016, Accuracy: 9705/10000 (97%)
    
    




```python
fig = plt.figure()
plt.plot(train_counter, train_losses, color='blue')
plt.scatter(test_counter, test_losses, color='red')
plt.legend(['Train Loss', 'Test Loss'], loc='upper right')
plt.xlabel('number of training examples seen')
plt.ylabel('Cross Entropy Loss')
plt.show()

```


    
![png](output_73_0.png)
    
Questo grafico indica l'andamento della loss (in blu), batch per batch. I punti rossi rappresentano la loss calcolata sul test set, **evidenziando le differenze di performance tra il modello sul dataset di training e sul dataset di test**. 


```python
# Predictions
examples = enumerate(test_loader)
batch_idx, (example_data, example_targets) = next(examples)


for i in range(9):
  plt.subplot(3,3,i+1)
  
  image, label = train_dataset[i]
  with torch.no_grad():
    output = full_model(image.reshape(-1, 28*28).to(device))
    _, predicted = torch.max(output.data, 1)
    
    
  
  plt.tight_layout()
  plt.imshow(image[0], cmap='gray', interpolation='none')
  plt.title("Pred: {}".format(predicted.item()))
  plt.xticks([])
  plt.yticks([])

```


