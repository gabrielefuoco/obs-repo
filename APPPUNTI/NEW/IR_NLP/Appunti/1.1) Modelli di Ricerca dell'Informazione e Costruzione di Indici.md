
## Modello Booleano

Il modello booleano di ricerca utilizza query booleane per interrogare un indice di termini associati a documenti testuali.  
Le query sono espressioni composte da *termini di indice* (parole chiave) e *operatori booleani* (`AND`, `OR`, `NOT`).  La corrispondenza è *binaria*: un documento soddisfa la query o no.  
Questo modello, sebbene utilizzato in sistemi come sistemi di posta elettronica, cataloghi di biblioteche e macOS Spotlight (e storicamente in sistemi legali come WestLaw), presenta significative limitazioni.

### Esempio: WestLaw

WestLaw è il più grande servizio commerciale di ricerca legale a pagamento. Iniziato nel 1975, ha aggiunto la classificazione nel 1992 e una nuova ricerca federata nel 2010. Gestisce decine di terabyte di dati e conta circa 700.000 utenti. La maggior parte degli utenti utilizza ancora query booleane.
 * **Espressione:** `LIMIT! /3 STATUTE ACTION /S FEDERAL /2 TORT /3 CLAIM`
- Pur gestendo enormi quantità di dati e utilizzando query booleane con operatori di prossimità (es. `/3` per "entro 3 parole", `/S` per "nella stessa frase"), dimostra sia i vantaggi che gli svantaggi del modello.  
- Le query possono essere precise e lunghe, permettendo un raffinamento incrementale, ma la rigidità del modello limita la flessibilità.

### Vantaggi del Modello Booleano

* **Query precise e lunghe:** Consentono di specificare con precisione i criteri di ricerca.
* **Operatori di prossimità:** Permettono di definire la relazione spaziale tra i termini.
* **Sviluppo incrementale:** Le query possono essere raffinate gradualmente, aggiungendo termini e operatori.
* **Differenze dalla ricerca sul web:** Le query booleane sono più precise e controllate rispetto alle ricerche sul web.

### Esempio di query

Un esempio illustra le difficoltà: trovare opere di Shakespeare contenenti "Brutus" e "Caesar" ma non "Calpurnia" richiede un'elaborazione complessa e inefficiente, impossibile con un semplice approccio booleano.  
- Si potrebbe cercare "Brutus" e "Caesar" in tutte le opere di Shakespeare, quindi eliminare le righe che contengono "Calpurnia"? No, NOT Calpurnia non è un'operazione semplice.

Un'operazione più sofisticata, come trovare "Romans" vicino a "countrymen", è altrettanto problematica. 
Il modello booleano non supporta il recupero classificato (ranking dei risultati in base alla rilevanza).


![[Pasted image 20241001111231.png|663]]

### Limitazioni del Modello Booleano

Il modello booleano è molto rigido: l'operatore `AND` richiede la presenza di tutti i termini, mentre `OR` richiede la presenza di almeno uno. Questa rigidità presenta diverse limitazioni:

* **Query brevi:** Il modello incoraggia l'utilizzo di query brevi e semplici.
* **Scarsa flessibilità:** Richiede una scelta precisa dei termini di indice, portando a risultati potenzialmente controintuitivi. Non prevede l'espansione della query (né in linea né a runtime) con termini semanticamente equivalenti. Ogni termine di indice è considerato in modo isolato.

##### Difficoltà nel controllo dei risultati:

* **Numero di documenti recuperati:** Tutti i documenti corrispondenti alla query vengono restituiti, senza possibilità di controllo sulla quantità.
* **Classificazione dei risultati:** Tutti i documenti corrispondenti soddisfano la query logicamente, senza possibilità di ordinamento in base alla rilevanza.
* **Feedback di rilevanza:** È difficile utilizzare il feedback dell'utente (indicazione di rilevanza o irrilevanza di un documento) per migliorare la query. Non essendo possibile un ranking dei risultati, non è possibile migliorare il processo di retrieval tramite il feedback di rilevanza.

##### Espressività limitata:

* **Complessi requisiti dell'utente:** Il modello ha difficoltà nell'esprimere richieste complesse. Non permette di regolare l'importanza dei termini nella query.
* **Sparsità:** La rappresentazione dei dati soffre del problema della sparsità, comune anche ad altri modelli di retrieval.

**Conclusioni:** Il modello booleano è adatto solo quando le esigenze dell'utente sono esprimibili tramite query corte e semplici, e l'obiettivo è solo determinare la presenza o l'assenza di un elemento.

## Estensione del modello booleano

### Incorporare metodi di classificazione

* **Obiettivo:** Preservare il modello booleano, ma aggiungere un meccanismo di ordinamento dei risultati.
* **Metodo:**
	- Assegnando pesi (in [0,1]) ai termini nei documenti e nelle query, si può calcolare la somiglianza (es. Jaccard) tra query e documenti, ordinando i risultati in base a questa. 
 * **Feedback di rilevanza:** Selezionare i documenti in base alla loro classificazione. Permette di raffinare ulteriormente l'ordinamento.
* **Risultato:** Si ottiene un ordine all'interno della lista dei risultati.

### Insiemi fuzzy

* **Concetto:** Rilassare i confini (boundaries) degli insiemi utilizzati nel recupero booleano.
	 * **Standard:** d è in A o non è in A.
	 * **Fuzzy:** d è più o meno in A.
* **Grado di appartenenza:** wA è il grado di appartenenza di un elemento all'insieme A. Permette di definire alcune **operazioni:**
	 * **Intersezione (and):** $w(A∩B) = min(wA, wB)$
	 * **Unione (or):** $w(A∪B )= max(wA, wB)$

### MMM: Mixed Min and Max Model

Il modello MMM (Mixed Min and Max) calcola la similarità $S$ tra una query e un documento usando una combinazione lineare del massimo e del minimo dei pesi dei termini del documento. 

 Sia $d$ il documento con pesi dei termini $w_1, w_2, \dots, w_n$ corrispondenti ai termini $t_1, t_2, \dots, t_n$.
  
 **Query disgiuntiva:**
- Per una query disgiuntiva $q_{or} = (t_1 \lor t_2 \lor \dots \lor t_n)$, la similarità è definita come:  
	-  $S(q_{or}, d) = \lambda_{or} \cdot \max(w_1, \dots, w_n) + (1 - \lambda_{or}) \cdot \min(w_1, \dots, w_n)$.  
- Il parametro $\lambda_{or}$ ($0 \le \lambda_{or} \le 1$) controlla il peso relativo del massimo e del minimo; 
	- in logica booleana standard, $\lambda_{or} = 1$. 
- Se la query è puramente disgiuntiva, solo il peso del primo termine viene considerato.

 **Query congiuntiva:**
 - Per una query congiuntiva $q_{and} = (t_1 \land t_2 \land \dots \land t_n)$, la similarità è:
	 - $S(q_{and}, d) = \lambda_{and} \cdot \min(w_1, \dots, w_n) + (1 - \lambda_{and}) \cdot \max(w_1, \dots, w_n)$. 
 - Analogamente, $\lambda_{and}$ ($0 \le \lambda_{and} \le 1$) bilancia il peso del minimo e del massimo, assumendo il valore 1 nella logica booleana standard.


### Modello di Paice

* **Differenza da MMM:** Tiene conto di tutti i pesi dei documenti, mentre MMM considera solo i pesi massimi e minimi.
* **Vantaggio:** Migliora la risposta alla query.
* **Svantaggio:** Costo computazionale più alto rispetto a MMM.

### Modello P-norma

**Caratteristiche:**
 * Documenti con pesi dei termini.
 * Termini di query con pesi associati ai termini.
 * Operatori con coefficienti che indicano il grado di rigore.
**Calcolo della somiglianza:**
 * Considera ogni documento e query come punti multidimensionali (considerando due schemi di pesatura).

## Sparsità

La sparsità della rappresentazione matrice termine-documento è un problema significativo

**Rappresentazione di matrici termine-documento:** Adottando un modello di rappresentazione di questo un modello poco sparso è irrealistico.

**Esempio:** Considera un insieme di **N = 1 milione di documenti**, ognuno con circa **1000 parole.**(Un esempio simile è la market-basket-analisys)
Assumiamo una media di **6 byte/parola**, inclusi spazi e punteggiatura.
* Questo corrisponde a **6 GB di dati** nei documenti.
Supponiamo che ci siano **M = 500K termini distinti** tra questi documenti.
##### Problemi:

* +Una matrice di incidenza termine-documento di dimensioni **500K x 1M** avrebbe mezzo Trillion (milione di miliardi) di 0 e 1.
##### Nella realtà:

* La matrice ha **non più di un miliardo di 1**.
* Le matrici di incidenza termine-documento sono **estremamente sparse**.
##### Rappresentazione migliore:

**Registrare solo le posizioni 1.** Desiderata per una struttura dati:
* Capacità di rappresentare concetti e relazioni.
* Capacità di supportare la localizzazione di questi concetti nella collezione di documenti.

Una soluzione ottimale è l'utilizzo di un **indice inverso**, che mappa ogni termine all'insieme dei documenti in cui appare.

## Indice Inverso

**Definizione:** 
L'indice inverso mappa ogni termine all'insieme dei documenti in cui compare.
Offre vantaggi significativi in termini di efficienza di memorizzazione e velocità di ricerca, gestendo efficacemente la sparsità dei dati.  

**Costruzione:**
 * **Identificazione dei documenti indicizzati:** Per ogni termine:
	 * Identifica tutti i documenti che sono indicizzati da quel termine.
	 * Memorizza gli ID di quei documenti (docID).
 * **Creazione della matrice termine-documento:**
	 * Inizialmente, rappresenta l'indice inverso come un array di documenti indicizzati.
	 * Trasponi l'array per ottenere una matrice termine-documento.

**Struttura:**
- La struttura dati più comune è la **lista di postings**, un elenco di docID per ogni termine, spesso implementata con liste concatenate o array di lunghezza variabile, a seconda delle esigenze di performance e di aggiornamento.  
- La memorizzazione può avvenire su disco (sequenza continua di postings) o in memoria.


 **Considerazioni:**
 * La scelta della struttura dati per l'indice inverso dipende dalle esigenze specifiche dell'applicazione.
 * È importante considerare i compromessi tra efficienza di memorizzazione, velocità di accesso e facilità di aggiornamento.

###### Il *core di un indice* è composto da due elementi principali:

* **Dizionario:** Contiene le parole del corpus, ciascuna associata a un puntatore alla lista di posting corrispondente.
* **Liste di posting:** Ogni lista contiene i documenti in cui la parola corrispondente compare, insieme alla sua frequenza nel documento.

Questo sistema di indicizzazione è doppiamente indicizzato:

* **Per chiave:** Il dizionario permette di accedere rapidamente alla lista di posting per una parola specifica.
* **Per documento:** Le liste di posting permettono di identificare rapidamente i documenti in cui una parola specifica compare, insieme alla sua frequenza nel documento

![[1) Intro-20241003104139467.png|393]]

L'**indexer** raccoglie i postings per ogni token, ovvero l'insieme di documenti in cui quel token appare. 

L'**input dell'indexer** è uno stream di coppie termine-ID documento. Questo stream rappresenta l'elenco di tutti i termini trovati nei documenti, insieme all'ID del documento in cui sono stati trovati. 

### Passaggi dell'indexer

Il processo di indicizzazione (indexing) prevede quattro fasi:

1. **Generazione di una sequenza di coppie (Termine, docID):** L'indexer analizza i documenti e genera una sequenza di coppie, dove ogni coppia rappresenta un termine e l'ID del documento in cui il termine appare.

2. **Ordina per Termine:** La sequenza di coppie viene ordinata in base al termine.

3. **Ordina per docID:** Le coppie con lo stesso termine vengono raggruppate e ordinate in base al docID.

4. **Unisci voci multiple:** Se un termine appare più volte nello stesso documento, le voci vengono unite in un'unica voce, mantenendo la frequenza del termine nel documento.

5. **Aggiungi frequenza del documento:** Per ogni voce, viene aggiunta l'informazione sulla frequenza del termine nel documento.

6. **Dividi in:**
	 * **Dizionario:** Contiene tutti i termini distinti trovati nei documenti, con il loro corrispondente termID.
	 * **Postings:** Contiene le liste di postings per ogni termine, ovvero l'insieme di documenti in cui il termine appare, con la loro frequenza. 

## Elaborazione delle query

**Esempio:** L'elaborazione di query come "*Brutus AND Caesar*" avviene intersecando le liste di *postings* dei singoli termini.
L'algoritmo `INTERSECT` (vedi sotto) esegue questa intersezione in tempo lineare rispetto alla somma delle dimensioni delle liste, grazie all'ordinamento delle liste stesse.

##### Passaggi:

1. **Individua Brutus nel Dizionario:**
 * Recupera i suoi postings (lista di documenti in cui compare Brutus).
2. **Individua Caesar nel Dizionario:**
 * Recupera i suoi postings (lista di documenti in cui compare Caesar).
3. **Unisci i due postings (interseca gli insiemi di documenti):**
 * Attraversa i due postings simultaneamente, in tempo lineare nel numero totale di voci di postings. 
	 * Garantito dal fatto che manteniamo le due liste ordinate
 * Questo significa che l'algoritmo impiega un tempo proporzionale al numero di documenti in cui compare Brutus più il numero di documenti in cui compare Caesar.
		![[1) Intro-20241003104909533.png|438]]
**Nota:** Questo processo di unione dei postings è un esempio di come vengono elaborate le query booleane (AND, OR, NOT) nei motori di ricerca. 

```
INTERSECT(p1, p2)
   answer ← ⟨ ⟩
   while p1 ≠ NIL and p2 ≠ NIL
       do if docID(p1) = docID(p2)
             then ADD(answer, docID(p1))
                  p1 ← next(p1)
                  p2 ← next(p2)
          else if docID(p1) < docID(p2)
             then p1 ← next(p1)
             else p2 ← next(p2)
  return answer
```

## Ottimizzazione dell'elaborazione delle query

**Scenario:** Una query composta da una congiunzione AND di *n* termini.
![[1) Intro-20241003105153597.png]]
**Obiettivo:** Determinare l'ordine ottimale per l'elaborazione della query.

##### Strategia:

Per query con più termini congiunti da `AND`, l'ordine di elaborazione influenza significativamente l'efficienza.  La strategia ottimale consiste nell'elaborare i termini in ordine crescente di frequenza documentale, iniziando dal termine meno frequente. 
Questo minimizza la dimensione degli insiemi intermedi durante le intersezioni

##### Esempio:

* Se la query è "Brutus AND Caesar AND Antony", e la frequenza documentale è:
 * Brutus: 1000 documenti
 * Caesar: 500 documenti
 * Antony: 200 documenti
* L'ordine ottimale sarebbe: Antony, Caesar, Brutus.

##### Query booleane arbitrarie:

* **Esempio:** (Brutus OR Caesar) AND NOT (Antony OR Cleopatra)
* **Strategia:**
	* Per query booleane arbitrarie (incluse `OR` e `NOT`), si stima la dimensione degli insiemi `OR` (come somma delle frequenze dei termini) e si procede in ordine crescente di queste stime.  Il tempo di elaborazione rimane lineare rispetto al numero totale di voci nei *postings*.

##### Nota:

* Il tempo di elaborazione è "lineare" rispetto al numero totale di voci nei postings.
* L'ordine di elaborazione può influenzare significativamente l'efficienza, soprattutto per query complesse.

## Query di frase

##### Importanza:

* Le query di frase sono un elemento chiave per la "ricerca avanzata".
* Sono facilmente comprensibili dagli utenti.
* L'obiettivo è rispondere a query come "Dipartimenti Unical" come una frase completa.
* La frase "*Ci sono 14 dipartimenti presso Unical*" non è una corrispondenza, poiché non corrisponde alla query di frase.
* Per questo tipo di query, non è sufficiente memorizzare solo coppie (Termine, docID).

##### Approcci:

* **Indici biword:**
 * Indizza ogni coppia consecutiva di termini nel testo come una frase (come se avvessimo un *n-gram* che preserva le parole con $n=2$).
 * **Esempio**: "*Amici, Romani, Concittadini*" genera i biword "*amici romani*" e "*romani concittadini*".
 * Ogni biword diventa un termine del dizionario.
 * L'elaborazione delle query di frase a due parole è immediata.

* **Scomposizione di frasi più lunghe:**
 * "Corsi del dipartimento DIMES" può essere scomposto nella query booleana sui biword: *"dipartimento DIMES AND corsi dipartimento".*
 * Senza i documenti, non è possibile verificare se i documenti che corrispondono alla query booleana contengano effettivamente la frase.
	 * Rischio di falsi positivi.
 * Espansione dell'indice a causa di un dizionario più grande.
	 * Inattuabile per più di due parole, anche per i biword (aumenta di molto la complessità).
 * **Gli indici biword non sono la soluzione standard, ma possono essere parte di una strategia composita.**

### Etichettatura Morfologica (POST) e Biword Estensi

* **Analisi del testo:** suddividere il testo in termini e assegnare a ciascun termine la sua categoria grammaticale attraverso l'analisi POST (*Part-of-Speech tagging*)
* **Nomi (N) e Articoli/Preposizioni (X):** classificare i termini come nomi o articoli/preposizioni.
* **Biword Estensi:** identificare le sequenze di termini della forma $NX*N$ creando "**biword estesi**"  Questo permette di gestire frasi più complesse rispetto agli indici biword semplici.
###### Esempio: "il cacciatore nella segale"

* **POST:** $N X X N$
* **Biword Estenso:** "cacciatore segale"

#### Elaborazione delle Query

* **Analisi della query:** suddividere la query in termini N e X.
* **Segmentazione in biword:** segmentare la query in biword estesi.
* **Ricerca nell'indice:** cercare i biword estesi nell'indice.

## Indici Posizionali

Gli indici posizionali memorizzano, per ogni termine, le posizioni di ogni occorrenza in cui compaiono i token del termine nei documenti.

```
<termine, numero di documenti contenenti il termine;
doc1: posizione1, posizione2 … ;
doc2: posizione1, posizione2 … ;
…>
```

Questo permette di supportare query di frase e di prossimità, non gestibili dagli indici biword

#### Algoritmo di Unione Ricorsivo

L'algoritmo di unione ricorsivo è utilizzato per processare le query di frase, gestendo la complessità delle posizioni, con la necessità di gestire più della semplice uguaglianza.
* **Query di frase:** estraggono le voci dell'indice invertito per ogni termine distinto (es. "to", "be", "or", "not").
* **Fusione delle liste di doc:posizione:** uniscono le liste di doc:posizione per elencare tutte le posizioni con la frase completa (es. "to be or not to be").

#### Indici Posizionali 

* **Utilizzo degli indici posizionali:** gli indici posizionali possono essere utilizzati per le query di prossimità.
* **Limiti degli indici biword:** gli indici biword non possono essere utilizzati per le query di prossimità.
* **Algoritmo di unione ricorsivo:** utilizzato per le query di frase, ma con la necessità di gestire più della semplice uguaglianza.

### Dimensione dell'indice posizionale

Un indice posizionale espande in modo sostanziale l'archiviazione dei postings.
* Bisogna avere una voce per ogni occorrenza, non solo una per documento.
* La dimensione dell'indice dipende dalla dimensione media del documento. (Anche se gli indici possono essere compressi)

##### Regola empirica:

* Un indice posizionale è 2-4 volte più grande di un indice non posizionale.
* La dimensione dell'indice posizionale è il 35-50% del volume del testo originale.
* Tuttavia, un indice posizionale è lo standard di utilizzo a causa della potenza e dell'utilità delle query di frase e prossimità, sia utilizzate esplicitamente che implicitamente in un sistema di recupero di ranking. 

## Costruzione dell'indice basata sull'ordinamento

La costruzione in memoria, pur fattibile per indici relativamente piccoli (es. 100 milioni di postings), non è scalabile per collezioni di dimensioni reali, ad esempio quella del New York Times (150 anni di notizie).  Questo richiede l'utilizzo del disco, con conseguenti vincoli di performance dovuti all'accesso ai dati.

### Scalabilità della costruzione dell'indice

* La costruzione dell'indice in memoria non è scalabile.
* Non è possibile inserire l'intera collezione in memoria, ordinarla e poi riscriverla.

### Basi hardware

I sistemi IR moderni sfruttano server con ampia memoria (GB) e spazio su disco molto maggiore (2-3 ordini di grandezza).  La tolleranza ai guasti è costosa, quindi si preferisce l'utilizzo di molte macchine standard. L'accesso alla memoria è significativamente più veloce di quello al disco, e l'I/O del disco è ottimizzato leggendo e scrivendo blocchi di dati (8KB-256KB), minimizzando il numero di accessi.

#### Ordinamento usando il disco come "memoria"?

* Non possiamo usare lo stesso algoritmo di costruzione dell'indice per collezioni più grandi, usando il disco invece della memoria, perchè ordinare 100 milioni di record su disco è troppo lento (implica troppe ricerche su disco).
* Abbiamo bisogno di un algoritmo di ordinamento esterno.

## Block Sort-Based Indexing: Indicizzazione basata sull'ordinamento a blocchi

L'ordinamento diretto su disco di grandi quantità di dati (es. 100 milioni di record) è inefficiente:
- L'algoritmo *BSBI* risolve questo problema con un approccio basato sull'ordinamento esterno.  
- *BSBI* suddivide i dati in blocchi più piccoli (es. 10 milioni di record), che vengono ordinati in memoria (es. con Quicksort) e scritti su disco.  
- Successivamente, questi blocchi ordinati (run) vengono uniti in un unico indice ordinato tramite un algoritmo di merge esterno.

**Idea di base dell'algoritmo**:
 * Accumulare i postings per ogni blocco, ordinarli e scriverli su disco.
 * Quindi unire i blocchi in un unico ordine ordinato.

```
BSBIndexConstruction()
1   n ← 0
2   while (all documents have not been processed)
3       do n ← n + 1
4          block ← ParseNextBlock()
5          BSBI-Invert(block) //costruisce inverted index
6          WriteBlockToDisk(block, fn)  //scriviamo sul disco, presuppone la   ù
           presenza di una struttura di blocco sul disco
7   MergeBlocks(f_1, ..., f_n; f_merged)

```

#### Ordinamento di 10 blocchi di 10 milioni di record.

L'ordinamento di 10 blocchi da 10 milioni di record ciascuno avviene in due fasi: ordinamento interno di ogni blocco (usando ad esempio Quicksort con complessità $O(N \log N)$) e successiva fusione delle run ordinate tramite un algoritmo di ordinamento esterno (come un merge sort esterno). 
L'utilizzo dello spazio su disco può essere ottimizzato evitando la necessità di due copie complete dei dati.

## Tecniche di Merge

Esistono due approcci principali per unire run ordinate: il *merge binario* e il *multi-way merge*.

##### Merge Binario:

Si possono effettuare merge binari, utilizzando un albero di merge con $\log_2(n)$ livelli, dove *n* è il numero di run. Ad esempio, con 10 run, si avrebbero 4 livelli ($\log_2(10) \approx 4$). 
Ad ogni livello, si leggono blocchi di run (es. 10 milioni di elementi per blocco), si uniscono e si riscrivono su disco. Invece di un unico merge finale, si adotta una struttura ad albero con merge parziali a ogni livello:

1. **Partizionamento:** I dati vengono suddivisi in partizioni.
2. **Merge parziale:** Ogni partizione viene caricata in memoria e sottoposta a merge, generando un nuovo indice.
3. **Aggiornamento dell'indice:** L'indice aggiornato viene utilizzato per il merge del livello successivo.

![[1) Intro-20241003112202805.png]]

##### Multi-way Merge:

**Assunzione:** Si assume una condivisione solo parziale del lessico tra i vari documenti.

Un approccio più efficiente è il multi-way merge, che legge simultaneamente da tutti i blocchi. Questo richiede:

* L'apertura simultanea di tutti i file di blocco.
* Un buffer di lettura per ogni blocco e un buffer di scrittura per il file di output.
* L'utilizzo di una coda di priorità per selezionare, ad ogni iterazione, il `termID` più basso non ancora elaborato.
* L'unione delle liste di postings corrispondenti a quel `termID` (provenienti dai vari blocchi) e la scrittura del risultato.

Questo metodo è efficiente a condizione che si leggano e si scrivano blocchi di dimensioni adeguate.

##### Problema della Crescita del Lessico:

La gestione del lessico durante il merge presenta sfide:

* **Gestione dei termini e degli ID:** È fondamentale gestire correttamente i termini (token pre-processati) e i loro identificativi (ID).
* **Compressione con perdita:** La compressione con perdita può essere applicata solo a termini non essenziali per la comprensione del testo.
* **Valutazione dei token:** Sono necessarie strategie per valutare l'importanza dei token e decidere quali possono essere compressi con perdita.

## SPIMI: Indicizzazione in memoria a passaggio singolo (approccio lazy)

Questo algoritmo risolve il problema della crescita del lessico durante l'indicizzazione di grandi collezioni.  Supera le limitazioni degli algoritmi basati sull'ordinamento, che potrebbero non riuscire a mantenere in memoria la mappatura (termine, termID) e il dizionario in crescita

**Idee chiave: sono complementari:**
* **Idea chiave 1:** *Generare dizionari separati per ogni blocco*. Non c'è bisogno di mantenere la mappatura termine-termID tra i blocchi (mapping across block).
* **Idea chiave 2:** *Non ordinare*. Accumulare i postings nelle liste di postings man mano che si verificano.

Questo genera indici invertiti completi per ogni blocco, successivamente mergeabili in un unico indice.  SPIMI è più veloce e risparmia memoria perché evita l'ordinamento e la memorizzazione dei `termID` intermedi.  
In pratica, le liste di postings sono strutture dinamiche riallocate all'occorrenza.

### SPIML-Invert (token_stream)

```python

1.  output_file = NEWFILE()
2.  dictionary = NEWHASH()
3.  while (free memory available)
4.      do token ← next(token_stream)
5.      if term(token) ∉ dictionary
6.          then postings_list = ADDToDICTIONARY(dictionary, term(token))
7.      else postings_list = GETPOSTINGSLIST(dictionary, term(token))
8.      if full(postings_list)
9.          then postings_list = DOUBLEPOSTINGSLIST(dictionary, term(token))
10.     ADDToPOSTINGSLIST(postings_list, docID(token))
11.     sorted_terms ← SORTTERMS(dictionary)
12.     WRITEBLOCKToDISK(sorted_terms, dictionary, output_file)
13.     return output_file

# Linea 5: il token raw viene pre-processato; viene ricondotto a un index-term.
# Linea 10: il posting (occorrenza del termine) nel documento viene aggiunto immediatamente.
# Linee 8-9: se la lista di postings raggiunge la dimensione limite, viene raddoppiata.
# Linea 11: L'ordinamento dei termini avviene dopo aver raccolto tutti i postings per un blocco.
```
L'algoritmo itera finché c'è memoria disponibile, aggiungendo i token al dizionario e alle liste di postings.  Quando una lista è piena, viene raddoppiata.  Infine, il dizionario ordinato e le liste di postings vengono scritti su disco.
## Indicizzazione Distribuita

L'indicizzazione distribuita prevede l'esecuzione di due task paralleli: il parsing e l'inversione dell'indice. 
Un'unica macchina master coordina il processo, suddividendo l'indice in una serie di task paralleli. 
Ogni split rappresenta un insieme di documenti gestito come blocchi. La macchina master assegna i ruoli ai diversi nodi del sistema.

### Parsing Distribuito

Il master assegna porzioni di documenti (split) a macchine parser inattive. Ogni parser legge i documenti, estrae le coppie (termine, documento ID) e le scrive in *j* partizioni (fold) basate su una suddivisione lessicografica dei termini (*es. a-f, g-p, q-z per j=3*).

### Inversione Distribuita dell'Indice

Ogni inverter raccoglie le coppie (termine, documento ID) per una specifica partizione, ordina le liste di occorrenze (postings) per termine e le scrive su disco. 

![[1) Intro-20241003093756427.png]]

La figura mostra come l'indicizzazione distribuita possa essere vista come un'istanza particolare del modello MapReduce.

##### Fase di Map:

* Partendo dalla collezione di documenti in input, la fase di map produce liste di coppie (termine, documento).

##### Fase di Reduce:

* La fase di reduce prende le liste di occorrenze (coppie termine-documento) e produce le liste di postings, ovvero le liste di documenti in cui un termine compare. 

![[1) Intro-20241003093804693.png]]
## Indicizzazione Distribuita

L'algoritmo di costruzione dell'indice è un'istanza di MapReduce.

* **MapReduce:** Un framework robusto e concettualmente semplice per il calcolo distribuito, che permette di eseguire calcoli complessi senza dover scrivere codice per la parte di distribuzione.
* **Sistema di indicizzazione di Google (circa 2002):** Composto da una serie di fasi, ciascuna implementata in MapReduce.

### Schema per la costruzione dell'indice in MapReduce

**Funzioni map e reduce:**
 * `map`: input → list(k, v)
 * `reduce`: (k,list(v)) → output
 **Istanza dello schema per la costruzione dell'indice:**
 * `map`: collection → list(termID, docID)
 * `reduce`: (<termID1, list(docID)>, <termID2, list(docID)>, …) → (postings list1, postings list2, …)

### Gestione di Documenti Dinamici

Per gestire l'arrivo, l'eliminazione e la modifica di documenti, si possono adottare diverse strategie:

* **Indice Principale e Ausiliario:**  Mantenere un indice principale e uno o più indici ausiliari per i nuovi documenti. La ricerca avviene su entrambi, combinando i risultati.
* **Eliminazione:** Utilizzare un vettore di bit per indicare i documenti eliminati, filtrando i risultati di ricerca.
* **Re-indicizzazione:** Periodicamente, re-indicizzare tutto in un unico indice principale.  L'aggiornamento di termini esistenti richiede modifiche alle liste di postings, mentre i nuovi termini vanno aggiunti al dizionario.

##### Problemi con gli indici principali e ausiliari

La semplice fusione di un indice ausiliario in uno principale, sebbene efficiente con file separati per ogni lista di postings (simile ad una semplice append), diventa inefficiente con un unico grande file per l'indice, come spesso accade nella pratica.  Questo porta a problemi di performance durante i merge frequenti.

## Fusione Logaritmica

La fusione logaritmica è una tecnica di ordinamento che utilizza una serie di indici di dimensioni crescenti per ordinare un insieme di dati. 

##### Principio di funzionamento:

1. **Serie di indici:** Si mantiene una serie di indici, ciascuno con una dimensione doppia rispetto al precedente. Ad esempio, si potrebbe avere un indice di dimensione 1, 2, 4, 8, 16, e così via.
2. **Memoria e disco:** L'indice più piccolo $(Z_0)$ è mantenuto in memoria, mentre gli indici più grandi $(I_0, I_1, ...)$ sono memorizzati sul disco.
3. **Fusione e scrittura:** Quando l'indice $Z_0$ diventa troppo grande (supera la sua capacità), viene scritto sul disco come $I_0$. In alternativa, se $I_0$ esiste già, $Z_0$ viene unito con $I_0$ per formare $Z_1$.
4. **Iterazione:** Se $Z_1$ non è ancora troppo grande, viene mantenuto in memoria. Altrimenti, viene scritto sul disco come $I_1$. Se $I_1$ esiste già, $Z_1$ viene unito con $I_1$ per formare $Z_2$.
5. **Ripetizione:** Questo processo di fusione e scrittura viene ripetuto fino a quando tutti i dati non sono stati ordinati.

![[1) Intro-20241007154640070.png|436]]

![[1) Intro-20241007154611506.png|450]]
## Indicizzazione Dinamica

#### Indice Ausiliario e Principale

* **Fusione T/n:**
 * Si utilizzano indici ausiliari di dimensione $n$ e un indice principale con $T$ postings.
 * Il tempo di costruzione dell'indice è $O\left( \frac{T^2}{n} \right)$ nel caso peggiore, poiché un posting potrebbe essere toccato $\frac{T}{n}$ volte.

#### Fusione Logaritmica

* **Efficienza:** Ogni posting viene fuso al massimo $O\left( log\left( \frac{T}{n} \right) \right)$ volte, con una complessità di $O\left( T \cdot log\left( \frac{T}{n} \right) \right)$
* **Vantaggi:** La fusione logaritmica è molto più efficiente per la costruzione dell'indice rispetto alla fusione $\frac{T}{n}$.
* **Svantaggi:** L'elaborazione delle query richiede la fusione di $O\left( log\left( \frac{T}{n} \right) \right)$ indici, mentre con un solo indice principale e ausiliario la complessità è $O(1)$.

## Ulteriori Problemi con Più Indici

Mantenere le statistiche a livello di collezione con più indici è complesso. Ad esempio, per la correzione ortografica, è difficile scegliere l'alternativa corretta con il maggior numero di risultati. 

* **Problema:** Come mantenere le migliori alternative con più indici e vettori di bit di invalidazione?
* **Possibile soluzione:** Ignorare tutto tranne l'indice principale per l'ordinamento.

Il ranking dei risultati si basa su queste statistiche, rendendo la loro gestione cruciale.

### Indicizzazione Dinamica nei Motori di Ricerca

I motori di ricerca effettuano l'indicizzazione dinamica con:

* **Modifiche incrementali frequenti:** es. notizie, blog, nuove pagine web.
* **Ricostruzioni periodiche dell'indice da zero:** L'elaborazione delle query viene commutata sul nuovo indice e il vecchio indice viene eliminato.

### Requisiti per la Ricerca in Tempo Reale

La ricerca in tempo reale richiede:

* **Bassa latenza:** Elevata produttività di valutazione delle query.
* **Elevato tasso di ingestione:** Immediata disponibilità dei dati.
* **Letture e scritture concorrenti:** Gestione di letture e scritture simultanee dell'indice.
* **Dominanza del segnale temporale:** Priorità ai dati più recenti.

## Costruzione dell'Indice: Riepilogo

### Indicizzazione basata sull'ordinamento

Esistono diverse tecniche di indicizzazione basata sull'ordinamento:

* **Inversione in memoria naive.**
* **Indicizzazione basata sull'ordinamento bloccato (BSBI).**
* **L'ordinamento per fusione è efficace per l'ordinamento basato su disco rigido (evita le ricerche!).**

### Indicizzazione in memoria a passaggio singolo (SPIMI)

* **Nessun dizionario globale.**
* **Genera un dizionario separato per ogni blocco.**
* **Non ordinare i postings.**
* **Accumulare i postings nelle liste di postings man mano che si verificano.**

