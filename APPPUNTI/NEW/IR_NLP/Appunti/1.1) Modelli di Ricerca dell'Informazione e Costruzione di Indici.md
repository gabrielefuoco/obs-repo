
## Modello Booleano

Il modello booleano di ricerca confronta una query booleana con gli insiemi di termini utilizzati per indicizzare il contenuto testuale.

##### Query Booleana:

* **Espressione:** Combinazione di termini di indice tramite operatori booleani.
* **Struttura:** L'espressione contiene:
 * **Termini di indice:** Parole chiave estratte dai documenti indicizzati.
 * **Operatori booleani:** `AND`, `OR`, e `NOT`, applicati ai termini. Il modello è rigido e la corrispondenza è binaria: un documento soddisfa la condizione o no.

##### Applicazioni:

Molti sistemi di ricerca utilizzano ancora il modello booleano, tra cui:

* Sistemi di posta elettronica
* Cataloghi di biblioteche
* macOS Spotlight

### Esempio: WestLaw

WestLaw è il più grande servizio commerciale di ricerca legale a pagamento. Iniziato nel 1975, ha aggiunto la classificazione nel 1992 e una nuova ricerca federata nel 2010. Gestisce decine di terabyte di dati e conta circa 700.000 utenti. La maggior parte degli utenti utilizza ancora query booleane.

##### Esempi di Query:

* **Query 1:** Qual è il termine di prescrizione nei casi che coinvolgono il Federal Tort Claims Act?

 * **Espressione:** `LIMIT! /3 STATUTE ACTION /S FEDERAL /2 TORT /3 CLAIM`

 * **Spiegazione:** `/3` indica "entro 3 parole", `/S` indica "nella stessa frase". Questi operatori di prossimità aggiungono informazioni contestuali.

* **Query 2:** Requisiti per le persone disabili per poter accedere a un luogo di lavoro.

 * **Espressione:** `disabl! /p access! /s work-site work-place (employment /3 place`

 * **Nota:** Lo spazio tra i termini indica una disgiunzione (OR), non una congiunzione (AND).

### Vantaggi del Modello Booleano

* **Query precise e lunghe:** Consentono di specificare con precisione i criteri di ricerca.
* **Operatori di prossimità:** Permettono di definire la relazione spaziale tra i termini.
* **Sviluppo incrementale:** Le query possono essere raffinate gradualmente, aggiungendo termini e operatori.
* **Differenze dalla ricerca sul web:** Le query booleane sono più precise e controllate rispetto alle ricerche sul web.

## Esempio di query

Quali opere di Shakespeare contengono le parole "Brutus" e "Caesar" ma NON "Calpurnia"?
- Si potrebbe cercare "Brutus" e "Caesar" in tutte le opere di Shakespeare, quindi eliminare le righe che contengono "Calpurnia"?
Perché questa non è la risposta?
* Lento (per corpora di grandi dimensioni).
* l'interpretazione "NOT Calpurnia" non è banale.
* Altre operazioni (ad esempio, trovare la parola "Romans" VICINO a "countrymen") non sono fattibili.
* Nessun recupero classificato (migliori documenti da restituire).

![[Pasted image 20241001111231.png|663]]
==come funziona?==

### Limitazioni del Modello Booleano

Il modello booleano è molto rigido: l'operatore `AND` richiede la presenza di tutti i termini, mentre `OR` richiede la presenza di almeno uno. Questa rigidità presenta diverse limitazioni:

* **Query brevi:** Il modello incoraggia l'utilizzo di query brevi e semplici.
* **Scarsa flessibilità:** Richiede una scelta precisa dei termini di indice, portando a risultati potenzialmente controintuitivi. Non prevede l'espansione della query (né in linea né a runtime) con termini semanticamente equivalenti. Ogni termine di indice è considerato in modo isolato.

##### Difficoltà nel controllo dei risultati:

* **Numero di documenti recuperati:** Tutti i documenti corrispondenti alla query vengono restituiti, senza possibilità di controllo sulla quantità.
* **Classificazione dei risultati:** Tutti i documenti corrispondenti soddisfano la query logicamente, senza possibilità di ordinamento in base alla rilevanza.
* **Feedback di rilevanza:** È difficile utilizzare il feedback dell'utente (indicazione di rilevanza o irrilevanza di un documento) per migliorare la query. Non essendo possibile un ranking dei risultati, non è possibile migliorare il processo di retrieval tramite il feedback di rilevanza.

##### Espressività limitata:

* **Complessi requisiti dell'utente:** Il modello ha difficoltà nell'esprimere richieste complesse. Non permette di regolare l'importanza dei termini nella query.
* **Sparsità:** La rappresentazione dei dati soffre del problema della sparsità, comune anche ad altri modelli di retrieval.

**Conclusioni:** Il modello booleano è adatto solo quando le esigenze dell'utente sono esprimibili tramite query corte e semplici, e l'obiettivo è solo determinare la presenza o l'assenza di un elemento.

## Estensione del modello booleano

### Incorporare metodi di classificazione

* **Obiettivo:** Preservare il modello booleano, ma aggiungere un meccanismo di ordinamento dei risultati.
* **Metodo:**
 * **Pesi dei termini:** Assegnare pesi ai termini nei documenti e/o nelle query.
 * Il peso del termine misura il grado in cui quel termine caratterizza un documento.
 * I pesi dei termini sono in [0, 1].
 * Nel modello booleano standard, tutti i pesi sono 0 o 1.
 * **Calcolo dei risultati:**
 * Calcolare l'insieme dei risultati mediante il modello booleano standard.
 * Calcolare la distanza/somiglianza con la query per ogni documento (ad esempio Jaccard).
 * Classificare i risultati in base alle distanze/somiglianze vettoriali.
 * **Feedback di rilevanza:** Selezionare i documenti in base alla loro classificazione.
* **Risultato:** Si ottiene un ordine all'interno della lista dei risultati.

### Insiemi fuzzy

* **Concetto:** Rilassare i confini (boundaries) degli insiemi utilizzati nel recupero booleano.
* **Modello booleano come insieme fuzzy:**
 * **Standard:** d è in A o non è in A.
 * **Fuzzy:** d è più o meno in A.
* **Grado di appartenenza:** wA è il grado di appartenenza di un elemento all'insieme A.
* **Operazioni:**
 * **Intersezione (and):** $w(A∩B) = min(wA, wB)$
 * **Unione (or):** $w(A∪B )= max(wA, wB)$

### MMM: Mixed Min and Max Model

* **Termini:** t1, t2, ..., tn
* **Documento:** d, con pesi dei termini di indice: w1, w2, ..., wn
* **Query disgiuntiva:** $q_{or} = (t1 \lor t2 \lor \dots \lor tn)$
* **Somiglianza query-documento:**
 * $S(q_{or}, d) = λ_{or}  max(w1,.. ,wn) + (1 - λ_{or})  min(w1,.. , wn)$
 * $λ_{or}$ è un parametro che controlla il peso relativo del massimo e del minimo.
 * Con la logica booleana regolare, $λ_{or} = 1$.
 * Se la query è puramente disgiuntiva, si considera solo il primo termine.

* **Query congiuntiva:**
 * $q_{and} = (t1 \land t2 \land ... \land tn)$
 * $S(q_{and}, d) = λ_{and} min(w1,.. , wn) + (1 - λ_{and})  max(w1,.. , wn)$
 * $λ_{and}$ è un parametro che controlla il peso relativo del minimo e del massimo.
 * Con la logica booleana regolare, $λ_{and} = 1$.

### Modello di Paice

* **Differenza da MMM:** Tiene conto di tutti i pesi dei documenti, mentre MMM considera solo i pesi massimi e minimi.
* **Vantaggio:** Migliora la risposta alla query.
* **Svantaggio:** Costo computazionale più alto rispetto a MMM.

### Modello P-norma

* **Caratteristiche:**
 * Documenti con pesi dei termini.
 * Termini di query con pesi associati ai termini.
 * Operatori con coefficienti che indicano il grado di rigore.
* **Calcolo della somiglianza:**
 * Considera ogni documento e query come punti multidimensionali (considerando due schemi di pesatura).

## Sparsità

**Rappresentazione di matrici termine-documento:** Adottando un modello di rappresentazione di questo un modello poco sparso è irrealistico.

**Esempio:** Considera un insieme di **N = 1 milione di documenti**, ognuno con circa **1000 parole.**(Un esempio simile è la market-basket-analisys)
Assumiamo una media di **6 byte/parola**, inclusi spazi e punteggiatura.
* Questo corrisponde a **6 GB di dati** nei documenti.
Supponiamo che ci siano **M = 500K termini distinti** tra questi documenti.
##### Problemi:

* Una matrice di incidenza termine-documento di dimensioni **500K x 1M** avrebbe mezzo Trillion (milione di miliardi) di 0 e 1.
##### Nella realtà:

* La matrice ha **non più di un miliardo di 1**.
* Le matrici di incidenza termine-documento sono **estremamente sparse**.
##### Rappresentazione migliore:

* **Registrare solo le posizioni 1.**
Desiderata per una struttura dati:
* Capacità di rappresentare concetti e relazioni.
* Capacità di supportare la localizzazione di questi concetti nella collezione di documenti.

## Indice Inverso

* **Definizione:** L'indice inverso mappa ogni termine all'insieme dei documenti in cui compare.
* **Vantaggi:**
 * **Efficienza di memorizzazione:** È più efficiente per memorizzare e gestire matrici sparse.
 * **Ricerca efficiente:** Consente di identificare rapidamente i documenti che contengono un determinato termine.

* **Costruzione:**
 * **Identificazione dei documenti indicizzati:** Per ogni termine:
 * Identifica tutti i documenti che sono indicizzati da quel termine.
 * Memorizza gli ID di quei documenti (docID).
 * **Creazione della matrice termine-documento:**
 * Inizialmente, rappresenta l'indice inverso come un array di documenti indicizzati.
 * Trasponi l'array per ottenere una matrice termine-documento.

* **Struttura:**
 * **Liste di postings:** Per ogni termine *t*, memorizza un elenco di tutti i documenti che contengono *t*.
 * **DocID:** Ogni documento è identificato da un docID univoco.

* **Implementazione:**
 * **Array di dimensioni fisse:** Non sono adatti per liste di postings di dimensioni variabili, poiché il numero di elementi contenuti in questi array è variabile.
 * **Liste concatenate:** Offrono flessibilità per gestire liste di postings di dimensioni variabili.
 * **Array di lunghezza variabile:** Offrono un compromesso tra dimensioni e facilità di inserimento.

* **Memorizzazione:**
 * **Su disco:** Una sequenza continua di postings è la soluzione più comune ed efficiente.
 * **In memoria:** Le liste concatenate e gli array di lunghezza variabile sono opzioni valide.

* **Considerazioni:**
 * La scelta della struttura dati per l'indice inverso dipende dalle esigenze specifiche dell'applicazione.
 * È importante considerare i compromessi tra efficienza di memorizzazione, velocità di accesso e facilità di aggiornamento.

###### Il **core di un indice** è composto da due elementi principali:

* **Dizionario:** Contiene le parole del corpus, ciascuna associata a un puntatore alla lista di posting corrispondente.
* **Liste di posting:** Ogni lista contiene i documenti in cui la parola corrispondente compare, insieme alla sua frequenza nel documento.

Questo sistema di indicizzazione è doppiamente indicizzato:

* **Per chiave:** Il dizionario permette di accedere rapidamente alla lista di posting per una parola specifica.
* **Per documento:** Le liste di posting permettono di identificare rapidamente i documenti in cui una parola specifica compare, insieme alla sua frequenza nel documento

![[1) Intro-20241003104139467.png|393]]

L'indexer raccoglie i postings per ogni token, ovvero l'insieme di documenti in cui quel token appare. 

L'input dell'indexer è uno stream di coppie termine-ID documento. Questo stream rappresenta l'elenco di tutti i termini trovati nei documenti, insieme all'ID del documento in cui sono stati trovati. 

## Passaggi dell'indexer

1. **Sequenza di coppie (Termine, docID):** L'indexer analizza i documenti e genera una sequenza di coppie, dove ogni coppia rappresenta un termine e l'ID del documento in cui il termine appare.

2. **Ordina per Termine:** La sequenza di coppie viene ordinata in base al termine.

3. **Ordina per docID:** Le coppie con lo stesso termine vengono raggruppate e ordinate in base al docID.

4. **Unisci voci multiple:** Se un termine appare più volte nello stesso documento, le voci vengono unite in un'unica voce, mantenendo la frequenza del termine nel documento.

5. **Aggiungi frequenza del documento:** Per ogni voce, viene aggiunta l'informazione sulla frequenza del termine nel documento.

6. **Dividi in:**

 * **Dizionario:** Contiene tutti i termini distinti trovati nei documenti, con il loro corrispondente termID.
 * **Postings:** Contiene le liste di postings per ogni termine, ovvero l'insieme di documenti in cui il termine appare, con la loro frequenza. 

## Elaborazione delle query

**Esempio:** Brutus AND Caesar

##### Passaggi:

1. **Individua Brutus nel Dizionario:**
 * Recupera i suoi postings (lista di documenti in cui compare Brutus).
2. **Individua Caesar nel Dizionario:**
 * Recupera i suoi postings (lista di documenti in cui compare Caesar).
3. **Unisci i due postings (interseca gli insiemi di documenti):**
 * Attraversa i due postings simultaneamente, in tempo lineare nel numero totale di voci di postings. 
	 * Garantito dal fatto che manteniamo le due liste ordinate
 * Questo significa che l'algoritmo impiega un tempo proporzionale al numero di documenti in cui compare Brutus più il numero di documenti in cui compare Caesar.
		![[1) Intro-20241003104909533.png|438]]
**Nota:** Questo processo di unione dei postings è un esempio di come vengono elaborate le query booleane (AND, OR, NOT) nei motori di ricerca. 

```
INTERSECT(p1, p2)
1   answer ← ⟨ ⟩
2   while p1 ≠ NIL and p2 ≠ NIL
3       do if docID(p1) = docID(p2)
4             then ADD(answer, docID(p1))
5                  p1 ← next(p1)
6                  p2 ← next(p2)
7          else if docID(p1) < docID(p2)
8             then p1 ← next(p1)
9             else p2 ← next(p2)
10  return answer
```

## Ottimizzazione dell'elaborazione delle query

**Scenario:** Una query composta da una congiunzione AND di *n* termini.
![[1) Intro-20241003105153597.png]]
**Obiettivo:** Determinare l'ordine ottimale per l'elaborazione della query.

##### Strategia:

* **Elabora in ordine di frequenza crescente:**
 * Inizia con il termine che compare nel minor numero di documenti (insieme di postings più piccolo).
	 * Questo processo riduce il numero di operazioni di intersecazione e quindi il tempo di elaborazione complessivo
 * Ad ogni passo, interseca l'insieme corrente con il postings del termine successivo, riducendo progressivamente la dimensione dell'insieme.

##### Esempio:

* Se la query è "Brutus AND Caesar AND Antony", e la frequenza documentale è:
 * Brutus: 1000 documenti
 * Caesar: 500 documenti
 * Antony: 200 documenti
* L'ordine ottimale sarebbe: Antony, Caesar, Brutus.

##### Query booleane arbitrarie:

* **Esempio:** (Brutus OR Caesar) AND NOT (Antony OR Cleopatra)
* **Strategia:**
 1. **Ottieni la frequenza documentale per tutti i termini.**
 2. **Stima la dimensione di ogni OR (Upper Bound) come la somma delle sue frequenze documentali (stima conservativa).**
 3. **Elabora in ordine crescente di dimensioni OR.**

##### Nota:

* Il tempo di elaborazione è "lineare" rispetto al numero totale di voci nei postings.
* L'ordine di elaborazione può influenzare significativamente l'efficienza, soprattutto per query complesse.

## Query di frase

##### Importanza:

* Le query di frase sono un elemento chiave per la "ricerca avanzata".
* Sono facilmente comprensibili dagli utenti.
* L'obiettivo è rispondere a query come "Dipartimenti Unical" come una frase completa.
* La frase "*Ci sono 14 dipartimenti presso Unical*" non è una corrispondenza, poiché non corrisponde alla query di frase.
* Per questo tipo di query, non è sufficiente memorizzare solo coppie (Termine, docID).

##### Approcci:

* **Indici biword:**
 * Indizza ogni coppia consecutiva di termini nel testo come una frase (come se avvessimo un *n-gram* che preserva le parole con $n=2$).
 * **Esempio**: "*Amici, Romani, Concittadini*" genera i biword "*amici romani*" e "*romani concittadini*".
 * Ogni biword diventa un termine del dizionario.
 * L'elaborazione delle query di frase a due parole è immediata.

* **Scomposizione di frasi più lunghe:**
 * "Corsi del dipartimento DIMES" può essere scomposto nella query booleana sui biword: *"dipartimento DIMES AND corsi dipartimento".*
 * Senza i documenti, non è possibile verificare se i documenti che corrispondono alla query booleana contengano effettivamente la frase.
	 * Rischio di falsi positivi.
 * Espansione dell'indice a causa di un dizionario più grande.
	 * Inattuabile per più di due parole, anche per i biword (aumenta di molto la complessità).
 * **Gli indici biword non sono la soluzione standard, ma possono essere parte di una strategia composita.**

### Etichettatura Morfologica (POST) e Biword Estensi

* **Analisi del testo:** suddividere il testo in termini e assegnare a ciascun termine la sua categoria grammaticale (POST: target part of speach).
* **Nomi (N) e Articoli/Preposizioni (X):** classificare i termini come nomi o articoli/preposizioni.
* **Biword Estensi:** identificare le sequenze di termini della forma $NX*N$ (es. "il cacciatore nella segale").
	* Dunque includiamo uno o più articoli.
* **Dizionario:** ogni biword esteso diventa un termine nel dizionario.

##### Esempio:

"il cacciatore nella segale"

* **POST:** $N X X N$
* **Biword Estenso:** "cacciatore segale"

#### Elaborazione delle Query

* **Analisi della query:** suddividere la query in termini N e X.
* **Segmentazione in biword:** segmentare la query in biword estesi.
* **Ricerca nell'indice:** cercare i biword estesi nell'indice.

#### Indici Posizionali

* **Memorizzazione delle posizioni:** per ogni termine, memorizzare le posizioni in cui compaiono i token del termine nei documenti.

##### Formato:

```
<termine, numero di documenti contenenti il termine;
doc1: posizione1, posizione2 … ;
doc2: posizione1, posizione2 … ;
…>
```

#### Algoritmo di Unione Ricorsivo

* **Query di frase:** estraggono le voci dell'indice invertito per ogni termine distinto (es. "to", "be", "or", "not").
* **Fusione delle liste di doc:posizione:** uniscono le liste di doc:posizione per elencare tutte le posizioni con la frase completa (es. "to be or not to be").
* **Algoritmo di unione ricorsivo:** utilizzato per le query di frase, ma con la necessità di gestire più della semplice uguaglianza.

#### Indici Posizionali 

* **Utilizzo degli indici posizionali:** gli indici posizionali possono essere utilizzati per le query di prossimità.
* **Limiti degli indici biword:** gli indici biword non possono essere utilizzati per le query di prossimità.
* **Algoritmo di unione ricorsivo:** utilizzato per le query di frase, ma con la necessità di gestire più della semplice uguaglianza.

## Dimensione dell'indice posizionale

Un indice posizionale espande in modo sostanziale l'archiviazione dei postings.
* Bisogna avere una voce per ogni occorrenza, non solo una per documento.
* La dimensione dell'indice dipende dalla dimensione media del documento. (Anche se gli indici possono essere compressi)

##### Regola empirica:

 * Un indice posizionale è 2-4 volte più grande di un indice non posizionale.
 * La dimensione dell'indice posizionale è il 35-50% del volume del testo originale.
Tuttavia, un indice posizionale è lo standard di utilizzo a causa della potenza e dell'utilità delle query di frase e prossimità, sia utilizzate esplicitamente che implicitamente in un sistema di recupero di ranking. 

## Costruzione dell'indice basata sull'ordinamento

Mentre costruiamo l'indice, analizziamo i documenti uno alla volta. I postings finali per qualsiasi termine sono incompleti fino alla fine del processo. Questo approccio richiede molto spazio per collezioni di grandi dimensioni. 
- Ad esempio, con 8 byte per ogni coppia (termID, docID) e considerando 100 milioni di postings (un valore realistico per un anno di pubblicazioni), in linea di principio sarebbe possibile indicizzarlo in memoria oggi. 
- Tuttavia, le *collezioni tipiche sono molto più grandi*: il New York Times, ad esempio, fornisce un indice di oltre 150 anni di notizie. 
##### Pertanto, dobbiamo memorizzare i risultati intermedi su disco.

### Scalabilità della costruzione dell'indice

* La costruzione dell'indice in memoria non è scalabile.
 * Non è possibile inserire l'intera collezione in memoria, ordinarla e poi riscriverla.
* Come possiamo costruire un indice per collezioni molto grandi?
 * Tenendo conto dei **vincoli hardware**: memoria, disco, velocità, ecc.
### Basi hardware

* I server utilizzati nei sistemi IR ora hanno in genere diversi GB di memoria principale.
 * Lo spazio su disco disponibile è di diversi (2-3) ordini di grandezza maggiore.
* La **tolleranza ai guasti** è molto costosa:
 * È molto più economico utilizzare molte macchine normali piuttosto che una macchina tollerante ai guasti.
* L'accesso ai dati in memoria è molto più veloce dell'accesso ai dati su disco.
* Ricerche su disco: Nessun dato viene trasferito dal disco mentre la testina del disco viene posizionata.
 * Pertanto, il trasferimento di un grande blocco di dati dal disco alla memoria è più veloce del trasferimento di molti piccoli blocchi.
* L'I/O del disco è basato su blocchi:
 * Lettura e scrittura di blocchi interi (al contrario di piccoli blocchi)
 * Dimensioni dei blocchi: da 8 KB a 256 KB(dipende dal task, per il retrival si arriva a 64, 256 è il massimo).

#### Ordinamento usando il disco come "memoria"?

* Non possiamo usare lo stesso algoritmo di costruzione dell'indice per collezioni più grandi, usando il disco invece della memoria, perchè ordinare 100 milioni di record su disco è troppo lento, poichè implica troppe ricerche su disco.
* Abbiamo bisogno di un algoritmo di ordinamento esterno.

## BSBI: Indicizzazione basata sull'ordinamento a blocchi

* **Ordinamento con meno ricerche su disco** (minimizzare le ricerche sul disco)
	* Partiamo dall'assunzione che il corpus sia statico.
 * I record (termID, docID) vengono generati mentre analizziamo i documenti.
 * Supponiamo di ordinare 100 milioni di tali record da 8 byte per termID.
 * Definiamo un blocco di circa 10 milioni di tali record.
 * Possiamo facilmente inserirne un paio in memoria.
 * Avremo 10 blocchi di questo tipo per iniziare.
* **Idea di base dell'algoritmo**:
 * Accumulare i postings per ogni blocco, ordinarli e scriverli su disco.
 * Quindi unire i blocchi in un unico ordine ordinato.

```
BSBIndexConstruction()
1   n ← 0
2   while (all documents have not been processed)
3       do n ← n + 1
4          block ← ParseNextBlock()
5          BSBI-Invert(block) //costruisce inverted index
6          WriteBlockToDisk(block, fn)  //scriviamo sul disco, presuppone la   ù
           presenza di una struttura di blocco sul disco
7   MergeBlocks(f_1, ..., f_n; f_merged)

```

#### Ordinamento di 10 blocchi di 10 milioni di record.

L'ordinamento di 10 blocchi da 10 milioni di record ciascuno avviene in due fasi: ordinamento interno di ogni blocco (usando ad esempio Quicksort con complessità $O(N \log N)$) e successiva fusione delle run ordinate tramite un algoritmo di ordinamento esterno (come un merge sort esterno). 
L'utilizzo dello spazio su disco può essere ottimizzato evitando la necessità di due copie complete dei dati.

## Come Unire le Run Ordinate?

Esistono due approcci principali per unire run ordinate: il *merge binario* e il *multi-way merge*.

##### Merge Binario:

Si possono effettuare merge binari, utilizzando un albero di merge con $\log_2(n)$ livelli, dove *n* è il numero di run. Ad esempio, con 10 run, si avrebbero 4 livelli ($\log_2(10) \approx 4$). Ad ogni livello, si leggono blocchi di run (es. 10 milioni di elementi per blocco), si uniscono e si riscrivono su disco. Invece di un unico merge finale, si adotta una struttura ad albero con merge parziali a ogni livello:

1. **Partizionamento:** I dati vengono suddivisi in partizioni.
2. **Merge parziale:** Ogni partizione viene caricata in memoria e sottoposta a merge, generando un nuovo indice.
3. **Aggiornamento dell'indice:** L'indice aggiornato viene utilizzato per il merge del livello successivo.

![[1) Intro-20241003112202805.png]]

##### Multi-way Merge:

Un approccio più efficiente è il multi-way merge, che legge simultaneamente da tutti i blocchi. Questo richiede:

* L'apertura simultanea di tutti i file di blocco.
* Un buffer di lettura per ogni blocco e un buffer di scrittura per il file di output.
* L'utilizzo di una coda di priorità per selezionare, ad ogni iterazione, il `termID` più basso non ancora elaborato.
* L'unione delle liste di postings corrispondenti a quel `termID` (provenienti dai vari blocchi) e la scrittura del risultato.

Questo metodo è efficiente a condizione che si leggano e si scrivano blocchi di dimensioni adeguate.

**Assunzione:** Si assume una condivisione solo parziale del lessico tra i vari documenti.

##### Problema della Crescita del Lessico:

La gestione del lessico durante il merge presenta sfide:

* **Gestione dei termini e degli ID:** È fondamentale gestire correttamente i termini (token pre-processati) e i loro identificativi (ID).
* **Compressione con perdita:** La compressione con perdita può essere applicata solo a termini non essenziali per la comprensione del testo.
* **Valutazione dei token:** Sono necessarie strategie per valutare l'importanza dei token e decidere quali possono essere compressi con perdita.

## SPIMI: Indicizzazione in memoria a passaggio singolo (approccio lazy)

* **Problema con l'algoritmo basato sull'ordinamento:**
 * La mappatura (termine, termID) potrebbe non entrare in memoria.
 * Abbiamo bisogno del dizionario (che cresce dinamicamente) per implementare una mappatura termine-termID.

Idee chiave: sono complementari
* **Idea chiave 1:** *Generare dizionari separati per ogni blocco*. Non c'è bisogno di mantenere la mappatura termine-termID tra i blocchi (mapping across block).
* **Idea chiave 2:** *Non ordinare*. Accumulare i postings nelle liste di postings man mano che si verificano.

* Sulla base di queste due idee, generiamo un indice invertito completo per ogni blocco.
* Questi indici separati possono quindi essere uniti in un unico grande indice.
* SPIMI può indicizzare collezioni di qualsiasi dimensione a condizione che ci sia abbastanza spazio su disco disponibile.
* **Ogni lista di postings è una struttura dinamica e immediatamente disponibile per collezionare i postings**.
 * **più veloce** - non è necessario ordinare.
 * **risparmia memoria** - i termID dei postings non devono essere memorizzati e non vi sono fasi di sorting intermedie. 
 * In pratica, è una struttura che conserviamo in memoria e che viene riallocata all'occorrenza.
 * Evitiamo di tenere traccia dei term-id, l'algoritmo lavora direttamente con i termini

### SPIML-Invert (token_stream)

```python

1.  output_file = NEWFILE()
2.  dictionary = NEWHASH()
3.  while (free memory available)
4.      do token ← next(token_stream)
5.      if term(token) ∉ dictionary
6.          then postings_list = ADDToDICTIONARY(dictionary, term(token))
7.      else postings_list = GETPOSTINGSLIST(dictionary, term(token))
8.      if full(postings_list)
9.          then postings_list = DOUBLEPOSTINGSLIST(dictionary, term(token))
10.     ADDToPOSTINGSLIST(postings_list, docID(token))
11.     sorted_terms ← SORTTERMS(dictionary)
12.     WRITEBLOCKToDISK(sorted_terms, dictionary, output_file)
13.     return output_file

# Linea 5: il token raw viene pre-processato; viene ricondotto a un index-term.
# Linea 10: il posting (occorrenza del termine) nel documento viene aggiunto immediatamente.
# Linee 8-9: se la lista di postings raggiunge la dimensione limite, viene raddoppiata.
# Linea 11: L'ordinamento dei termini avviene dopo aver raccolto tutti i postings per un blocco.
```

## Indicizzazione Distribuita

L'indicizzazione distribuita prevede l'esecuzione di due task paralleli: il parsing e l'inversione dell'indice. 
Un'unica macchina master coordina il processo, suddividendo l'indice in una serie di task paralleli. 
Ogni split rappresenta un insieme di documenti gestito come blocchi. La macchina master assegna i ruoli ai diversi nodi del sistema.

### Parser

* Il master assegna uno split a una macchina parser inattiva.
* Il parser legge un documento alla volta ed emette coppie (termine, documento).
* Il parser scrive le coppie in *j* partizioni. Le partizioni (fold) sono determinate in modo lessicografico.
 * **Esempio:** Ogni partizione è per un intervallo di lettere iniziali dei termini (ad esempio, a-f, g-p, q-z), quindi *j* = 3.

Una volta completato il parsing, si procede con l'inversione dell'indice.

### Inverter

* Un inverter raccoglie tutte le coppie (termine, doc) (cioè, postings) per una partizione di termini.
* Ordina e scrive le liste di postings. 

![[1) Intro-20241003093756427.png]]

La figura mostra come l'indicizzazione distribuita possa essere vista come un'istanza particolare del modello MapReduce.

##### Fase di Map:

* Partendo dalla collezione di documenti in input, la fase di map produce liste di coppie (termine, documento).

##### Fase di Reduce:

* La fase di reduce prende le liste di occorrenze (coppie termine-documento) e produce le liste di postings, ovvero le liste di documenti in cui un termine compare. 

![[1) Intro-20241003093804693.png]]
## Indicizzazione Distribuita

L'algoritmo di costruzione dell'indice è un'istanza di MapReduce.

* **MapReduce:** Un framework robusto e concettualmente semplice per il calcolo distribuito, che permette di eseguire calcoli complessi senza dover scrivere codice per la parte di distribuzione.
* **Sistema di indicizzazione di Google (circa 2002):** Composto da una serie di fasi, ciascuna implementata in MapReduce.

### Schema per la costruzione dell'indice in MapReduce

* **Funzioni map e reduce:**
 * `map`: input → list(k, v)
 * `reduce`: (k,list(v)) → output
* **Istanza dello schema per la costruzione dell'indice:**
 * `map`: collection → list(termID, docID)
 * `reduce`: (<termID1, list(docID)>, <termID2, list(docID)>, …) → (postings list1, postings list2, …)

Fino ad ora, abbiamo ipotizzato che le collezioni siano statiche, ma:

* **Documenti in arrivo:** I documenti arrivano nel tempo e devono essere inseriti.
* **Documenti eliminati e modificati:** I documenti vengono eliminati e modificati (ad esempio, quando l'entità dell'editing è tale da toccare la maggior parte dei termini, come upgrade, motivi di privacy o cambio di normativa).

La gestione degli aggiornamenti richiede modifiche al dizionario e alle liste di postings: i *termini già presenti* nel dizionario necessitano di aggiornamenti alle liste di postings, mentre i *nuovi termini* devono essere aggiunti al dizionario stesso.

### Approccio più semplice

* **Indice principale e indice ausiliario:** Mantenere un "grande" indice principale, i nuovi documenti vanno in uno (o più) "piccolo" indice ausiliario.
* **Ricerca:** La ricerca viene effettuata su entrambi gli indici, unendo i risultati.
* **Eliminazioni:**
 * **Vettore di bit di invalidazione:** Un vettore di bit indica i documenti eliminati.
 * **Filtraggio:** I documenti in output su un risultato di ricerca vengono filtrati tramite questo vettore di bit di invalidazione.
* **Re-indicizzazione:** Periodicamente, re-indicizzare in un unico indice principale.

### Problemi con gli indici principali e ausiliari

* **Merge frequenti.**
* **Scarsa performance durante i merge.**
* **Efficienza della fusione:** La fusione dell'indice ausiliario nell'indice principale è efficiente se si mantiene un file separato per ogni lista di postings.
 * La fusione è la stessa di una semplice append.
 * Ma poi avremmo bisogno di molti file - inefficiente per il sistema operativo.
* **Ipotesi:** L'indice è un unico grande file.
 * **Realtà:** Usare uno schema da qualche parte nel mezzo (ad esempio, dividere le liste di postings molto grandi, raccogliere le liste di postings di lunghezza 1 in un unico file, ecc.).

## Fusione Logaritmica

La fusione logaritmica è una tecnica di ordinamento che utilizza una serie di indici di dimensioni crescenti per ordinare un insieme di dati. 

##### Principio di funzionamento:

1. **Serie di indici:** Si mantiene una serie di indici, ciascuno con una dimensione doppia rispetto al precedente. Ad esempio, si potrebbe avere un indice di dimensione 1, 2, 4, 8, 16, e così via.
2. **Memoria e disco:** L'indice più piccolo $(Z_0)$ è mantenuto in memoria, mentre gli indici più grandi $(I_0, I_1, ...)$ sono memorizzati sul disco.
3. **Fusione e scrittura:** Quando l'indice $Z_0$ diventa troppo grande (supera la sua capacità), viene scritto sul disco come $I_0$. In alternativa, se $I_0$ esiste già, $Z_0$ viene unito con $I_0$ per formare $Z_1$.
4. **Iterazione:** Se $Z_1$ non è ancora troppo grande, viene mantenuto in memoria. Altrimenti, viene scritto sul disco come $I_1$. Se $I_1$ esiste già, $Z_1$ viene unito con $I_1$ per formare $Z_2$.
5. **Ripetizione:** Questo processo di fusione e scrittura viene ripetuto fino a quando tutti i dati non sono stati ordinati.

![[1) Intro-20241007154640070.png|436]]

![[1) Intro-20241007154611506.png|450]]
## Indicizzazione Dinamica

#### Indice Ausiliario e Principale

* **Fusione T/n:**
 * Si utilizzano indici ausiliari di dimensione $n$ e un indice principale con $T$ postings.
 * Il tempo di costruzione dell'indice è $O\left( \frac{T^2}{n} \right)$ 
 nel caso peggiore, poiché un posting potrebbe essere toccato $\frac{T}{n}$ volte.

#### Fusione Logaritmica

* **Efficienza:** Ogni posting viene fuso al massimo $O\left( log\left( \frac{T}{n} \right) \right)$ volte, con una complessità di $O\left( T \cdot log\left( \frac{T}{n} \right) \right)$
* **Vantaggi:** La fusione logaritmica è molto più efficiente per la costruzione dell'indice rispetto alla fusione $\frac{T}{n}$.
* **Svantaggi:** L'elaborazione delle query richiede la fusione di $O\left( log\left( \frac{T}{n} \right) \right)$ indici, mentre con un solo indice principale e ausiliario la complessità è $O(1)$.

## Ulteriori Problemi con Più Indici

Mantenere le statistiche a livello di collezione con più indici è complesso. Ad esempio, per la correzione ortografica, è difficile scegliere l'alternativa corretta con il maggior numero di risultati. 

* **Problema:** Come mantenere le migliori alternative con più indici e vettori di bit di invalidazione?
* **Possibile soluzione:** Ignorare tutto tranne l'indice principale per l'ordinamento.

Il ranking dei risultati si basa su queste statistiche, rendendo la loro gestione cruciale.

### Indicizzazione Dinamica nei Motori di Ricerca

I motori di ricerca effettuano l'indicizzazione dinamica con:

* **Modifiche incrementali frequenti:** es. notizie, blog, nuove pagine web.
* **Ricostruzioni periodiche dell'indice da zero:** L'elaborazione delle query viene commutata sul nuovo indice e il vecchio indice viene eliminato.

### Requisiti per la Ricerca in Tempo Reale

La ricerca in tempo reale richiede:

* **Bassa latenza:** Elevata produttività di valutazione delle query.
* **Elevato tasso di ingestione:** Immediata disponibilità dei dati.
* **Letture e scritture concorrenti:** Gestione di letture e scritture simultanee dell'indice.
* **Dominanza del segnale temporale:** Priorità ai dati più recenti.

## Costruzione dell'Indice: Riepilogo

### Indicizzazione basata sull'ordinamento

Esistono diverse tecniche di indicizzazione basata sull'ordinamento:

* **Inversione in memoria naive.**
* **Indicizzazione basata sull'ordinamento bloccato (BSBI).**
* **L'ordinamento per fusione è efficace per l'ordinamento basato su disco rigido (evita le ricerche!).**

### Indicizzazione in memoria a passaggio singolo (SPIMI)

* **Nessun dizionario globale.**
* **Genera un dizionario separato per ogni blocco.**
* **Non ordinare i postings.**
* **Accumulare i postings nelle liste di postings man mano che si verificano.**

