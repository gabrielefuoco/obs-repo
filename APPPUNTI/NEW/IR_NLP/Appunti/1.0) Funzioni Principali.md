## Linguaggio naturale

Il linguaggio naturale, complesso e multimodale, è al centro dell'IR e del NLP.

##### Recupero delle informazioni (IR) e Elaborazione del linguaggio naturale (NLP)

Entrambi sono campi della scienza e dell'ingegneria.
* **NLP:** Si concentra sullo sviluppo di sistemi automatici che comprendono e generano linguaggi naturali.
* **IR:** Si concentra sul ritrovamento (semi-)automatico di dati testuali (non strutturati) che soddisfano un bisogno informativo all'interno di grandi collezioni.

###### Enunciato del problema di base:

Il problema base dell'IR consiste nel trovare documenti rilevanti a una query (termine singolo, frase, ecc.) all'interno di una collezione di documenti, utilizzando un modello di recupero per valutare la corrispondenza.

##### Problemi con i dati testuali

L'analisi di dati testuali presenta sfide generali (grandi dataset, alta dimensionalità, dati rumorosi, evoluzione continua) e sfide specifiche.  
Queste ultime includono la mancanza di una struttura progettata per i computer, la complessità semantica e strutturale del linguaggio naturale (ambiguità a livello morfologico, sintattico, semantico e pragmatico), la gestione di linguaggi diversi e il multilinguismo.

## IR vs. altre discipline

**Obiettivo comune:** Rendere più facile trovare le cose, ad esempio sul Web.

##### Attività comuni:

* Ottimizzazione dei motori di ricerca (SEO)
* Crawling dei dati
* Estrazione di documenti di interesse da un enorme mucchio

##### Differenze:

* **Web scraping:** Estrazione di informazioni.
* **Trovare schemi in grandi collezioni:** NLP, Linguistica computazionale.
* **Scoprire informazioni finora sconosciute:** NLP, Estrazione di testo.
* **Discriminazione del testo:** NLP (Machine Learning).
* **Comprensione del testo:** NLP (Machine Learning, AI generativa, ...). 

## Topic Detection and Tracking

Il *Topic Detection and Tracking*, rilevante prima dell'era dei big data, consiste nell'acquisizione e analisi incrementale di dati (es. news feed) per identificare topic emergenti e obsoleti.

##### Caratteristiche:

* **Analisi dei dati incrementale:** I dati vengono analizzati in modo continuo, man mano che vengono acquisiti.
* **Supervisionato o non supervisionato:** Non è specificato se il processo di analisi sia supervisionato o meno.

#### Difficoltà nel Topic Detection and Tracking

* **Task incrementale:** Il sistema automatico deve essere in grado di riconoscere nuovi topic emergenti e di identificare quelli obsoleti.
* **Capacità limitate:** La fase di tracking richiede risorse computazionali limitate.
* **Definizione di topic:** Un topic è un insieme di termini, anche se non è l'unica definizione possibile.
* **Segnale:** Il sistema deve tenere traccia del segnale, ovvero dei dati in ingresso, per identificare i topic.

### Modellazione di topic stocastica

- È una tecnica che utilizza modelli probabilistici per analizzare grandi quantità di testo e identificare i temi principali (topic) presenti nei documenti. 
- Si basa sull'idea che ogni documento sia una combinazione di diversi topic, e che ogni parola abbia una probabilità di appartenere a un determinato topic.
- Questo approccio permette di scoprire i temi principali, analizzare la loro distribuzione nei documenti e classificare i documenti in base ai topic che contengono. 
- Un esempio di modello di topic stocastico è il **Latent Dirichlet Allocation (LDA).** 

### Relation Extraction

* **Relazione con Named Entity Recognition:** Per estrarre le relazioni tra entità nominate, è necessario prima identificare le entità stesse.
* **Pattern frequenti:** La ricerca di pattern frequenti è un passo fondamentale per l'estrazione di relazioni.
* **Definizione in NLP:** In NLP, la relation extraction si riferisce all'identificazione di relazioni lessicali tra entità nominate.

### Summarization

* **Approccio tradizionale:** Negli anni '90, la summarization era affrontata come estrazione di parole chiave (key phrase extraction).
* **Guida alla summarization:** Le proprietà di un documento possono essere utilizzate per guidare il processo di summarization. 

### KDD Pipeline in NLP

I task NLP seguono una pipeline con fasi sequenziali (*unfolding*): 
- Indicizzazione degli elementi costitutivi
- Rappresentazione dei contenuti informativi
- Apprendimento a valle del *result set*. 
- La *feature selection* può avvalersi del machine learning.

### Valutazione

La **valutazione** dei risultati si basa su criteri statistici, tra cui l' *accuracy*, che presenta però limitazioni.  In assenza di un *gold standard* (set di dati perfetto creato da esperti), si può ricorrere a un *silver standard* generato automaticamente (es. usando GPT-4 per modelli linguistici di grandi dimensioni).

# Funzioni Principali

* **Funzione principale:** Il compito principale di un sistema di elaborazione del linguaggio naturale (*NLP*) è quello di **estrarre informazioni** da testi.
* **Macro categorie:** Le funzioni principali possono essere suddivise in due macro categorie:
 * **Indicativa:** Questa funzione serve a rivelare gli elementi dei contenuti in modo da consentire la determinazione della rilevanza del testo rispetto alle query. È fondamentale per l'esplorazione dei corpus e per il retrieval.
 * **Informativa:** Questa funzione consente di ottenere un surrogato del testo (o di una sua porzione) senza fare necessariamente riferimento al testo originale. Questo concetto è ancora valido per il retrieval, che consiste nel condensare i dati originali in un formato surrogato.

## Browsing

**Il *browsing***, tipico di sistemi ipertestuali, permette agli utenti di esplorare collezioni di testo senza dover specificare in anticipo i propri interessi, risultando utile per utenti con bisogni poco chiari o inespressi. 
- Gli utenti possono semplicemente indicare i documenti che trovano rilevanti.
- Può supportare l'annotazione.

## Estrazione di Informazioni

L'**estrazione di informazioni**, insieme a classificazione e clustering, è un task fondamentale nell'analisi di informazioni sul web.  
Consiste nel recuperare informazioni specifiche da documenti già identificati come rilevanti, estraendo o inferendo risposte dalla rappresentazione del testo. 
- Esempi includono il *web wrapping* e l'estrazione di informazioni da siti di *e-commerce*.

##### In altre parole:

* Il sistema ha già identificato un documento come rilevante per una specifica richiesta.
* L'estrazione di informazioni si concentra quindi sull'individuazione di informazioni specifiche all'interno di quel documento.
* Questo processo può essere facilitato da schemi o database pre-esistenti, come nel caso del retrieval sul web o nell'e-commerce.

##### Tipi di template:

* **Slot:** Spazi da riempire con sottostringhe del documento.
* **Riempitivi pre-specificati:** Valori fissi, indipendentemente dal testo.
* **Multipli riempitivi:** Slot che accettano più valori.
* **Ordine fisso:** Sequenza fissa degli slot.

##### Modelli di estrazione:

* **Specificazione di elementi:** Ad esempio, tramite espressioni regolari.
* **Modelli precedenti e successivi:** Contesto pre e post-slot per una migliore identificazione.
	* **Es**: *Estrazione di termini da un modello*. Definiziamo il template a priori e per ogni slot creiamo dei pattern.

## Recupero di Documenti

Il recupero di documenti seleziona documenti da una collezione in risposta a una query dell'utente, solitamente in linguaggio naturale.  Il processo prevede:

* Classificazione dei documenti per rilevanza alla query.
* Abbinamento tra la rappresentazione del documento e quella della query.
* Restituzione di un elenco di documenti pertinenti.

Esistono diversi modelli di recupero, tra cui booleano, spazio vettoriale e probabilistico.  Le differenze risiedono nella rappresentazione dei contenuti testuali, dei bisogni informativi e del loro abbinamento.


## Panoramica delle Applicazioni di Base

Le principali applicazioni si concentrano su:

* **Scoperta della conoscenza:**  Attraverso l'estrazione di informazioni e la distillazione delle informazioni (estrazione basata su struttura predefinita).
* **Filtraggio dell'informazione:** Rimozione di informazioni irrilevanti, con applicazioni nella web personalization e nei sistemi di raccomandazione (anche *collaborative based*).

##### Utilizzi tipici:

* **Categorizzazione gerarchica:** Organizzazione di documenti in una struttura gerarchica.
* **Riassunto del testo:** Creazione di riassunti concisi.
* **Disambiguazione del senso delle parole:** Determinazione del significato corretto di una parola nel contesto.
* **Filtraggio del Testo (o dell'Informazione)**: Rimuovere il testo non rilevante da un documento.
* **CRM e marketing:** Cross-selling e raccomandazioni di prodotti.
* **Raccomandazione di prodotti.**
* **Filtraggio di notizie e spam.**

### Categorizzazione Gerarchica

La categorizzazione gerarchica, tipicamente implementata tramite tassonomie, permette ai ricercatori di esplorare un insieme di dati in modo strutturato. 
Inizialmente, si naviga attraverso la gerarchia delle categorie per poi focalizzarsi su una specifica categoria di interesse, affinando così la ricerca. Questo sistema dovrebbe essere flessibile, consentendo l'aggiunta di nuove categorie e la rimozione di quelle obsolete.

##### Caratteristiche:

* Natura ipertestuale dei documenti: Analisi degli hyperlink
* Struttura gerarchica dell'insieme di categorie
* Decomposizione della classificazione come decisione ramificata (A un nodo interno)

### Summarization

La summarization consiste nel generare un riassunto del contenuto di un testo. Per testi brevi, il riassunto deve essere essenziale e coerente. L'utilizzo di profili aiuta a strutturare le informazioni importanti in campi semanticamente ben definiti. 
La tecnica di summarization dipende dalla natura dei documenti:
- Ad esempio, per la summarization di recensioni, è possibile addestrare il modello per analizzare aspetti specifici, evitando un riassunto trasversale e generando invece riassunti per ogni aspetto elencato.

La summarization facilita principalmente l'accesso alle informazioni:

* Estrazione delle parole chiave più utili da un insieme di documenti (es. un cluster) per descriverlo.
* Astrazione dei documenti in una collezione per evitare la lettura del contenuto completo.
* Riassunto dei documenti recuperati da una ricerca per consentire all'utente un'identificazione più rapida di quelli pertinenti alla query.

Un riassunto può essere di alto livello, fornendo una panoramica di tutti i punti principali, oppure più dettagliato. Gli approcci alla summarization possono essere classificati in base alla dimensione dell'unità di testo utilizzata:

* **Riassunti di parole chiave:** Utilizzano parole chiave per rappresentare il contenuto principale.
* **Riassunti di frasi:** Utilizzano frasi per rappresentare il contenuto principale.

### Disambiguazione del Senso delle Parole (WSD)

Mira ad assegnare il significato corretto a una parola in base al suo contesto.  Richiede l'analisi simultanea di tutti i termini nel contesto. 
*Un approccio efficace sfrutta inventari di sensi esistenti e misure di correlazione semantica.*  
- L'avvento di risorse linguistiche elettroniche come Wikipedia ha rinnovato l'interesse in questo campo
##### Esempio:

* La parola inglese "bank" può avere (almeno) due significati:
 * "The Bank of England" (istituto finanziario)
 * "The bank of the river Thames" (argine fluviale)
* Per disambiguare l'occorrenza di "bank" in "Last week I borrowed some money from the bank", è necessario considerare il contesto per determinare il significato corretto (istituto finanziario).


## Filtraggio nel Text Mining

##### Definizione:

Classifica i documenti come rilevanti o irrilevanti per un consumatore di informazioni, bloccando quelli irrilevanti.

**Esempio:** Un feed di notizie dove l'agenzia di stampa è il produttore e il giornale è il consumatore.

##### Caratteristiche:

* Considerabile come un caso di Text Classification con una singola etichetta (*rilevante o irrilevante*).
* Implementabile sia lato produttore (*instradamento selettivo*) che lato consumatore (*blocco*).
* Richiede la creazione e l'aggiornamento di un "*profilo*" per ogni consumatore (*lato produttore*) o un singolo profilo generale (*lato consumatore*).

#### Filtraggio Adattivo:

* Il profilo iniziale è definito dall'utente.
* Il profilo viene aggiornato in base al feedback dell'utente sulla pertinenza dei documenti consegnati.

#### Filtraggio - CRM (Customer Relationship Management)

Il filtraggio nel CRM aiuta le aziende ad analizzare il feedback dei clienti attraverso standardizzazione e raggruppamento dei dati.

#### Filtraggio - Raccomandazione di Prodotti

La raccomandazione di prodotti utilizza approcci basati sul contenuto (analisi delle preferenze utente) e collaborativi (analisi di utenti simili).
#### Rilevamento dello Spam

Il rilevamento dello spam, applicato al text mining per classificare email, presenta sfide legate ai costi asimmetrici degli errori (falsi positivi/negativi) e alla distribuzione non uniforme delle classi (più spam che email legittime).

## Vocabolario: Modello di Rappresentazione dei Testi

### Definizione di Termine

**Il problema principale** nell'indicizzazione di un documento è la selezione dei termini da includere, bilanciando l'inclusione di tutte le parole con la necessità di considerare solo quelle importanti e la gestione delle frequenze di parole comuni e rare.
**Il contesto** è la composizione testuale e l'obiettivo dell'indice, che deve essere funzionale.

**La soluzione** prevede la modellazione delle relazioni tra le parole, creando pattern relazionali che definiscono l'importanza di ogni termine.


##### Cosa è un termine?

La definizione di "termine" è flessibile: può essere una parola singola, una coppia di parole, una frase, la radice di una parola (word stem), un n-gramma (sequenza di n caratteri), o un tipo di parola.

##### Modellazione delle relazioni tra termini:

Per semplificare i modelli, si evitano relazioni basate su paragrafi, frasi, ordine delle parole e strutture sintattiche complesse, preferendo invece relazioni semantiche come "*is-a*" e "*part-of*".

L'obiettivo finale è identificare termini che rappresentano la semantica del testo, minimizzando la necessità di codifica manuale

##### Altri argomenti correlati:

* **Analisi lessicale e morfologica:** Include elaborazione della punteggiatura, conversione in minuscolo, rimozione di stopwords, stemming, lemmatizzazione e tagging delle parti del discorso.
* **Analisi della semantica del discorso (Anafora):** Considera l'anafora letterale/pronominale, l'ellissi testuale e la meronomia referenziale.
* **Pragmatica, Semiotica e Morfologia:** Questi campi forniscono ulteriori prospettive per una comprensione più completa del linguaggio.

## Tokenizzazione: Problemi Lessicali e Morfologici

Un token rappresenta un termine candidato durante la fase di pre-processing nell'analisi del testo. La tokenizzazione, quindi, si occupa di come organizzare un testo in unità significative.

### Punteggiatura

La gestione della punteggiatura presenta diverse sfide:

* **Sequenze con trattino:** Termini come *state-of-the-art*, *co-education*, *lowercase*, *lower-case*, *lower case* pongono il problema di come trattarli. Mantenere le parole separate è una soluzione semplice ma può compromettere la semantica. Un approccio più sofisticato è necessario, ma non sempre scalabile.

* **Apostrofi:** La gestione di apostrofi in termini come *Italy’s capital* richiede una decisione su come rappresentarli (*Italy AND s*, *Italys*, *Italy’s*?).

* **Acronimi:** Acronimi come *U.S.A* e *USA* possono essere trattati in modo diverso a seconda del contesto.

* **Entità nominate:** Entità come *San Francisco* e *Hewlett-Packard* richiedono un trattamento specifico per evitare di frammentarle in unità non semantiche.

### Numeri

La gestione dei numeri richiede una scelta tra rimozione e mantenimento. Generalmente, si rimuovono le stringhe puramente numeriche, mentre si mantengono quelle alfanumeriche.

##### Motivazioni per il mantenimento dei numeri:

* Preservazione delle informazioni numeriche contenute nel testo.
* Necessità di individuare codici di errore, intervalli temporali, ecc.

**Metadati:** I metadati (data di creazione, formato, ecc.) sono spesso indicizzati separatamente. La complessità dell'interazione tra numeri e testo rende difficile una soluzione universale.

**Varietà di tipi numerici:** I numeri possono assumere diverse forme:

* **Date:** *3/20/91*, *Mar. 12, 1991*, *20/3/91*.
* **Tempo:** *55 B.C.*
* **Codici:** *B-52*.
* **Identificatori:** *My PGP key is 324a3df234cb23e*.
* **Numeri di telefono:** *(800) 234-2333*.

**Case delle lettere:** Generalmente, si converte tutto il testo in minuscolo (lowercase).

## Stopwords

Le stopwords sono parole grammaticali (es. articoli, preposizioni) con scarso potere informativo, specifiche della lingua.  La loro rimozione è fondamentale nella pipeline di data analysis, in quanto colonne con distribuzione troppo omogenea o eterogenea (dovuta alla presenza di stopwords) non sono informative. 

**Motivi per rimuovere le stopwords:**
 * Non si riferiscono a oggetti o concetti.
 * Hanno poco contenuto semantico.
 * Sono le parole più comuni.
 * Impattiamo sulla dimensionalità della rappresentazione dei nostri testi
 Ciò implica **sparsità**: insieme sono un problema per le misure di distanza, che perdono di sensibilità $\to$ *perdita di potere discriminante*

 **Casi in cui le stopwords potrebbero essere necessarie:**
 * Query di frase: "King of Denmark"
 * Titoli: "Let it be", "To be or not to be"
 * Query "relazionali": "flights to London"

**Gestione delle stopwords:**
 La gestione ottimale prevede tecniche di compressione e ottimizzazione delle query che minimizzino l'impatto delle stopwords sul sistema.
 
### Personalizzazione della Stop-List

La stop-list deve contenere termini che si aspettano di essere molto frequenti all'interno della collezione
- La personalizzazione della stop-list mira non solo a ridurla, ma anche ad arricchirla, *rimuovendo termini molto frequenti nel corpus* (poco esplicativi per la distinzione tra documenti) e *considerando l'esclusione di termini super rari* (operazione delicata, poiché ciò che distingue un testo è più importante di ciò che li accomuna). 
- La decisione di *escludere termini che appaiono una sola volta* dipende dal tipo di termine e non esiste una regola fissa.

## Normalizzazione

La normalizzazione uniforma la forma delle parole, permettendo di trovare corrispondenze tra termini con forme diverse ma stesso significato (es. "U.S.A." e "USA").  
È fondamentale per garantire l'efficacia della ricerca, normalizzando in modo coerente testo indicizzato e query. 

###### Risultato:

Il risultato è la creazione di *termini*, voci nel dizionario di Information Retrieval (IR), spesso organizzati in *classi di equivalenza* che raggruppano termini equivalenti. 

###### Esempi di operazioni di normalizzazione includono:

* **Eliminazione di punti:** "U.S.A.", "USA" → "USA"
* **Eliminazione di trattini:** "anti-discriminatory", "antidiscriminatory" → "antidiscriminatory"
* **Eliminazione di accenti:** "French résumé" vs. "resume"
* **Eliminazione di umlaut:** "German Tuebingen" vs. "Tübingen"

Questo processo semplifica la rappresentazione del testo, focalizzandosi sulla struttura sintattica a discapito di alcune sfumature semantiche. Diverse parole con significati simili vengono ridotte a una sola forma.

##### Espansione asimmetrica:

Un'alternativa alle classi di equivalenza è l'espansione asimmetrica. In questo caso, l'inserimento di un termine porta all'inclusione di diverse varianti durante la ricerca:

* Inserire: "window" → Ricerca: "window", "windows"
* Inserire: "windows" → Ricerca: "Windows", "windows", "window"
* Inserire: "Windows" → Ricerca: "Windows"

Questo approccio può essere più potente, ma meno efficiente.

##### Criterio più importante: uso della lingua

La scelta del metodo di normalizzazione dipende fortemente dall'uso della lingua. Considerando che gli utenti spesso omettono accenti e trattini nelle query, è spesso preferibile normalizzare a una forma senza questi elementi.

##### Tokenizzazione e normalizzazione:

Tokenizzazione e normalizzazione sono processi interdipendenti e spesso dipendono dalla lingua. Ad esempio, i formati di data "3/10/2000" (US) e "10/3/2000" (UE) richiedono un trattamento diverso.

## Case Folding

Tipicamente, si riducono tutte le lettere in minuscolo.
Se trattiamo questo task in maniera sofisticata dobbiamo avere un modo per riconoscere le named entity.
Eventualmente, possiamo mantenere solo gli acronimi
- **Eccezione:** maiuscole a metà frase (Es: General Motors).

Spesso è meglio mettere tutto in minuscolo, poiché gli utenti useranno le minuscole indipendentemente dalla "corretta" capitalizzazione.

### Riduzione della matrice dei dati tramite algebra lineare

Come fase di post-processing, si può applicare l'algebra lineare alla matrice dei dati per ridurla ulteriormente. Questo processo si basa sull'identificazione di colonne linearmente dipendenti, che rappresentano sinonimi. 

L'obiettivo è ridurre questi sinonimi a una sola dimensione, concentrandosi sui pattern sintattici e non sulle sfumature semantiche. 

## Spell Checking

* **Prossimità tra stringhe:** Un algoritmo di spell checking dovrebbe essere in grado di gestire la prossimità tra stringhe, ovvero la possibilità di correggere un errore di battitura in più modi equivalenti.
* **Edit distance:** L'edit distance misura la differenza tra due stringhe, ma non fornisce informazioni sul contesto.
* **Contesto:** Per una correzione accurata, sarebbe necessario conoscere il contesto semantico della parola, ma noi lavoriamo solo sul campo lessicale.
* **Evitare spell checking:** Se non strettamente necessario, è meglio evitare il spell checking.
* **Corpus rumorosi:** In alcuni casi, come ad esempio i messaggi istantanei, il corpus è molto rumoroso e il spell checking potrebbe essere inutile.

## Emoticon

Le emoticon, costruite con simboli ASCII, possono essere sostituite con termini o mantenute con un markup, e la loro pesatura è delegata a fasi successive. Sono importanti per l'analisi del sentiment.

## Tesauri

I tesauri generalizzano termini con significato correlato ma forme superficiali diverse, creando un indice più uniforme. Questo approccio affronta il problema di sinonimi e omonimi.

* **Gestione di sinonimi e omonimi:** I tesauri permettono di gestire sia i sinonimi (parole con significato simile ma forma diversa) che gli omonimi (parole con forma uguale ma significato diverso).

* **Esempio:** Classi di equivalenza possono essere costruite manualmente, ad esempio color = colour

* **Implementazione:** Ci sono due approcci principali:

 * **Indicizzazione multipla:** Quando un documento contiene "color", viene indicizzato anche sotto "color-colour" (e viceversa). Questo crea una classe di equivalenza esplicita nell'indice.

 * **Espansione della query:** Quando una query contiene "color", la ricerca viene estesa anche al termine "colour". Questo approccio espande lo spazio di ricerca per includere termini correlati.

### Errori di Ortografia

Un approccio per gestire gli errori di ortografia è l'utilizzo di algoritmi come Soundex. Soundex crea classi di equivalenza di parole basate su euristiche fonetiche, raggruppando parole che suonano in modo simile anche se scritte diversamente.

## Lemmatizzazione

- La lemmattizzazione è un processo di normalizzazione che riduce le forme flessionali di una parola alla sua forma base, o lemma. Questo si applica in particolare a verbi (riducendoli all'infinito) e sostantivi (riducendoli al singolare).
- Richiede un'analisi morfologica completa e, sebbene preservi i concetti principali (es. "the boy's cars are different colors" → "the boy car be different color"), i suoi benefici per il retrieval sono generalmente modesti.  È un effetto collaterale dell'analisi sintattica, non l'obiettivo principale.

## Stemming

Lo stemming, o "suffix stripping", è un processo che riduce le parole alle loro radici prima dell'indicizzazione. La radice è considerata il prefisso della parola. Questo processo è intuitivo per i sostantivi, ma più complesso per i verbi.

È importante notare che lo stemming **distrugge informazioni lessicali**. Dovrebbe essere utilizzato solo quando non è necessaria un'elaborazione semantica dei testi. È spesso combinato con la rimozione delle *stop words*. Non ha senso combinarlo con la lemmattizzazione, in quanto lo stemming è un processo più aggressivo.

* **Dipendenza dalla lingua:** Gli algoritmi di stemming sono specifici per ogni lingua.

* **Esempio:** Le parole "automate(s)", "automatic" e "automation" potrebbero essere tutte ridotte a "automat".

##### Esempi di equivalenze create dallo stemming:

* "compressed" e "compression" sono entrambe considerate equivalenti a "compress".
* "compress" e "compress" sono entrambe considerate equivalenti a "compress". (Questo evidenzia la natura semplicistica dello stemming).

### Stemming di Porter


L'algoritmo di stemming di Porter, il più comune per l'inglese, itera su un insieme di regole pre-costruite, applicando ripetutamente riduzioni basate sul *longest match*.  
- Utilizza una notazione che descrive le parole come $[c](vc)m[v]$
	- Dove `c` rappresenta una consonante, `v` una vocale, e `m` la "misura" (ripetizioni di `vc`).  
- L'algoritmo è composto da 5 fasi, applicate in sequenza, ciascuna con un insieme di regole basate su condizioni e sostituzioni di suffissi 
	- (es. `sses` → `ss`, `ed` → (vuoto se preceduto da vocale)).  
- Un esempio di applicazione dell'algoritmo: `GENERALIZATIONS` → `GENERALIZATION` → `GENERALIZE` → `GENERAL` → `GENER`.

| Conditions | Suffix | Replacement | Examples |
| ------------------ | ------ | ----------- | ---------------------------------- |
| | sses | ss | caresses -> caress |
| | ies | i | ponies -> poni, ties -> ti |
| | ss | ss | caress -> caress |
| | s | | cats -> cat |
| (m > 0) | eed | ee | feed -> feed, agreed -> agree |
| (*v*) | ed | | plastered -> plaster, bled -> bled |
| (*v*) | ing | | motoring -> motor, sing -> sing |
| (m > 1) | e | | probate -> probat, rate -> rate |
| (m = 1 and not *o) | e | | cease -> ceas |
Dove:
- `*v*` - lo stem contiene una vocale.
- `*o` - lo stem termina in `cvc`, dove la seconda `c` non è W, X o Y (es. -WIL, -HOP).

### Vantaggi dello Stemming

Lo stemming può migliorare le prestazioni del retrieval, ma i risultati sono contrastanti. L'efficacia dipende dal vocabolario specifico e può portare alla perdita di sottili distinzioni semantiche.  
Mentre per l'inglese i risultati sono contrastanti (migliora il recall ma danneggia la precisione per alcune query, es. "operative" → "oper"), per altre lingue come spagnolo, tedesco e finlandese è generalmente più utile.

### Algoritmi di Stemming

Le prestazioni di vari algoritmi di stemming sono simili.

* **Stemmer di Lovins:**
	* Passaggio singolo, rimozione del suffisso più lungo (circa 250 regole).
* **Stemmer di Paice/Husk**
* **Snowball**
