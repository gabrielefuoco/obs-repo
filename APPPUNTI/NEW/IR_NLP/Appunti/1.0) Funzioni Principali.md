## Linguaggio naturale

* **Unicità:** Il linguaggio naturale è un linguaggio specifico per gli umani, progettato per gestire la complessità della comunicazione.
* **Interazione multimodale:** È fondamentale per interagire in contesti che coinvolgono anche altre modalità, come la visione.

### Recupero delle informazioni (IR) e Elaborazione del linguaggio naturale (NLP)

Entrambi sono campi della scienza e dell'ingegneria.
* **NLP:** Si concentra sullo sviluppo di sistemi automatici che comprendono e generano linguaggi naturali.
* **IR:** Si concentra sul ritrovamento (semi-)automatico di dati testuali (non strutturati) che soddisfano un bisogno informativo all'interno di grandi collezioni.

### Definizione di IR

###### Enunciato del problema di base:

* **Dato:**
 * **Documenti:** si presume che sia una collezione statica.
 * **Bisogno informativo (query):** Un singolo termine di ricerca, una stringa di termini, una frase in linguaggio naturale o un'espressione stilizzata che utilizza simboli speciali.
 * **Modello di recupero:** Un meccanismo per determinare se un documento corrisponde alla query.
* **Obiettivo:** Recuperare documenti con informazioni rilevanti per il bisogno informativo dell'utente e che aiutino l'utente a completare un'attività.
 * **Risultati:** Un insieme di risultati. 

## Problemi con i dati testuali

* **Sfide generali:** Tutti i problemi e le sfide noti nella gestione/estrazione dei dati si estendono ai dati testuali, come:
 * Grandi set di dati
 * Alta dimensionalità
 * Dati rumorosi
 * Dati e conoscenza in continua evoluzione
 * Comprensibilità dei risultati
* **Sfide specifiche:** Oltre alle sfide generali, i dati testuali presentano problemi specifici:
 * **Progettazione:** Il testo non è progettato per essere utilizzato dai computer.
 * **Struttura e semantica:** Struttura e semantica complesse e scarsamente definite, con ambiguità nel linguaggio, morfologia, sintassi, semantica e pragmatica.
 * **Linguaggi:** Linguaggi generali vs. linguaggi specifici di dominio.
 * **Multilinguismo:** La gestione di lingue diverse.

## IR vs. altre discipline

**Obiettivo comune:** Rendere più facile trovare le cose, ad esempio sul Web.

##### Attività comuni:

* Ottimizzazione dei motori di ricerca (SEO)
* Crawling dei dati
* Estrazione di documenti di interesse da un enorme mucchio

##### Differenze:

* **Web scraping:** Estrazione di informazioni.
* **Trovare schemi in grandi collezioni:** NLP, Linguistica computazionale.
* **Scoprire informazioni finora sconosciute:** NLP, Estrazione di testo.
* **Discriminazione del testo:** NLP (Machine Learning).
* **Comprensione del testo:** NLP (Machine Learning, AI generativa, ...). 

## Topic Detection and Tracking

**Contesto:** Fine anni '90 (inizio del text mining)

**Definizione:** Capacità di un sistema automatico di acquisire dati (principalmente news o feed) in streaming.

**Importanza:** Era un campo di ricerca rilevante prima dell'avvento dei big data.

##### Caratteristiche:

* **Analisi dei dati incrementale:** I dati vengono analizzati in modo continuo, man mano che vengono acquisiti.
* **Supervisionato o non supervisionato:** Non è specificato se il processo di analisi sia supervisionato o meno.
#### Difficoltà nel Topic Detection and Tracking

* **Task incrementale:** Il sistema automatico deve essere in grado di riconoscere nuovi topic emergenti e di identificare quelli obsoleti.
* **Capacità limitate:** La fase di tracking richiede risorse computazionali limitate.
* **Definizione di topic:** Un topic è un insieme di termini, anche se non è l'unica definizione possibile.
* **Segnale:** Il sistema deve tenere traccia del segnale, ovvero dei dati in ingresso, per identificare i topic.
### Modellazione di topic stocastica

- È una tecnica che utilizza modelli probabilistici per analizzare grandi quantità di testo e identificare i temi principali (topic) presenti nei documenti. 
- Si basa sull'idea che ogni documento sia una combinazione di diversi topic, e che ogni parola abbia una probabilità di appartenere a un determinato topic.
- Questo approccio permette di scoprire i temi principali, analizzare la loro distribuzione nei documenti e classificare i documenti in base ai topic che contengono. 
- Un esempio di modello di topic stocastico è il **Latent Dirichlet Allocation (LDA).** 

### Relation Extraction

* **Relazione con Named Entity Recognition:** Per estrarre le relazioni tra entità nominate, è necessario prima identificare le entità stesse.
* **Pattern frequenti:** La ricerca di pattern frequenti è un passo fondamentale per l'estrazione di relazioni.
* **Definizione in NLP:** In NLP, la relation extraction si riferisce all'identificazione di relazioni lessicali tra entità nominate.

### Summarization

* **Approccio tradizionale:** Negli anni '90, la summarization era affrontata come estrazione di parole chiave (key phrase extraction).
* **Guida alla summarization:** Le proprietà di un documento possono essere utilizzate per guidare il processo di summarization. 

## KDD Pipeline in NLP

* **Fasi:** I task di NLP si svolgono attraverso una sequenza di fasi (*unfolding*).
* **Rappresentazione:** Prima di poter rappresentare i contenuti informativi, è necessario indicizzare gli elementi costitutivi.
* **Apprendimento:** L'apprendimento avviene a valle del set di risultati (*result set*).
* **Feature Selection:** Il machine learning può essere utilizzato anche in fase di selezione delle features.

### Valutazione

* **Criteri statistici:** La statistica offre diversi criteri per la valutazione.
* **Accuracy:** L'accuracy presenta diversi problemi nella stima delle prestazioni di un classificatore. 

# Funzioni Principali

* **Funzione principale:** Il compito principale di un sistema di elaborazione del linguaggio naturale (*NLP*) è quello di **estrarre informazioni** da testi.
* **Macro categorie:** Le funzioni principali possono essere suddivise in due macro categorie:
 * **Indicativa:** Questa funzione serve a rivelare gli elementi dei contenuti in modo da consentire la determinazione della rilevanza del testo rispetto alle query. È fondamentale per l'esplorazione dei corpus e per il retrieval.
 * **Informativa:** Questa funzione consente di ottenere un surrogato del testo (o di una sua porzione) senza fare necessariamente riferimento al testo originale. Questo concetto è ancora valido per il retrieval, che consiste nel condensare i dati originali in un formato surrogato.

## Browsing

* **Definizione:** La navigazione (browsing) è solitamente parte di sistemi di ipertesto e ipermedia.
* **Scopo:** Permette agli utenti di scorrere collezioni di testo alla ricerca di informazioni utili.
* **Vantaggi:** Gli utenti non hanno bisogno di:
 * generare descrizioni di ciò che desiderano
 * specificare in anticipo gli argomenti di interesse.
* **Funzionamento:** Gli utenti possono semplicemente indicare i documenti che trovano rilevanti.
* **Utilizzo:** La navigazione è utile quando un utente:
 * non ha un bisogno chiaro,
 * non riesce a esprimere il suo bisogno in modo accurato,
 * o è un utente occasionale delle informazioni. 
* **Supporto all'annotazione:** Eventualmente, se la navigazione non è limitata a se stessa, può servire a supporto dell'annotazione.
#### Utilizzo: Valutazioni e Gold Standard

**Scenario:** Valutazioni in assenza di un esperto di dominio per generare un gold standard.

* **Problema:** In alcuni casi, non è possibile ottenere un gold standard (set di dati perfetto) generato da un esperto di dominio.
* **Soluzione:** Si può ricorrere a un **silver standard**, generato in modo automatico.
* **Esempio:** Nel caso di AI generativa e problemi di allineamento dei modelli linguistici di grandi dimensioni (LLM), spesso si utilizza GPT-4 per generare un silver standard.

## Estrazione di Informazioni

**Contesto:** Insieme alla classificazione e al clustering, l'estrazione di informazioni è un task fondamentale nell'analisi di informazioni sul web.

**Esempio:** Web wrapping

**Definizione:** Recuperare informazioni specifiche dai documenti, estraendo o inferendo risposte dalla rappresentazione del testo. In pratica, dato un documento già identificato come rilevante per un task di retrieval (a monte di esso), si concentra sulla selezione di informazioni specifiche.

##### Esempi:

* **Retrieval sul web:** Schemi pre-esistenti possono aiutare a identificare le informazioni rilevanti.
* **E-commerce:** La costruzione di un database può facilitare l'estrazione di informazioni specifiche.
##### In altre parole:

* Il sistema ha già identificato un documento come rilevante per una specifica richiesta.
* L'estrazione di informazioni si concentra quindi sull'individuazione di informazioni specifiche all'interno di quel documento.
* Questo processo può essere facilitato da schemi o database pre-esistenti, come nel caso del retrieval sul web o nell'e-commerce.

##### Tipi di template:

* **Slot:** Gli slot nel template sono tipicamente riempiti da una sottostringa del documento.
* **Riempitivi pre-specificati:** Alcuni slot possono avere un set fisso di riempitivi pre-specificati che potrebbero non apparire nel testo stesso.
* **Multipli riempitivi(filler):** Alcuni slot possono consentire più filler.
* **Ordine fisso:** Si assume che gli slot siano sempre in un ordine fisso.

##### Modelli di estrazione:

* **Specificare un elemento:** Specificare un elemento da estrarre per uno slot, ad esempio, utilizzando un modello di regex.
* **Modelli precedenti e successivi:** Potrebbe essere necessario un modello precedente (pre-filler) per identificare il contesto appropriato e un modello successivo (post-filler) per identificare la fine del riempitivo. 
	* **Es**: *Estrazione di termini da un modello*. Definiziamo il template a priori e per ogni slot creiamo dei pattern.

## Recupero di Documenti

Selezionare documenti da una collezione in risposta a una query dell'utente.

* La richiesta di ricerca è solitamente formulata in linguaggio naturale.
* Classificare questi documenti in base alla loro rilevanza rispetto alla query.
* Abbinamento tra la rappresentazione del documento e la rappresentazione della query.
* Restituire un elenco di possibili testi pertinenti, le cui rappresentazioni corrispondono meglio alla rappresentazione della richiesta.

**Modelli di recupero:** Booleano, Spazio vettoriale, Probabilistico, Basato sulla logica(no), ecc.

##### Differenze rispetto a:

* Rappresentazione dei contenuti testuali.
* Rappresentazione dei bisogni informativi.
* E il loro abbinamento. 

## Panoramica delle Applicazioni di Base

Le principali aree di applicazione coprono due aspetti:
* **Scoperta della conoscenza**
 * **Estrazione di informazioni:** il processo di estrazione di informazioni utili da grandi quantità di dati.
* **Distillazione delle informazioni**
 * **Estrazione basata su struttura:** l'estrazione di informazioni basata su una struttura predefinita di documenti, per identificare i documenti rilevanti per un'informazione target.

##### Utilizzi tipici:

* **Categorizzazione gerarchica:** Usare il retrival per organizzare i documenti in una struttura gerarchica basata su categorie (Seguendo l'esempio delle web directory).
* **Riassunto del testo:** Creare un riassunto conciso di un documento, mantenendo le informazioni più importanti.
* **Disambiguazione del senso delle parole:** Determinare il significato corretto di una parola in un contesto specifico.
- **Filtraggio del Testo (o dell'Informazione)**: Rimuovere il testo non rilevante da un documento. Inizialmente accoppiato alla web personalization, da cui nascono gli attuali sistemi di raccomandazione.
- **Approcci alla** **Web Personalization:**
	* Selezionare l'informazione per un gruppo di utenti target in base alle loro specifiche esigenze.
	* **Collaborative based:** il sistema continua a proporre non solo i prodotti in base alle specifiche esigenze, ma anche cose spesso acquistate da utenti con un profilo simile, per un discorso anche di diversificazione e serendipità.

##### Altri esempi di applicazioni:

* **CRM e marketing:** (Customer Relationship Managment) Cross-selling, raccomandazioni di prodotti.
* **Raccomandazione di prodotti:** Suggerire prodotti pertinenti agli utenti in base alle loro preferenze.
* **Consegna di informazioni nelle organizzazioni per la gestione della conoscenza:** Fornire informazioni pertinenti ai dipendenti per migliorare la collaborazione e la produttività.
* **Personalizzazione dell'accesso alle informazioni:** Adattare l'accesso alle informazioni alle esigenze individuali degli utenti.
* **Filtraggio di notizie in newsgroup Usenet:** Rimuovere le notizie non pertinenti o indesiderate.
* **Rilevamento di messaggi spam:** Identificare e filtrare i messaggi di posta elettronica indesiderati. 

### Categorizzazione Gerarchica

La categorizzazione gerarchica, tipicamente implementata tramite tassonomie, permette ai ricercatori di esplorare un insieme di dati in modo strutturato. 
Inizialmente, si naviga attraverso la gerarchia delle categorie per poi focalizzarsi su una specifica categoria di interesse, affinando così la ricerca. Questo sistema dovrebbe essere flessibile, consentendo l'aggiunta di nuove categorie e la rimozione di quelle obsolete.

##### Caratteristiche:

* Natura ipertestuale dei documenti: Analisi degli hyperlink
* Struttura gerarchica dell'insieme di categorie
* Decomposizione della classificazione come decisione ramificata (A un nodo interno)

### Summarization

La summarization consiste nel generare un riassunto del contenuto di un testo. Per testi brevi, il riassunto deve essere essenziale e coerente. L'utilizzo di profili aiuta a strutturare le informazioni importanti in campi semanticamente ben definiti. La tecnica di summarization dipende dalla natura dei documenti; ad esempio, per la summarization di recensioni, è possibile addestrare il modello per analizzare aspetti specifici, evitando un riassunto trasversale e generando invece riassunti per ogni aspetto elencato.

La summarization facilita principalmente l'accesso alle informazioni, come mostrato nei seguenti esempi:

* Estrazione delle parole chiave più utili da un insieme di documenti (es. un cluster) per descriverlo.
* Astrazione dei documenti in una collezione per evitare la lettura del contenuto completo.
* Riassunto dei documenti recuperati da una ricerca per consentire all'utente un'identificazione più rapida di quelli pertinenti alla query.

Un riassunto può essere di alto livello, fornendo una panoramica di tutti i punti principali, oppure più dettagliato. Gli approcci alla summarization possono essere classificati in base alla dimensione dell'unità di testo utilizzata:

* **Riassunti di parole chiave:** Utilizzano parole chiave per rappresentare il contenuto principale.
* **Riassunti di frasi:** Utilizzano frasi per rappresentare il contenuto principale.

### Disambiguazione del Senso delle Parole (WSD)

La disambiguazione del senso delle parole (WSD) consiste nell'assegnare a una parola il significato corretto in base al contesto in cui appare. È importante notare che, pur mirando alla disambiguazione di ogni parola singolarmente, è necessario considerare l'intero contesto per una corretta interpretazione. Questo richiede un'analisi simultanea di tutti i termini nel contesto (aspetto da ampliare per supportare strumenti automatici).

**Un approccio efficace:** Prevede di sfruttare un inventario di sensi esistenti e misure di correlazione semantica per scegliere il significato più appropriato per ogni parola.

La WSD è un esempio del problema più generale di risoluzione delle ambiguità del linguaggio naturale.

##### Esempio:

* La parola inglese "bank" può avere (almeno) due significati:
 * "The Bank of England" (istituto finanziario)
 * "The bank of the river Thames" (argine fluviale)
* Per disambiguare l'occorrenza di "bank" in "Last week I borrowed some money from the bank", è necessario considerare il contesto per determinare il significato corretto (istituto finanziario).

**Approcci Knowledge-Based:** Questa task ha conosciuto un rinnovato interesse con la disponibilità di risorse linguistiche elettroniche e, grazie al web, di risorse come Wikipedia (la cui tassonomia può essere assimilata a quella di un thesaurus).

## Filtraggio nel Text Mining

##### Definizione:

Il filtraggio nel text mining consiste nel classificare un flusso di documenti inviati da un produttore di informazioni a un consumatore di informazioni, bloccando la consegna di quelli irrilevanti per il consumatore.

**Esempio:** Un feed di notizie dove l'agenzia di stampa è il produttore e il giornale è il consumatore.

##### Caratteristiche:

* Considerabile come un caso di Text Classification con una singola etichetta (*rilevante o irrilevante*).
* Implementabile sia lato produttore (*instradamento selettivo*) che lato consumatore (*blocco*).
* Richiede la creazione e l'aggiornamento di un "*profilo*" per ogni consumatore (*lato produttore*) o un singolo profilo generale (*lato consumatore*).

##### Filtraggio Adattivo:

* Il profilo iniziale è definito dall'utente.
* Il profilo viene aggiornato in base al feedback dell'utente sulla pertinenza dei documenti consegnati.

### Filtraggio - CRM (Customer Relationship Management)

**Obiettivo:** Aiutare le aziende a comprendere le opinioni dei clienti.

##### Processo:

1. **Standardizzazione:** I dati di feedback vengono convertiti in un formato uniforme.
2. **Raggruppamento:** I dati standardizzati vengono raggruppati in base alla similarità del contenuto.
3. **Assegnazione:** I nuovi feedback vengono assegnati alle categorie predefinite.

### Filtraggio - Raccomandazione di Prodotti

##### Approcci:

* **Basato sul contenuto:** Analizza i dati dell'utente (es. categorie preferite, autori) per suggerire prodotti simili.
* **Collaborativo:** Suggerisce prodotti acquistati da utenti con profili simili, basandosi su cronologia e valutazioni.

**Tendenza attuale:** Combinare entrambi gli approcci per migliorare l'accuratezza delle raccomandazioni.

### Rilevamento dello Spam

**Applicazione:** Classificare le email come spam o legittime utilizzando il text mining.

##### Sfide:

* La valutazione del modello è complessa a causa dei costi asimmetrici degli errori (falsi positivi vs. falsi negativi).
* La distribuzione non uniforme delle classi (molto più spam che email legittime) complica l'apprendimento del classificatore. 

## Vocabolario: Modello di Rappresentazione dei Testi

### Definizione di Termine

**Problema:** Quali termini di un documento indicizzare? Dobbiamo considerare tutte le parole o solo quelle ritenute "importanti"? Come gestire la frequenza delle parole, considerando che alcune sono molto comuni mentre la maggior parte è rara?

**Contesto:** I testi sono composti da parole. La selezione dei termini per l'indicizzazione (es. indice analitico di un libro) è fondamentale. L'indice è uno strumento per raggiungere un obiettivo specifico.

**Risposta:** È necessario mettere in relazione le parole tra loro, creando pattern relazionali. L'importanza di una parola dipende da diversi concetti.

##### Cosa è un termine?

La definizione di "termine" è flessibile: può essere una parola singola, una coppia di parole, una frase, la radice di una parola (word stem), un n-gramma (sequenza di n caratteri), o un tipo di parola.

##### Modellazione delle relazioni tra termini:

Possiamo considerare diverse relazioni tra termini, ma per semplificare i modelli, si evitano generalmente:

* Paragrafi, frasi e ordine delle parole.
* Strutture sintattiche complesse come ellissi e anafore.

Si preferiscono invece le relazioni semantiche (es. relazioni "is-a", "part-of").

**Obiettivo:** Identificare termini che catturano la semantica del testo, evitando un'intensa elaborazione manuale (codifica manuale).

##### Altri argomenti correlati:

* **Analisi lessicale e morfologica:** Include elaborazione della punteggiatura, conversione in minuscolo, rimozione di stopwords, stemming, lemmatizzazione e tagging delle parti del discorso.
* **Analisi della semantica del discorso (Anafora):** Considera l'anafora letterale/pronominale, l'ellissi testuale e la meronomia referenziale.
* **Pragmatica, Semiotica e Morfologia:** Questi campi forniscono ulteriori prospettive per una comprensione più completa del linguaggio.

## Tokenizzazione: Problemi Lessicali e Morfologici

Un token rappresenta un termine candidato durante la fase di pre-processing nell'analisi del testo. La tokenizzazione, quindi, si occupa di come organizzare un testo in unità significative.

### Punteggiatura

La gestione della punteggiatura presenta diverse sfide:

* **Sequenze con trattino:** Termini come *state-of-the-art*, *co-education*, *lowercase*, *lower-case*, *lower case* pongono il problema di come trattarli. Mantenere le parole separate è una soluzione semplice ma può compromettere la semantica. Un approccio più sofisticato è necessario, ma non sempre scalabile.

* **Apostrofi:** La gestione di apostrofi in termini come *Italy’s capital* richiede una decisione su come rappresentarli (*Italy AND s*, *Italys*, *Italy’s*?).

* **Acronimi:** Acronimi come *U.S.A* e *USA* possono essere trattati in modo diverso a seconda del contesto.

* **Entità nominate:** Entità come *San Francisco* e *Hewlett-Packard* richiedono un trattamento specifico per evitare di frammentarle in unità non semantiche.

### Numeri

La gestione dei numeri richiede una scelta tra rimozione e mantenimento. Generalmente, si rimuovono le stringhe puramente numeriche, mentre si mantengono quelle alfanumeriche.

##### Motivazioni per il mantenimento dei numeri:

* Preservazione delle informazioni numeriche contenute nel testo.
* Necessità di individuare codici di errore, intervalli temporali, ecc.

**Metadati:** I metadati (data di creazione, formato, ecc.) sono spesso indicizzati separatamente. La complessità dell'interazione tra numeri e testo rende difficile una soluzione universale.

**Varietà di tipi numerici:** I numeri possono assumere diverse forme:

* **Date:** *3/20/91*, *Mar. 12, 1991*, *20/3/91*.
* **Tempo:** *55 B.C.*
* **Codici:** *B-52*.
* **Identificatori:** *My PGP key is 324a3df234cb23e*.
* **Numeri di telefono:** *(800) 234-2333*.

**Case delle lettere:** Generalmente, si converte tutto il testo in minuscolo (lowercase).

## Stopwords

È una fase banale, ma risulta fondamentale perchè è quella con l'impatto maggiore.
* **Definizione:** Parole grammaticali che servono a scopi grammaticali ma con scarso potere informativo. Sono specifiche della lingua.

* **Principio cardine nella pipeline di data analysis:** Le colonne con distribuzione super omogenea o super eterogenea vengono rimosse perché non sono informative. 
	* Le dimensioni delle rappresentazioni dovrebbero avere un giusto trade-off di generalizzazione e omogeneità.

* **Motivi per rimuovere le stopwords:**
 * Non si riferiscono a oggetti o concetti.
 * Hanno poco contenuto semantico.
 * Sono le parole più comuni.
 * Impattiamo sulla dimensionalità della rappresentazione dei nostri testi
 * Ciò implica **sparsità**: insieme sono un problema per le misure di distanza, che perdono di sensibilità $\to$ *perdita di potere discriminante*

* **Casi in cui le stopwords potrebbero essere necessarie:**
 * Query di frase: "King of Denmark"
 * Titoli: "Let it be", "To be or not to be"
 * Query "relazionali": "flights to London"

* **Gestione delle stopwords:**
 * **Buone tecniche di compressione:** Lo spazio per includere le stopwords in un sistema è molto piccolo.
 * **Buone tecniche di ottimizzazione delle query:** Si paga poco al momento della query per includere le stopwords. 
## Personalizzazione della Stop-List

**Obiettivo:** Non solo ridurre la stop-list, ma arricchirla.
* La stop-list deve contenere termini che si aspettano di essere molto frequenti all'interno della collezione. 

##### Come e con cosa?

* **Rimozione di termini comuni al corpus:** 
 * Se un termine è frequente nel corpus di documenti, è poco esplicativo per la distinzione tra i documenti stessi.
 * La rimozione di questi termini è giustificata quando si analizzano i documenti nel contesto del corpus.
* **Mantenimento dei termini frequenti per l'analisi individuale:**
 * Se si analizza ogni documento in modo indipendente, non è possibile rimuovere i termini più frequenti across collection.
- **Termini Super Rari**
	* **Esclusione:**
	 * È ragionevole considerare l'esclusione dei termini super rari, ma è un'operazione ancora più delicata della rimozione dei termini comuni.
	 * Ciò che distingue un testo dall'altro è più importante di ciò che li accomuna.
	* **Termini che appaiono una sola volta nel corpus:**
	 * La decisione di escluderli dipende dal tipo di termine.
	 * Non esiste una regola fissa.

## Normalizzazione

L'obiettivo della normalizzazione è uniformare la forma delle parole nel testo indicizzato e nelle query di ricerca. Questo permette di trovare corrispondenze tra termini che, pur avendo forme diverse, rappresentano lo stesso concetto. 
Ad esempio, "U.S.A." e "USA" dovrebbero essere trattati come equivalenti.
È fondamentale normalizzare in modo coerente il testo indicizzato e i termini delle query per garantire l'efficacia della ricerca.

##### Risultato:

* **Termine:** Un tipo di parola, dopo la normalizzazione.
* **Voce nel dizionario IR:** Ogni termine normalizzato rappresenta una voce nel dizionario di Information Retrieval (IR).

##### Classi di equivalenza:

La normalizzazione spesso si basa sulla creazione di classi di equivalenza, raggruppando termini considerati equivalenti. Esempi di operazioni di normalizzazione includono:

* **Eliminazione di punti:** "U.S.A.", "USA" → "USA"
* **Eliminazione di trattini:** "anti-discriminatory", "antidiscriminatory" → "antidiscriminatory"
* **Eliminazione di accenti:** "French résumé" vs. "resume"
* **Eliminazione di umlaut:** "German Tuebingen" vs. "Tübingen"

Questo processo semplifica la rappresentazione del testo, focalizzandosi sulla struttura sintattica a discapito di alcune sfumature semantiche. Diverse parole con significati simili vengono ridotte a una sola forma.

##### Espansione asimmetrica:

Un'alternativa alle classi di equivalenza è l'espansione asimmetrica. In questo caso, l'inserimento di un termine porta all'inclusione di diverse varianti durante la ricerca:

* Inserire: "window" → Ricerca: "window", "windows"
* Inserire: "windows" → Ricerca: "Windows", "windows", "window"
* Inserire: "Windows" → Ricerca: "Windows"

Questo approccio può essere più potente, ma meno efficiente.

##### Criterio più importante: uso della lingua

La scelta del metodo di normalizzazione dipende fortemente dall'uso della lingua. Considerando che gli utenti spesso omettono accenti e trattini nelle query, è spesso preferibile normalizzare a una forma senza questi elementi.

##### Tokenizzazione e normalizzazione:

Tokenizzazione e normalizzazione sono processi interdipendenti e spesso dipendono dalla lingua. Ad esempio, i formati di data "3/10/2000" (US) e "10/3/2000" (UE) richiedono un trattamento diverso.

## Case Folding

Tipicamente, si riducono tutte le lettere in minuscolo.
Se trattiamo questo task in maniera sofisticata dobbiamo avere un modo per riconoscere le named entity.
Eventualmente, possiamo mantenere solo gli acronimi
**Eccezione:** maiuscole a metà frase?

* **Esempio:** General Motors
* **Esempio:** Fed vs. fed
* **Esempio:** SAIL vs. sail

Spesso è meglio mettere tutto in minuscolo, poiché gli utenti useranno le minuscole indipendentemente dalla "corretta" capitalizzazione.

### Riduzione della matrice dei dati tramite algebra lineare

Come fase di post-processing, si può applicare l'algebra lineare alla matrice dei dati per ridurla ulteriormente. Questo processo si basa sull'identificazione di colonne linearmente dipendenti, che rappresentano sinonimi. 

L'obiettivo è ridurre questi sinonimi a una sola dimensione, concentrandosi sui pattern sintattici e non sulle sfumature semantiche. 

## Spell Checking

* **Prossimità tra stringhe:** Un algoritmo di spell checking dovrebbe essere in grado di gestire la prossimità tra stringhe, ovvero la possibilità di correggere un errore di battitura in più modi equivalenti.
* **Edit distance:** L'edit distance misura la differenza tra due stringhe, ma non fornisce informazioni sul contesto.
* **Contesto:** Per una correzione accurata, sarebbe necessario conoscere il contesto semantico della parola, ma noi lavoriamo solo sul campo lessicale.
* **Evitare spell checking:** Se non strettamente necessario, è meglio evitare il spell checking.
* **Corpus rumorosi:** In alcuni casi, come ad esempio i messaggi istantanei, il corpus è molto rumoroso e il spell checking potrebbe essere inutile.

## Emoticon

* **Costruzione:** Utilizzano simboli di punteggiatura (in codifica ASCII).
* **Preservazione:** Per evitare perdite, possono essere sostituite con termini, eventualmente con un markup.
* **Pesatura:** La pesatura dei termini può essere demandata al modulo successivo.
* **Importanza:** Sono importanti nell'analisi del sentiment e in task simili. 

## Tesauri

I tesauri generalizzano termini con significato correlato ma forme superficiali diverse, creando un indice più uniforme. Questo approccio affronta il problema di sinonimi e omonimi.

* **Gestione di sinonimi e omonimi:** I tesauri permettono di gestire sia i sinonimi (parole con significato simile ma forma diversa) che gli omonimi (parole con forma uguale ma significato diverso).

* **Esempio:** Classi di equivalenza possono essere costruite manualmente, ad esempio:
 * car = automobile
 * color = colour

* **Implementazione:** Ci sono due approcci principali:

 * **Indicizzazione multipla:** Quando un documento contiene "automobile", viene indicizzato anche sotto "car-automobile" (e viceversa). Questo crea una classe di equivalenza esplicita nell'indice.

 * **Espansione della query:** Quando una query contiene "automobile", la ricerca viene estesa anche al termine "car". Questo approccio espande lo spazio di ricerca per includere termini correlati.

### Errori di Ortografia

Un approccio per gestire gli errori di ortografia è l'utilizzo di algoritmi come Soundex. Soundex crea classi di equivalenza di parole basate su euristiche fonetiche, raggruppando parole che suonano in modo simile anche se scritte diversamente.

## Lemmatizzazione

La lemmattizzazione è un processo di normalizzazione che riduce le forme flessionali di una parola alla sua forma base, o lemma. Questo si applica in particolare a verbi (riducendoli all'infinito) e sostantivi (riducendoli al singolare).

* **Esempio:** La frase "the boy's cars are different colors" potrebbe essere lemmattizzata in "the boy car be different color". Si noti che i concetti principali vengono preservati. In questo caso, la lemmattizzazione è un effetto collaterale dell'analisi sintattica, non l'obiettivo principale.

La lemmattizzazione richiede un'analisi morfologica completa per identificare correttamente il lemma di ogni parola nel dizionario.

* **Benefici per il retrieval:** I benefici della lemmattizzazione per il retrieval di informazioni sono generalmente modesti.

## Stemming

Lo stemming, o "suffisso stripping", è un processo che riduce le parole alle loro radici prima dell'indicizzazione. La radice è considerata il prefisso della parola. Questo processo è intuitivo per i sostantivi, ma più complesso per i verbi.

È importante notare che lo stemming **distrugge informazioni lessicali**. Dovrebbe essere utilizzato solo quando non è necessaria un'elaborazione semantica dei testi. È spesso combinato con la rimozione delle *stop words*. Non ha senso combinarlo con la lemmattizzazione, in quanto lo stemming è un processo più aggressivo.

* **Dipendenza dalla lingua:** Gli algoritmi di stemming sono specifici per ogni lingua.

* **Esempio:** Le parole "automate(s)", "automatic" e "automation" potrebbero essere tutte ridotte a "automat".

##### Esempi di equivalenze create dallo stemming:

* "compressed" e "compression" sono entrambe considerate equivalenti a "compress".
* "compress" e "compress" sono entrambe considerate equivalenti a "compress". (Questo evidenzia la natura semplicistica dello stemming).

### Stemming di Porter

Itera su un insieme di regole pre-costruite e data una parola si agisce ripetutamente su essa. La decisione finale di stemming viene presa sulla base di quello che è il longest match.

* Algoritmo più comune per lo stemming dell'inglese.
 * Uno stemmer a corrispondenza più lunga in più passaggi.
 * Delle regole in un comando composto, seleziona quella che si applica al suffisso più lungo.
* Notazione: v: vocale(i), c: consonante(i), (vc)m: vocale(i) seguite da consonante(i), ripetute m volte.
* Ogni parola può essere scritta: $[c](vc)m[v]$
 * m è chiamato la "misura" della parola.
* Convenzioni + 5 fasi di riduzioni.
 * Fasi applicate in sequenza.
 * **Esempio:** GENERALIZATIONS -> GENERALIZATION (Fase 1) -> GENERALIZE (Fase 2) -> GENERAL (Fase 3) -> GENER (Fase 4)
 * Ogni fase consiste in un insieme di comandi. 

| Conditions | Suffix | Replacement | Examples |
| ------------------ | ------ | ----------- | ---------------------------------- |
| | sses | ss | caresses -> caress |
| | ies | i | ponies -> poni, ties -> ti |
| | ss | ss | caress -> caress |
| | s | | cats -> cat |
| (m > 0) | eed | ee | feed -> feed, agreed -> agree |
| (*v*) | ed | | plastered -> plaster, bled -> bled |
| (*v*) | ing | | motoring -> motor, sing -> sing |
| (m > 1) | e | | probate -> probat, rate -> rate |
| (m = 1 and not *o) | e | | cease -> ceas |
Dove:
- `*v*` - lo stem contiene una vocale.
- `*o` - lo stem termina in `cvc`, dove la seconda `c` non è W, X o Y (es. -WIL, -HOP).

### Vantaggi dello Stemming

Lo stemming può influenzare le prestazioni del retrieval, generalmente in meglio, ma i risultati sono contrastanti.

* **Dipendenza dal vocabolario:** L'efficacia dello stemming dipende dal vocabolario specifico.
* **Perdita di sottili distinzioni:** Le sottili distinzioni semantiche possono essere perse attraverso lo stemming.

##### Risultati per l'inglese:

* I risultati sono molto contrastanti.
* Aiuta il recall per alcune query, ma danneggia la precisione per altre.
* **Esempio:** "operative" (odontoiatria) ⇒ "oper".

##### Risultati per altre lingue:

* Sicuramente utile per spagnolo, tedesco, finlandese, ecc.
* **Guadagni di prestazioni:** 30% per il finlandese.
* **Efficacia dello stemming automatico:** Lo stemming automatico è efficace quanto la conflazione manuale.

### Algoritmi di Stemming

Le prestazioni di vari algoritmi di stemming sono simili.

* **Stemmer di Lovins:**
 * Passaggio singolo, rimozione del suffisso più lungo (circa 250 regole).
* **Stemmer di Paice/Husk:**
* **Snowball:**

Tutti questi algoritmi hanno lo stesso impatto, ma presentano diverse peculiarità. 
