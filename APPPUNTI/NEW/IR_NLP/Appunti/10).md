## Language Modeling

Il *language modeling* è un task autoregressivo che si concentra sulla generazione di testo.  L'input consiste in una sequenza di parole osservate, $x_1, ..., x_t$ (dove *t* rappresenta il time step). Il task consiste nel predire la parola successiva, $x_{t+1}$.

Si assume che il vocabolario sia noto a priori e che il generatore campioni da esso secondo specifiche strategie.

![[10)-20241118151227953.png|552]]

La probabilità di generare un testo T può essere vista come il prodotto delle probabilità condizionate di osservare ogni parola, data la sequenza di parole precedenti:

![[10)-20241118151329195.png|585]]

Il language modeling è importante non solo per la semplice predizione della parola successiva, ma anche per una vasta gamma di applicazioni nel campo del linguaggio naturale, tra cui:

* **Machine Translation:**  Può essere considerato un caso particolare di language modeling, in quanto implica una logica di encoding nel linguaggio sorgente e decoding nel linguaggio target.
* **Speech Recognition:**  La predizione di parole successive è fondamentale per la trascrizione accurata del parlato.
* **Spelling/Grammar Correction:**  Il modello può identificare e correggere errori ortografici e grammaticali.
* **Summarization:**
    * **Estrattiva:** Evidenzia le frasi più importanti da un testo.
    * **Astrattiva:** Rimodula il testo originale creando un riassunto.  Anche la summarization astrattiva può essere considerata un caso particolare di language modeling, poiché, data una sequenza di testo in input, genera una nuova sequenza di testo in output.


# N-gram Language Models

Un n-gram è una porzione di testo composta da *n* token consecutivi.  I modelli n-gram collezionano statistiche di occorrenza di n-gram per stimare la probabilità della parola successiva.

Esempi:

* **Unigrammi:** "the", "students", "opened", "their"
* **Bigrammi:** "the students", "students opened", "opened their"
* **Trigrammi:** "the students opened", "students opened their"
* **Four-grammi:** "the students opened their"

Invece di considerare l'intero testo precedente, si utilizza una finestra di *n-1* parole per semplificare il problema.

Si fa l'assunzione di Markov:  $x^{(t+1)}$ dipende solo dalle *n-1* parole precedenti.

![[10)-20241118152157750.png]]

**Come ottenere le probabilità degli n-gram e (n-1)-gram?**

Contandole in un ampio corpus di testo!

![[10)-20241118152225539.png]]

![[10)-20241118152314916.png]]


![[10)-20241118152452827.png]]

**Problemi:**

* **Problema 1: Numeratore = 0:**  La probabilità di un n-gram potrebbe essere zero se non è presente nel corpus.
* **Problema 2: Denominatore = 0:** La probabilità di un (n-1)-gram potrebbe essere zero.  Una soluzione potrebbe essere quella di valutare l'n-2-gram, aumentando la finestra di contesto, ma non garantisce il successo.

L'utilizzo di un valore di *n* relativamente grande aumenta le dimensioni del modello (model size) e non garantisce un miglioramento delle prestazioni.  Questo approccio porta a problemi di:

* **Sparsità:**  Aumenta all'aumentare di *n*.
* **Granularità:** Diminuisce all'aumentare di *n*.

Di conseguenza, si rischia di ottenere probabilità piatte e poco informative.  Anche con un corpus di testo di dimensioni adeguate, un modello con *n* grande potrebbe generare testo grammaticalmente corretto ma privo di coerenza e fluidità.

## Costruire un Language Model Neurale

![[10)-20241118153020961.png]]

L'obiettivo è costruire un modello neurale basato su una finestra di contesto (window-based).

![[10)-20241118153136194.png]]

Ogni parola possiede un encoding.  Questo encoding viene ulteriormente trasformato tramite una matrice di parametri U. Questa trasformazione permette di individuare la parola con il punteggio (score) più alto, corrispondente alla parola più probabile nel contesto.

![[10)-20241118153152658.png]]

Dato un certo numero di input (rappresentati da one-hot vector),  gli embeddings delle parole vengono trasformati da una funzione f (es. regressione logistica).  L'apprendimento della codifica avviene nel layer nascosto (consideriamo un singolo hidden layer).  Questa codifica intermedia viene poi utilizzata in una nuova trasformazione lineare. Infine, la funzione softmax converte i punteggi grezzi (raw scores) in probabilità.

## A Fixed-Window Neural Language Model

![[10)-20241118153353157.png]]

Questo modello non richiede la memorizzazione delle statistiche di conteggio degli n-gram.

**Problemi:**

* **Dimensione della matrice W:**  Una finestra di contesto ampia implica una matrice W di grandi dimensioni.
* **Asimmetria dell'input:** Gli input vengono moltiplicati per pesi completamente diversi.  Non c'è simmetria rispetto all'ordine di presentazione dell'input.  Il fatto che una parola preceda o segua un'altra è rilevante.  Dato un contesto con una parola centrale, si desidera una rappresentazione globale del contesto per il riconoscimento di entità nominate (named entity).  Per questo, si utilizza una matrice W che non viene applicata allo stesso modo a ogni input, causando asimmetria. L'ottimizzazione dei valori in W avvantaggia le parole in momenti diversi (quelle all'inizio della sequenza potrebbero utilizzare una parte non ottimizzata della matrice).


## Reti Neurali Ricorrenti (RNN)

L'obiettivo è condividere i pesi (w) per ogni parola nella sequenza di input.

![[10)-20241118153747550.png]]

L'output può essere generato ad ogni time step o solo all'ultimo, a seconda del task specifico.  Ad esempio, nell'analisi del sentiment, interessa solo l'output finale. In questo esempio, w contribuisce ad ogni passo, quindi la codifica del passo precedente influenza ogni time step successivo. Un'architettura neurale che segue questo principio, prendendo in input una sequenza di parole, è detta rete neurale ricorrente.

![[10)-20241118154009409.png]]

Se ogni x è un vettore di dimensione b con tutti 0 e un solo 1, abbiamo una codifica one-hot di ogni parola. Questa codifica viene utilizzata nella trasformazione descritta nell'immagine.

Distinguiamo due matrici di pesi:  $w_h$ (trasformazioni dallo stato precedente) e $w_e$ (per l'input corrente al passo t).

Ogni blocco al passo *t* prende in input la codifica della parola al passo *t* e l'output trasformato (moltiplicato per la sua matrice di pesi) del passo precedente.

Ad ogni passo *t* otteniamo la codifica $h_t$.


**Pro:**
* **Simmetria dei pesi:** I pesi vengono applicati ad ogni timestep, garantendo simmetria nell'elaborazione della sequenza.
* **Dimensione del modello costante:** La dimensione del modello non aumenta con l'aumentare della lunghezza della sequenza di input.

**Contro:**
* **Lunghezza della sequenza limitata:** La lunghezza della sequenza non è arbitraria.  Il modello ha difficoltà nell'elaborare sequenze lunghe a causa di un effetto di "perdita di memoria".  Quando si valuta la probabilità della parola successiva, si osserva un'attenuazione significativa dei valori delle probabilità delle parole precedenti nella sequenza.
* **Tempo di addestramento:** L'addestramento del modello RNN richiede tempi lunghi.

## Addestramento di un Modello Linguistico RNN

L'addestramento di un modello linguistico RNN richiede un ampio corpus di testo con sequenze molto lunghe. Ad ogni timestep, il modello riceve in input una sequenza e predice la distribuzione di probabilità per la parola successiva.  La funzione di costo (loss) ad ogni timestep *t* è la cross-entropy:

$j^{(t)}(\theta) = CE(y^{(t)}, \hat{y}^{(t)}) = -\sum_{w\in V} y^{(t)}_w \log(\hat{y}^{(t)}_w)=-\log \hat{y}^{(t)}_{x_{t+1}}$

dove:

* $y^{(t)}$ è il one-hot vector rappresentante la parola effettiva al passo (t+1).
* $\hat{y}^{(t)}$ è la distribuzione di probabilità predetta dal modello al passo *t*.
* V è il vocabolario.


La loss complessiva sull'intero training set è la media delle loss calcolate ad ogni timestep *t*.  Si noti che ad ogni passo *t* si ha una predizione $\hat{y}^{(t)}$.

![[10)-20241118155458516.png]]

La loss totale è la somma cumulativa delle loss individuali ad ogni timestep.

Tuttavia, calcolare la loss e i gradienti sull'intero corpus contemporaneamente è computazionalmente troppo costoso in termini di memoria.

# Backpropagation Through Time (BPTT) per RNNs

Per addestrare una Recurrent Neural Network (RNN) si utilizza la Backpropagation Through Time (BPTT), una variante dell'algoritmo di backpropagation.  BPTT calcola i gradienti dei pesi rispetto alla funzione di costo.

![[10)-20241118160122136.png]]

Il gradiente rispetto ad un peso ripetuto (come $W_h$ nella figura) è la somma dei gradienti calcolati ad ogni timestep in cui quel peso contribuisce al calcolo.  In altre parole, è la somma di gradienti di forma identica calcolati ad ogni timestep.

Questo è un'applicazione della regola della catena, nello specifico la *Multivariable Chain Rule*.

![[10)-20241118160326719.png|690]]

Per chiarire, se dovessimo calcolare la derivata parziale di una funzione composta $f(a(b(x)))$ rispetto a $x$, dovremmo applicare la regola della catena due volte: una volta per la funzione $a$ e una volta per la funzione $b$.  Analogamente, nel caso delle RNN, ad ogni timestep dobbiamo valutare la derivata rispetto a $W_h$.  La derivata di $W_h$ al passo *t* rispetto a $W_h$ al passo *t* è 1.

Quindi, ad un generico timestep *t*, per calcolare il gradiente rispetto a $W_h$, dobbiamo propagare all'indietro il gradiente cumulativo fino all'inizio della sequenza, sfruttando il fatto che la matrice $W_h$ rimane costante ad ogni passo.  Questo processo di propagazione all'indietro dei gradienti attraverso il tempo è ciò che caratterizza la BPTT.

## Valutazione del modello

Il language model viene valutato tramite la **Perplexity**
$$\prod_{t=1}^t\left( \frac{1}{P_{ML}(x^{(t+1)})|x^{(t)},\dots,x^{(1)}} \right)^{1/t}$$
rappresenta l'inverso della probabilità del corpus, normalizzato dall'esponente che rappresetna il numero di parole
![[10)-20241118162753924.png]]

## Vanishing Gradient

nel calcolare derivate ricorsivamente, andiamo a  valutarle per probabilità molto piccole: i gradienti diventano sempre più piccoli e viene abbattuto il gradiente al passo t

![[10)-20241118163104268.png]]
definizione di h^{(t)}: applicazione di una funzione di attivazione (solitamente una funzione non lineare come la tangente iperbolica o la funzione ReLU) alla combinazione lineare dell'embedding dell'input al timestep *t*, del bias e della trasformazione dello stato nascosto al timestep precedente.


diagonalizzazione della derivata della funzione di attivazione per w_h

# registrazione 2 

spiegazione della derivata di J


![[10)-20241118163617737.png]]

quanto è vero che a creare il vanishing del gradiente è la potenza L di valori molto piccoli?
vale quando gli autovalori della matrice sono <1
il valore del gradiente al passo i può essere riscritto usando gli autovettori di W_h. con valori molto piccoli approssima a zero, dunque abbiamo dimostrato che in teroria è possibile
nella pratica tale problema esiste. 
può accadere che gli autovalori di W possono essere >1, e questo porta a un effetto contrario: esplosione del gradiente

![[10)-20241118163905967.png]]
![[10)-20241118163913300.png]]
è un problema di divergenza del gradiente, si risolve più facilmente del problema vanishing.
è molto frequente, si risolve con operazioni di normalizzazione: può essere uno scaling tale per cui i valori abbiamo norma pari a 1
in una rete ricorrente si usa spesso il clipping, che è un tresholding del gradiente
si sceglie una soglia e ad ogni passo scaliamo il gradiente rispetto una soglia fissata

## Fixing the vanishing gradient problem
Gradient Signal from far away iS Iost because it's much smaller than gradient Signal from close-by.
So, model weights are updated only with respect to near effects, not long-term effects.

![[10)-20241118164417586.png]]
ha bisogno di un intervento architetturale per essere risolto: anzichè riscrivere lo stato corrente tenendo conto dell'intera lunghezza, aggiorniamo lo stato rispetto a un contesto più breve ma tenendo separatamente una sorta di buffer che ci dice quanto usare dal contesto precedente nella generazione delle nuove parole


# Long Short-Term Memory

A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of
vanishing gradients
• Everyone Cites that paper but really a crucial part of the modern LSTM is from Gers et al. (2000)
Only started to be recognized as promising through the work of S's student Alex Graves c. 2006
• Work in which he also invented CTC (connectionist temporal classification) for speech recognition
But only really became well-known after Hinton brought it to Google in 2013
• Following Graves having been a postdoc with Hinton


vogliamo riprogettare una rnn con una sorta di memoria, per sistemare la parte iniziale che danneggia la back propagation
introduciamo la notazione c, che sta per cella di memoria, serve per gestire l'informazione a lungo termine
abilitiamo delle informazioni di lettura scrittura e cancellazione
la selezione di quale informazione deve essere gestita è controllata da determinati gates
vettori della stessa dimensionalità, ad ogni timestep il vettore dei gates sarà o aperto o chiuso
i loro valori sono dinamici e cambiano a seconda di input e contesto

![[10)-20241118164929500.png]]
partendo dal basso, bogliamo calcolare gli hidden h^{(t)} e le celle c^{(t)}
h^{(t)} è una combinazioone element wise tra l'attivazione dello stato della cella (tangente iperbloica) per o^{(t)}, che è l output gate(filtro), controlla quali parte della cella di memoria vanno a contribuire allo stato hidden al passo t

c^t è la combinazione tra c al passo precedente per lo stato al passo corrente di quella che il nuovo contenuto da inserire in memoria
la combinazioneè controllata da due gate f(forget) e i(input)
lo stato di memoria al passo t è la combinazione tra una parte dello stato di memoria al passo precedente e la combinazione del nuovo contenuto, determinato dal nostro input trasformato combinandolo linearmente con l'hidden state al passo precedente
il risultato è $\tilde{c}^{(t)}$
