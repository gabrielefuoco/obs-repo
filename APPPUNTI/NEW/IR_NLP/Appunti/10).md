## Language Modeling

Il *language modeling* è un task autoregressivo che si concentra sulla generazione di testo.  L'input consiste in una sequenza di parole osservate, $x_1, ..., x_t$ (dove *t* rappresenta il time step). Il task consiste nel predire la parola successiva, $x_{t+1}$.

Si assume che il vocabolario sia noto a priori e che il generatore campioni da esso secondo specifiche strategie.

![[10)-20241118151227953.png|552]]

La probabilità di generare un testo T può essere vista come il prodotto delle probabilità condizionate di osservare ogni parola, data la sequenza di parole precedenti:

![[10)-20241118151329195.png|585]]

Il language modeling è importante non solo per la semplice predizione della parola successiva, ma anche per una vasta gamma di applicazioni nel campo del linguaggio naturale, tra cui:

* **Machine Translation:**  Può essere considerato un caso particolare di language modeling, in quanto implica una logica di encoding nel linguaggio sorgente e decoding nel linguaggio target.
* **Speech Recognition:**  La predizione di parole successive è fondamentale per la trascrizione accurata del parlato.
* **Spelling/Grammar Correction:**  Il modello può identificare e correggere errori ortografici e grammaticali.
* **Summarization:**
    * **Estrattiva:** Evidenzia le frasi più importanti da un testo.
    * **Astrattiva:** Rimodula il testo originale creando un riassunto.  Anche la summarization astrattiva può essere considerata un caso particolare di language modeling, poiché, data una sequenza di testo in input, genera una nuova sequenza di testo in output.


## n-gram Language Models

n gram: chunck, porzione di testo, fatta da n token consecutivi
collezioniamo statistiche di occorrenza di n gramm e li usiamo per stimare la probabilità della parola successiva.

unimgrams: "the", "students", "opened", "their"
bigrams: "the students", "students opened", "opened their"
trigrams: "the students opened", "students opened their"
four-grams: "the students opened their"

invece di guardare a ritroso fino all'inizio del testo, usiamo solo una finestra di n-1 parole
voglaimo semplificare il problema iniziale


First we make a Markov assumption: $x^{(t+1)}$ depends only on the preceding n-1 words
![[10)-20241118152157750.png]]

Question: How do we get these n-gram and (n-1)-gram probabilities?
Answer: By counting them in some large corpus Of text!
![[10)-20241118152225539.png]]

![[10)-20241118152314916.png]]


![[10)-20241118152452827.png]]

- problema 1: numeratore = 0
- problema 2: denominatore = 0
	- valutiamo l'n-2 gram, aumentiamo la finestra ma non garantisce il successo

usare un n relativamente grande aumenta la model size e inoltre non è garanzia di successo: 
- costruire un modello del genere porta a problemi di sparsità e granularità: più è grande n e più aumentiamo la sparsità e diminuiamo la granularità, dunque aumentiamo i lrischio di avere un piattume nelle probabilità
- se implementassimo una cosa del genere con un'opportuna dimensione del testo, potremmo avere un pezzo di testo che dal punto di vista grammaticale non ha problemi ma che pecca di coerenza (non è "fluido")
