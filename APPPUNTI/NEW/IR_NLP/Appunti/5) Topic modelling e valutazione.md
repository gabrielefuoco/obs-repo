## Librerie di riferimento pre-LLM

* **NLTK (Natural Language Toolkit):** Una libreria Python completa per l'elaborazione del linguaggio naturale, che offre una vasta gamma di strumenti per l'analisi del testo, tra cui tokenizzazione, stemming, lemmatizzazione, analisi morfologica, analisi sintattica e classificazione del testo.

* **SpaCy:** Una libreria Python per l'elaborazione del linguaggio naturale, progettata per la velocit√† e l'efficienza. Offre funzionalit√† avanzate per l'analisi del testo, come il riconoscimento di entit√† denominate (NER), l'estrazione di entit√†, l'analisi del sentimento e la classificazione del testo. √à utilizzata anche a livello commerciale per queste attivit√†.

* **Gensim:** Una libreria Python per l'analisi di argomenti e la modellazione di argomenti. Fornisce implementazioni di modelli di argomenti stocastici, come LDA (*Latent Dirichlet Allocation*), che possono essere utilizzati per scoprire argomenti latenti in grandi set di dati di testo.

## Latent Dirichlet Allocation 

Latent Dirichlet Allocation (LDA) √® un modello di topic modeling di riferimento. Esistono diverse tecniche matematicamente eleganti che si basano su LDA, per coprire oltre ai tre aspetti di base anche aspetti riguardo il ruolo di autori o categorie speciali per i documenti.

LDA pu√≤ essere utilizzato per modellare, oltre ai topic, anche un profilo di autore in maniera supervisionata o semi supervisionata (ad esempio, identificare autori fake o meno, esistenti o meno). Queste estensioni complicano il modello da un punto di vista computazionale, ma hanno avuto un discreto successo (in un'era precedente ai Large Language Models).

Si sfrutta una propriet√† importante: **coniugate prior tra le distribuzioni** (tra Dirichlet e polinomiale).

![[Repo/APPPUNTI/NEW/IR_NLP/Appunti/Allegati/5)-20241031095017801.png]]
$\alpha$ e $\eta$ sono parametri di dispersione che influenzano la distribuzione di probabilit√† sui topic e sulle parole. $\alpha$ controlla la dispersione della distribuzione di probabilit√† sui topic, mentre $\eta$ controlla la dispersione della distribuzione di probabilit√† sulle parole.

Ci√≤ che importa √® mantenere la differenza in ordine di grandezza, con l'obiettivo di essere pi√π inclusivi nel modellare la distribuzione di probabilit√† per topic (grana fine nel modellare ogni topic), e meno dispersivi per modellare la distribuzione di probabilit√† sui topic per ogni documento (assumiamo che ogni documento contenga dati caratteristici di pochi topic). 

### Pseudo

$$\text{For each topic, generate a Dirichlet distribution over terms:}$$
$$\beta_k \sim \text{Dir}_M(\eta), k \in \{1, \ldots, K\}$$

$$\text{For each document } d_i, i \in \{1, \ldots, N\}$$
- $\text{Generate a Dirichlet distribution over topics: } \theta_i \sim \text{Dir}_K(\alpha)$
- $\text{For each word position } j \text{ in document } d_i:$
 - $\text{Choose a topic } z_{ij} \text{ from the distribution in step a., i.e., } z_{ij} \sim \text{Multi}(\theta_i)$
 - $\text{Choose word } w_{ij} \text{ from topic } z_{ij} \text{, i.e., } w_{ij} \sim \text{Multi}(\beta_{z_{ij}})$

# Modellazione di documenti segmentati per argomento

## Modello Generativo Basato su Segmenti [Ponti, Tagarelli, Karypis, 2011]

Questo modello rappresenta un approccio pi√π granulare alla modellazione argomento-documento, basato sulla segmentazione del testo in unit√† pi√π piccole. 

##### Traduzione in un modello probabilistico congiunto per dati triadici:

Il modello si basa su un'idea chiave: la traduzione della relazione tra argomenti, documenti e segmenti in un modello probabilistico congiunto. Questo modello considera la probabilit√† di un documento, dato un argomento e un segmento, e viceversa.

##### Motivazione:

Per documenti lunghi, assegnare un solo argomento a un intero documento pu√≤ essere riduttivo. La struttura semantica complessa di un documento lungo potrebbe richiedere una rappresentazione pi√π fine, che tenga conto della presenza di diversi segnali informativi in diverse parti del documento.

##### Approcci alternativi:

* **Soft clustering:** Invece di assegnare un documento a un solo cluster (hard clustering), il soft clustering permette a un documento di appartenere a pi√π cluster con una certa probabilit√†.
	* Tuttavia, se i documenti hanno una struttura logica, esplicita o implicita, con una semantica complessa, ci saranno segnali informativi caratterizzanti nell'introduzione e nella conclusione. Questo pu√≤ portare al **topic drift**, ovvero alla deriva tematica.
* **Segmentazione:** La segmentazione del documento in unit√† pi√π piccole (segmenti) permette di catturare la struttura semantica fine del documento e di associare diversi argomenti a diversi segmenti.

##### Overclustering:

Per catturare la struttura dei micro-topic, si pu√≤ utilizzare l'overclustering, ovvero stimare un numero di cluster (k) molto pi√π alto del numero effettivo di argomenti. Questo approccio √® stato utilizzato in precedenza con modelli vector space e bag of words, dove la segmentazione veniva applicata prima del clustering dei segmenti e poi dei documenti.

##### Integrazione della segmentazione nei topic model:

L'obiettivo di questo lavoro √® di integrare la segmentazione del testo nei topic model, creando un modello a grana pi√π fine che tenga conto della variabile dei segmenti. 

![[Repo/APPPUNTI/NEW/IR_NLP/Appunti/Allegati/5)-20241031100023606.png|394]]

Questa immagine rappresenta una rete probabilistica, spesso usata nei modelli di generazione del linguaggio, in cui:
- **D** rappresenta i documenti,
- **Z** rappresenta i topic,
- **S** rappresenta i segmenti del documento (o potenzialmente frasi/sottosezioni),
- **V** rappresenta il vocabolario (le parole).

Il modello assume che, dato un documento, si scelga un topic e, condizionatamente a questo e al segmento, si campiona una parola. Questo processo di campionamento riflette la relazione tra i topic, i segmenti e le parole all'interno del testo.

Il joint model diventa triadico, con le variabili latenti che rimangono i topic. √à pi√π un'estensione di PLSA, evitando la complessit√† di RDA. Non rappresenta in maniera esplicita la relazione tra argomenti, documenti e segmenti, ma migliora la modellazione dei topic con una grana pi√π fine. 

I processi rappresentati dalle frecce indicano che:

1. **Dato un documento \(D\), scegliamo un topic \(Z\) secondo la probabilit√† condizionata $\text{Pr}(z|d)$.**
2. **Dato il topic \(Z\), selezioniamo un segmento \(S\) con probabilit√† $\text{Pr}(s|z)$ .**
3. **Dato il topic \(Z\), possiamo anche scegliere una parola \(W\) con probabilit√† $\text{Pr}(w|z)$.**
4. **Dato un segmento \(S\), scegliamo una parola \(W\) con probabilit√† $\text{Pr}(w|s)$ .**

Il campionamento in questo contesto viene effettuato iterativamente o ripetutamente per generare contenuti a partire dai documenti, utilizzando la struttura probabilistica del modello. 

## Segmentazione del Testo

La segmentazione del testo √® un processo fondamentale per la comprensione e l'analisi di documenti. L'obiettivo √® suddividere il testo in unit√† significative, chiamate segmenti, che rappresentano concetti o temi distinti.

Esistono diversi approcci alla segmentazione del testo:

* **Soluzione Naive:** Segmentare il testo in base ai paragrafi. Questo metodo √® semplice ma spesso non √® accurato, poich√© un paragrafo pu√≤ contenere pi√π temi.
* **Soluzione Intelligente:** In assenza di markup, √® necessario utilizzare tecniche di segmentazione non supervisionata. Un esempio √® il "TextTiling", che utilizza un modello vector space con tf-idf per calcolare la similarit√† tra segmenti consecutivi.

### Text Tiling

Il Text Tiling √® un metodo di segmentazione non supervisionato che utilizza un modello vettoriale per rappresentare il testo. I passaggi principali sono:

1. **Rappresentazione vettoriale:** Il testo viene rappresentato come un vettore nello spazio vettoriale.
2. **Calcolo della similarit√†:** La similarit√† tra segmenti consecutivi viene calcolata utilizzando la similarit√† coseno.
3. **Curva delle similarit√†:** Viene tracciata una curva che rappresenta la similarit√† tra i segmenti.
4. **Identificazione dei punti di discontinuit√†:** I minimi locali della curva indicano un cambiamento di topic.
5. **Segmentazione:** Il testo viene segmentato in corrispondenza dei minimi locali.

La curva delle similarit√† mostra massimi locali che rappresentano una continuit√† di topic. I minimi locali, invece, indicano un cambiamento di topic. L'approccio del Text Tiling prevede di spezzare il testo in corrispondenza dei minimi locali, ottenendo cos√¨ segmenti che rappresentano temi distinti.

### Modello di argomenti latenti (LDA) per il Text Tiling

Il modello LDA pu√≤ essere utilizzato per il Text Tiling, considerando la probabilit√† di un documento, un segmento e una parola come segue:

$$ \operatorname{Pr}(d , s, w) = \operatorname{Pr}(d) \sum_{z \in Z} \operatorname{Pr}(z \mid d) \operatorname{Pr}(s \mid z) \operatorname{Pr}(w \mid z, s) $$

dove:

* $d$ √® il documento.
* $s$ √® il segmento.
* $w$ √® la parola.
* $z$ √® l'argomento.
* $Z$ √® l'insieme di tutti gli argomenti.

Il processo di generazione di un documento, un segmento e una parola pu√≤ essere descritto come segue:

$$
\begin{align}
&\text{Select a document } d \text{ from } \mathcal{D} \Rightarrow \operatorname{Pr}(d) \\
&\text{For each segment } s \in S_d: \\
&\text{1. Choose a topic } z \text{ for the document } d \Rightarrow \operatorname{Pr}(z \mid d) \\
&\text{2. Associate topic-to-segment probability to the segment } s \text{ for the selected topic } z \Rightarrow \operatorname{Pr}(s \mid z) \\
&\text{3. For each word } w \text{ in the segment } s: \\
&\text{Choose a word } w \text{ from the current topic and segment } \Rightarrow \operatorname{Pr}(w \mid z, s)
\end{align}
$$

### Algoritmo di inferenza EM per LDA

L'algoritmo di inferenza EM (Expectation-Maximization) per LDA prevede due passaggi:

* **E-step:** Calcolo della probabilit√† a posteriori dell'argomento dato il documento, il segmento e la parola:

$$ \begin{aligned} \text{E-step} \quad \Pr(z|d, s, w) &= \frac{\Pr(z, d, s, w)}{\Pr(d, s, w)} = \frac{\Pr(z|d)\Pr(s|z)\Pr(w|z, s)}{\sum_{z \in Z} \Pr(z|d)\Pr(s|z)\Pr(w|z, s)} \\ 
\end{aligned} $$

* **M-step:** Aggiornamento delle probabilit√† di argomento, segmento e parola:

$$
\begin{aligned}
\text{M-step} \quad \mathbf{E}[l] &= \sum_{d \in D} \sum_{s \in S} \sum_{w \in V} n(d, s, w) \times \sum_{z \in Z} \Pr(z|d, s, w) \log(\Pr(d, s, w)) \ 
\\
\text{Update formulas} \quad \Pr(z|d) &\propto \sum_{s \in S} \sum_{w \in V} n(d, s, w) \Pr(z|d, s, w) \ 
\\
\Pr(s|z) &\propto \sum_{d \in D} \sum_{w \in V} n(d, s, w) \Pr(z|d, s, w) \ 
\\
\Pr(w|z, s) &\propto \sum_{d \in D} n(d, s, w) \Pr(z|d, s, w) \end{aligned} 
$$

dove:

* $n(d, s, w)$ √® il numero di volte in cui la parola $w$ appare nel segmento $s$ del documento $d$.
* $V$ √® il vocabolario.

L'algoritmo EM viene ripetuto fino a convergenza, ovvero fino a quando le probabilit√† non cambiano significativamente tra le iterazioni.

### Applicazione del Text Tiling con LDA

Il Text Tiling con LDA pu√≤ essere utilizzato per segmentare un documento in base ai suoi argomenti. L'algoritmo EM viene utilizzato per inferire le probabilit√† di argomento, segmento e parola. I segmenti vengono quindi identificati in base ai cambiamenti di argomento, come evidenziato dai minimi locali nella curva delle similarit√†.

# Valutazione dei modelli di argomenti

I modelli di topic, come Latent Dirichlet Allocation (LDA) e Probabilistic Latent Semantic Analysis (PLSA), cercano di scoprire temi latenti all'interno di un corpus di documenti. Questi modelli si basano sull'idea che ogni documento sia una miscela di topic e che ogni topic sia caratterizzato da una distribuzione di parole.

A differenza dei modelli linguistici neurali, la valutazione dei modelli di topic tradizionalmente non si concentra su aspetti come verbosit√†, fluency, affettivit√†, confabulazione o bias. Storicamente, si misurano **coerenza** e **perplessit√†**.

## Coerenza

Gli argomenti sono rappresentati dalle prime N parole con la probabilit√† pi√π alta di appartenere a quell'argomento specifico. Questo N √® relativamente grande rispetto al numero di topic che caratterizzano un documento.

La coerenza misura la co-occorrenza tra parole e argomenti: maggiore √® il punteggio, migliore √® la coerenza. Gli incrementi diminuiscono all'aumentare del numero di argomenti.

#### Punteggio di coerenza UMass

Per ogni argomento, il punteggio medio a coppie sulle prime N parole che descrivono quell'argomento √® calcolato come segue:

$$C_{\text{UMass}}(w_{i},w_{j})=\log \frac{D(w_{i},w_{j})+1}{D(w_{i})}$$

Dove:

* $D(w_{i}, w_{j})$: numero di volte in cui le parole $w_{i}$ e $w_{j}$ compaiono insieme nei documenti.
* $D(w_{i})$: numero di volte in cui la parola $w_{i}$ compare da sola.

#### Punteggio di coerenza UCI

Il punteggio di coerenza UCI si basa su finestre scorrevoli e sul PMI (Probabilit√† di Informazione Mutua) di tutte le coppie di parole utilizzando le prime N parole per occorrenza.

* La co-occorrenza delle parole √® vincolata a una *finestra scorrevole* e non all'intero documento.
$$C_{\text{UCI}}(w_{i},w_{j})=\log \frac{P(w_{i},w_{j})+1}{P(w_{i})\cdot P(w_{j})}$$

Dove:

* $P(w_{i}, w_{j})$: probabilit√† di vedere $w_{i}$ e $w_{j}$ insieme in una finestra scorrevole.
* $P(w_{i})$: probabilit√† di vedere $w_{i}$ in una finestra scorrevole.
 * Entrambe le probabilit√† sono stimate dall'intero corpus di oltre due milioni di articoli di Wikipedia in inglese utilizzando una finestra scorrevole di 10 parole.
- √à un approccio robusto ma parametrico: i parametri sono 2: stride e overlap

#### Coerenza basata sulla similarit√† intra/inter-argomento

Per ogni coppia di argomenti, la coerenza √® calcolata come la media della similarit√† intra-argomento divisa per la similarit√† inter-argomento.

* **Similarit√† intra-argomento:** media della similarit√† tra ogni possibile coppia di parole principali in quell'argomento.
* **Similarit√† inter-argomento:** media della similarit√† tra le parole principali di due argomenti diversi.
* Si presume che le embedding delle parole siano disponibili.
* Il coseno √® utilizzato come misura di similarit√†. 

### Coerenza e Separazione dei Topic

Se due topic hanno distribuzioni di probabilit√† simili, significa che attivano pi√π o meno gli stessi termini con la stessa probabilit√†. Questa osservazione suggerisce che la valutazione dei topic non dovrebbe limitarsi alla coerenza interna (compattezza) dei cluster, ma dovrebbe anche considerare la loro **separazione**. 

### Entropia e Cross-Entropia

Un **topic**, o **classe**, √® un concetto che introduciamo per raggruppare attributi. Per valutare il grado di incertezza associato a un topic, utilizziamo l'**entropia** della sua distribuzione di probabilit√†. In alcuni casi, potremmo anche considerare la **cross-entropia**, che misura l'incertezza di una distribuzione rispetto a un'altra.

## Distanza tra distribuzioni

### Divergenza di Kullback-Leibler

La divergenza di Kullback-Leibler (da Q a P), nota anche come entropia relativa di P rispetto a Q, √® definita come:

$$D_{KL}(P\|Q) = -\sum_{x\in X}P(x)\log\left( \frac{Q(x)}{P(x)} \right)$$

La divergenza di Kullback-Leibler presenta le seguenti propriet√†:

* **Non negativit√†:** $D_{KL}(P\|Q) \ge 0$, con uguaglianza se e solo se P = Q.
* **Asimmetria:** $D_{KL}(P\|Q) \neq D_{KL}(Q\|P)$.
* **Non metrica:** Non soddisfa la disuguaglianza triangolare.

### Divergenza di Jensen-Shannon

La divergenza di Jensen-Shannon √® definita come:

$$D_{JS} = \frac{1}{2}D_{KL}(P\|M) + \frac{1}{2}D_{KL}(Q\|M)$$

dove M √® la media di P e Q, ovvero:

$M = \frac{1}{2}(P + Q)$.

### Cross-entropia

Quando si confrontano due distribuzioni di probabilit√†, si utilizza spesso la cross-entropia, che pu√≤ essere relativizzata o normalizzata. La cross-entropia misura la differenza tra la distribuzione di probabilit√† prevista e la distribuzione di probabilit√† effettiva. 

## Perplessit√†

La perplessit√† √® una misura del grado di incertezza di un modello linguistico (LM) quando genera un nuovo token, mediato su sequenze molto lunghe. 
In termini di processi generativi, indica l'incertezza del modello nello scegliere la prossima parola da campionare durante la generazione del documento. **Minore √® la perplexity, migliore √® il modello.**

**Nota:** la perplessit√† dipende dalla specifica tokenizzazione utilizzata dal modello, quindi il confronto tra due LM ha senso solo se entrambi utilizzano la stessa tokenizzazione.

La perplessit√† √® la prima misura da utilizzare per valutare le prestazioni di un modello linguistico.

### Comprendere la Perplexity

La formula della perplexity √® una funzione esponenziale di una media normalizzata di logaritmi di probabilit√†. In sostanza, √® una funzione esponenziale dell'entropia. 

Per comprendere meglio il concetto, √® utile richiamare alcuni principi fondamentali:

* **Variabile 'x':** Rappresenta la sorgente informativa (es. una registrazione universitaria). I valori di 'x', indicati con x·µ¢, sono le parole, i token.
* **Entropia di 'x':** Misura il tasso di informazione trasmesso dalla sorgente. Il limite superiore per la codifica dei token √® rappresentato dall'entropia. Questo limite √® massimo quando la distribuzione degli eventi √® uniforme (tutti i possibili risultati hanno la stessa probabilit√†).
* **Token frequenti:** Gli eventi pi√π probabili, che minimizzano la lunghezza della descrizione.
* **Componente informativa:** La componente informativa della variabile che modella la frequenza dei token, P(x), √® -log P(x).
* **Perplexity:** Una variabile aleatoria uguale a 2 elevato all'entropia di x, H(x).

### Definizione

Sia X una fonte di informazioni testuali (variabile aleatoria sorgente), e i suoi valori x siano token.

Ricordiamo: l'entropia di X √® una misura del tasso di informazioni prodotte da X. L'entropia √® massima quando √® possibile osservare tutti gli esiti della variabile aleatoria. √à indicata con $H[X]$.

* **Teorema di Codifica Senza Rumore di Shannon:** il limite inferiore per la lunghezza attesa del codice di codifica dei token.
* per ogni token, la sua lunghezza di codifica √® $-log(p(x))$.
	* ovvero, i token frequenti dovrebbero essere assegnati a codici pi√π brevi: minimizzano la lunghezza della descrizione.

##### Perplessit√† di una singola variabile casuale X

$$PP[X]:=2^{H[X]}$$
### Perplessit√† di un processo stocastico

Siamo interessati a un processo stocastico œá di sequenze **non i.i.d.** (indipendenti e identicamente distribuite) di variabili casuali (X‚ÇÅ, X‚ÇÇ, ‚Ä¶).  Le occorrenze di parole all'interno di un testo non sono indipendenti.

#### Prima ipotesi: Stazionariet√†

La probabilit√† di osservare una parola non cambia a seconda della finestra di testo considerata.  Questa ipotesi non √® vera per un documento di testo, poich√© le parole sono distribuite in modo diverso all'inizio e alla fine di un testo. Tuttavia, ci√≤ implica che il limite dell'entropia media per token coincide con il limite dell'entropia media dell'ultimo token:

* $\lim_{ n \to \infty } \frac{1}{n}H[X_{1},\dots,X_{n}]$
* $\lim_{ n \to \infty } \frac{1}{n}H[X_{n}|X_{1},\dots,X_{n-1}]$


#### Seconda ipotesi: Ergodicit√†

Se il numero di osservazioni di una variabile √® molto grande, il valore atteso coincide con la media delle misurazioni.  Inoltre, per sequenze molto lunghe, la media dei logaritmi negativi delle probabilit√† sui vari step di generazione approssima l'entropia. L'ergodicit√† garantisce che l'aspettativa $ùîº[X‚ÇÅ]$ di qualsiasi singola variabile casuale X‚ÇÅ sulla distribuzione P del processo œá pu√≤ essere sostituita con la media temporale di una singola sequenza molto lunga (x‚ÇÅ, x‚ÇÇ, ‚Ä¶) estratta da œá (Teorema Ergodico di Birkhoff):

* $\frac{1}{n}\sum_{i=1}^n X_{i}\to_{n \to \infty} E_{p}[X_{1}]\text{ con probabilit√† 1}$

Questo risultato, insieme al Teorema di Shannon-McMillan-Breiman, implica:

* $-\frac{1}{n}\log p(X_{1},\dots,X_{n})\to_{n\to \infty }H[X]\text{ con probabilit√† 1}$


#### Applicazione all'entropia

Il risultato precedente sarebbe perfetto se conoscessimo le distribuzioni di probabilit√† p(x‚ÇÅ, x‚ÇÇ, ‚Ä¶).  Tuttavia, non le conosciamo, quindi ricorriamo a un modello linguistico q(x‚ÇÅ, x‚ÇÇ, ‚Ä¶) come approssimazione. Un limite superiore al tasso di entropia per p √® la cross-entropia del modello Q (il modello linguistico) rispetto alla sorgente P (il linguaggio naturale):

* $CE[P,Q]:=\lim_{ n \to \infty }-E_{p}\log q(X_{n}|X_{<n})=\lim_{ n \to \infty } -\frac{1}{n}E_{p}\log q(X_{1},\dots,X_{n})$


### Perplessit√† di un modello Q per un linguaggio considerato come una sorgente P sconosciuta:

La *perplexity* del modello linguistico Q rispetto al linguaggio naturale √® 2 elevato alla cross-entropia. In pratica, calcoliamo 2 elevato alla media, per ogni parola in ogni documento, della distribuzione di probabilit√† delle parole in esso contenute, indicata con w,d. N,d √® il numero di parole nel documento d, quindi normalizziamo i contributi di ciascuna parola. Sommiamo su tutti i documenti e poi eleviamo a 2. Questo √® il valore della *perplexity*. 
Formalmente:

$$PP[P,Q]:=2^{CE[P,Q]}$$
dove
$$-\frac{1}{n}\log q(X_{1},\dots,X_{n})\to_{n\to \infty}CE[P,Q]$$

### Perplessit√† di un campione di holdout:

$$
-\frac{\sum_{d=1}^M\log p(w_{d})}{\sum_{d=1}^MN_{d}}
$$
* M √® il numero di documenti nel campione di test.
* $w_{d}$ rappresenta le parole nel documento d.
* $N_{d}$ il numero di parole nel documento d.

### In LDA: 

$$\log p(w|\alpha,\beta)=E[\log p(\theta,z,w|\alpha,\beta)]-E[\log q(\theta,z)]$$
Nel caso di LDA, valutiamo alpha e beta, i parametri di dispersione. Questa forma deriva dall'utilizzo della variational inference, una tecnica pi√π accurata rispetto al Gibbs sampling.

### In SGM:

$$Pr(d,S_{d},V)=Pr(d)\prod_{S\in S_{d}}\sum_{z\in Z}Pr(z|d)Pr(s|z)\prod_{w\in V}Pr(w|z,s)$$
Anche in SGM, il modello LDA adattato a stream di dati, la *perplexity* si basa sul calcolo di probabilit√† simili, valutate per ogni documento, utilizzando i valori calcolati durante la fase di test.
