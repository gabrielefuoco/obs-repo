## Compressione

La compressione dei dati è fondamentale in Information Retrieval per diversi motivi:

* **Ridurre lo spazio di archiviazione su disco:** Minore spazio occupato significa costi di storage ridotti.
* **Aumentare la capacità di memoria:** Permette di mantenere in memoria principale una maggiore quantità di dati.
* **Accelerare il trasferimento dati:** Il trasferimento di dati compressi da disco a memoria è più veloce.
* **Migliorare le prestazioni di lettura:** La lettura e decompressione di dati compressi può essere più veloce rispetto alla lettura di dati non compressi, a patto di utilizzare algoritmi di compressione e decompressione efficienti.

**Nota:** Gli algoritmi di decompressione utilizzati nei sistemi di Information Retrieval sono generalmente molto veloci.

##### Compressione del Dizionario e del File di Posting

Si distinguono la compressione del dizionario, cruciale per mantenere in memoria il dizionario e alcune liste di postings, e la compressione dei file di postings, importante per ridurre lo spazio su disco e accelerare la lettura

### Compressione Senza Perdita vs. Con Perdita

Si usa prevalentemente la compressione *senza perdita*, mentre la compressione *con perdita* (es. pre-elaborazione del testo, poda delle liste di postings) è utilizzata con cautela, introducendo una minima perdita di qualità.

## Dimensione del vocabolario vs. dimensione della collezione

La *dimensione del vocabolario* (M, numero di parole distinte) cresce con la *dimensione della collezione* (T, numero di token). Non esiste un limite superiore definito, soprattutto con Unicode

### Legge di Heaps: $M = kT^b$

* **M** è la dimensione del vocabolario (cresce seguento una **power law**: *funzione lineare in scala doppia logaritmica con offset k),* **T** è il numero di token nella collezione.
* Valori tipici: $30 ≤ k ≤ 100$ e $b ≈ 0.5$.
* In un grafico log-log della dimensione del vocabolario M vs. T, la legge di Heaps prevede una linea con pendenza di circa ½.
* È la relazione più semplice possibile (lineare) tra i due in spazio log-log.
* $log M = log k + b log T$.

### Legge di Heaps per Reuters RCV1:

$$log_{10}M = 0.49 \log_{10}T + 1.64
\to
M = 10^{1.64}T^{0.49}$$
cioè:
- $k=10^{1.64}≈ 44$
- $b = 0.49$.

Buona aderenza empirica per Reuters RCV1:

* Per i primi 1.000.020 token, prevede 38.323 termini;

![[1) Intro-20241007160038284.png|371]]

Vi è una fase transitoria iniziale e poi inizia a fittare a regime

## Distribuzione Skewd di Tipo Power-law

La distribuzione skewd di tipo power-law è caratterizzata da una concentrazione di massa in una zona relativamente piccola della distribuzione, seguita da una coda lunga o grassa.

##### Caratteristiche:

* **Concentrazione di massa:** La maggior parte della massa è concentrata in una piccola porzione della distribuzione.
* **Coda lunga:** La distribuzione presenta una coda che si estende per un lungo periodo, con valori che diminuiscono lentamente.

**Nota:** La distribuzione skewd di tipo power-law è spesso osservata in fenomeni naturali e sociali.

### Distribuzione di tipo Power-law

La **distribuzione di tipo power law** è un modello matematico che descrive la distribuzione di molti fenomeni naturali e sociali, come la dimensione delle città, la frequenza delle parole in un linguaggio e la ricchezza delle persone. È caratterizzata da una concentrazione di massa in una zona relativamente piccola della distribuzione, seguita da una coda lunga o grassa(simile alla funzione esponenziale). In altre parole, pochi elementi hanno un valore molto alto, mentre molti altri hanno un valore molto basso.

### Legge di Pareto

Un esempio di questo modello è la **legge di Pareto**, nota anche come principio 80-20, che afferma che l'80% degli effetti deriva dal 20% delle cause. Nel contesto del linguaggio naturale, la legge di Zipf è un esempio di distribuzione di tipo power law, dove pochi termini sono molto frequenti, mentre molti altri sono molto rari.

##### Esempi di distribuzione di tipo power law:

* **Distribuzione della ricchezza tra individui:** Pochi individui possiedono la maggior parte della ricchezza, mentre molti altri hanno una ricchezza molto bassa.
* **Numero di pagine di siti web:** Pochi siti web hanno un numero molto elevato di pagine, mentre molti altri hanno un numero di pagine molto basso.
* **Numero di follower di un social network:** Pochi utenti hanno un numero molto elevato di follower, mentre molti altri hanno un numero di follower molto basso.
* **Dimensione delle città:** Poche città hanno una popolazione molto elevata, mentre molte altre hanno una popolazione molto bassa.
- **Frequenza delle parole in un documento**

##### Confronto tra la distribuzione di Poisson (legge degli eventi rari) con la legge di potenza:

Entrambe le distribuzioni sono asimmetriche (skewed), ma si differenziano per la loro natura:
La scelta tra le due distribuzioni dipende da come si modella il tasso minimo (min rate) nella distribuzione di Poisson.
- **Poisson:** È una distribuzione spaziale, che descrive la probabilità di un certo numero di eventi in un dato intervallo di tempo o spazio.
- **Legge di potenza:** È una distribuzione che descrive la relazione tra due variabili, dove una varia in modo proporzionale a una potenza dell'altra.

## Legge di Heaps e Legge di Zipf

La legge di Heaps (**prima legge di potenza**) fornisce una stima della dimensione del vocabolario in un corpus di testo. Tuttavia, nel linguaggio naturale, si osserva una distribuzione non uniforme delle parole: alcuni termini sono molto frequenti, mentre molti altri sono molto rari.

### Legge di Zipf (seconda legge di potenza)

Zipf (1949) ha scoperto una relazione empirica tra la *frequenza* di un termine e il suo *rango* nel vocabolario.
Emerge che vi siano termini più frequenti di altri, che sono in minoranza rispetto agli atlri
La legge di Zipf afferma che l' i-esimo termine più frequente ha una frequenza di collezione proporzionale a $\frac{1}{i}$:

$$cf_{i} \propto \frac{1}{i} = \frac{K}{i}$$

dove $cf_{i}$ è la frequenza del termine i-esimo, $i$ è il suo rango nel vocabolario(posizione in una classifica stabilita su una lista di frequenze) e $K$ è una costante di normalizzazione.

In forma logaritmica, la legge di Zipf si esprime come:

$$log(cf_{i}) = log(K) - log(i)$$

Questa equazione indica una relazione lineare inversa tra il logaritmo della frequenza del termine e il logaritmo del suo rango. Questa relazione è nota come legge di potenza.
È dunque una power law con slope negativa

##### Esempio:

Se il termine più frequente ("the") si verifica $cf1$ volte, allora il secondo termine più frequente ("of") si verifica $cf1/2$ volte, il terzo termine più frequente ("and") si verifica $cf1/3$ volte, e così via.

![[1) Intro-20241007160144378.png|392]]
- In posizioni di rank basse (origine) abbiamo alta frequenza.
- La frequenza scende linearmente con scala doppia logaritmica

## La legge di Zipf: le implicazioni di Luhn

Luhn (1958) osservò che:

* **Le parole estremamente comuni non sono molto utili per l'indicizzazione.** Questo perché sono troppo generiche e non forniscono informazioni specifiche sul contenuto di un documento.
* **Le parole estremamente rare non sono molto utili per l'indicizzazione.** Questo perché compaiono troppo raramente per essere significative.

**I concetti più discriminanti hanno una frequenza da bassa a media.** Questo significa che le parole che compaiono con una frequenza intermedia sono le più utili per l'indicizzazione, perché forniscono informazioni specifiche sul contenuto di un documento senza essere troppo rare da essere insignificanti.

La distribuzione delle parole in un corpus segue la legge di Zipf, che afferma che la frequenza di una parola è inversamente proporzionale al suo rango. Questo significa che le parole più frequenti sono molto più comuni delle parole meno frequenti.

![[1) Intro-20241010152315456.png]]

Il grafico mostra la distribuzione delle parole in un corpus. La maggior parte delle parole ha una frequenza medio-bassa, mentre poche parole hanno una frequenza molto alta.

- **In funzione del task specifico, non è immediato stabilire quando un termine è eccessivamente frequente e allo stesso modo quando è eccessivamente raro.**

Se fossimo capaci di stabilirlo, allora avremmo stabilito le due frequenze di taglio (superiore e inferiore). Ciò che è interno a queste frequenze di taglio è ciò che andrebbe mantenuto (vocabolario) nelle fasi successive.

**Determinare la frequenza di taglio è difficile.** Oltre ad essere task-dependent, è data-driven, dipende dal linguaggio utilizzato dal corpus (dominio dei dati).

In teoria, potremmo individuare due frequenze di taglio per effettuare un pruning dell'insieme dei termini candidati a formare il vocabolario (index terms).

##### Scopo delle frequenze di taglio:

L'obiettivo è escludere:

* **Termini troppo frequenti:** presenti in molti documenti ma poco significativi per la caratterizzazione del contenuto (es. articoli, preposizioni).
* **Termini troppo rari:** presenti in pochi documenti e quindi poco utili per l'analisi generale.

Lo scopo è quello di migliorare efficienza e accuratezza.
##### Criticità:

* **Individuazione delle frequenze di taglio ottimali:** non esiste una regola universale, la scelta dipende dal task specifico e dal dominio di applicazione.
* **Dipendenza dal dominio e dal linguaggio:** la frequenza di un termine può variare significativamente a seconda del contesto.

##### Esempi di regole pratiche:

Nonostante la mancanza di regole universali, l'esperienza su diversi benchmark ha permesso di identificare alcune linee guida:
* **Rimozione dei termini troppo frequenti:** escludere termini presenti in più del 50% dei documenti.
* **Rimozione dei termini troppo rari:** escludere termini presenti in meno di 3-5 documenti.

## Ponderazione della Rilevanza dei Termini

Abbiamo a che fare con set di risultati ridotti o enormi. Quando il set di risultati deve essere processato direttamente dall'utente che ha posto la query, non è pensabile che esso possa ispezionare l'intero set. In numerosi scenari, c'è solo attenzione ai primi *k* risultati.

#### Ricerca booleana:

La ricerca booleana, basata su operatori logici, presenta limiti: restituisce spesso troppi o troppo pochi risultati, è adatta solo ad utenti esperti e non è user-friendly per la maggior parte degli utenti, soprattutto nel contesto del web.

Dobbiamo proiettarci verso un approccio più generale, che è quello del **recupero classificato**

## Ranked Retrival

Il **recupero classificato (ranked retrieval)** risolve questi problemi restituendo i documenti ordinati per pertinenza alla query.

- Il recupero classificato si basa sull'idea di esprimere quantitativamente la pertinenza di un documento rispetto alla query.
- Si parla di **recupero** e non di **estrazione di informazioni**.
- Ad esempio, se una pagina è complessa e funge anche da hub verso altre pagine, ci interessa che sia un'autorità e non un hub. In tal caso, sarebbe necessario eseguire l'estrazione di informazioni. Ci basta che la pagina abbia una percentuale di contenuti che corrispondono alla query, e in tal caso la restituiamo tra le prime.

#### Query di testo libero

Le query di testo libero sono sequenze di parole in linguaggio naturale, anziché un linguaggio di query di operatori ed espressioni.

*Il recupero classificato è normalmente stato associato alle query di testo libero, e viceversa*.

Con il recupero classificato, il problema dell' "*abbondanza o carestia*" non è più un problema.

Il sistema presenta solo i primi *k* risultati (circa 10), evitando di sovraccaricare l'utente e accettando la potenziale imprecisione insita nelle query di testo libero.

Con una query free text ammettiamo ci sia un'esigenza informativa non netta al 100%. Dunque è utile presentare i risultati all'utente con una classifica

### Punteggio come base del recupero classificato

Il recupero classificato si basa sull'assegnazione di un punteggio di pertinenza (tra 0 e 1) a ciascun documento, misurando la corrispondenza tra il documento e la query.
Questo punteggio determina l'ordine di presentazione dei risultati, mostrando prima i documenti più probabilmente utili all'utente.

Un modo è quello di usare **misure di similarità** per insiemi finiti:
* Ad esempio: *Jaccard, Sorensen-Dice, Overlap, Simple Matching, ecc.*
* Sono efficienti e forniscono la normalizzazione della lunghezza (i documenti e le query non devono avere le stesse dimensioni).
Ma non considerano la frequenza del termine $(tf)$ nel documento:
* Più frequente è il termine di query nel documento, più alto dovrebbe essere il punteggio.
* La scarsità del termine nella collezione (frequenza di menzione del documento).
* I termini rari in una collezione sono più informativi dei termini frequenti.

## Misure di Similarità

| Coefficiente | Formula | Descrizione |
| ------------------- | ---------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Jaccard** | $J(A,B) = \frac{\|A \cap B\|}{\|A \cup B\|}$ | **Misura la similarità tra due insiemi come la dimensione della loro intersezione divisa per la dimensione della loro unione**. *Varia tra 0 (nessuna similarità) e 1 (identità).* |
| **Sørensen-Dice** | $DSC(A,B) = \frac{2 \times \|A \cap B\|}{\|A\| + \|B\|}$ | **Simile a Jaccard, ma pesa doppiamente gli elementi in comune**. *Varia tra 0 (nessuna similarità) e 1 (identità).* |
| **Overlap** | $O(A,B) = \frac{\|A \cap B\|}{min(\|A\|, \|B\|)}$ | **Misura la sovrapposizione tra due insiemi come la dimensione della loro intersezione divisa per la cardinalità dell'insieme più piccolo**. *Varia tra 0 (nessuna sovrapposizione) e 1 (un insieme è un sottoinsieme dell'altro).* |
| **Simple Matching** | $SM(A,B) = \frac{\|A \cap B\| + \|\overline{A} \cap \overline{B}\|}{\|A \cup B\|}$ | **Misura la similarità tra due insiemi considerando sia le presenze che le assenze di elementi**. *Varia tra 0 (nessuna similarità) e 1 (identità).* |

La cardinalità dell'unione di due insiemi X e Y include la cardinalità della loro intersezione. Analogamente, la cardinalità di Y include la cardinalità dell'intersezione. Queste due cardinalità sono indipendenti: conoscere una non implica conoscere l'altra. La loro relazione può essere espressa tramite la formula di inclusione-esclusione (si presume che la "notazione con la J" si riferisca a questa).

Per enfatizzare gli elementi comuni e ottenere una misura di similarità meno "generosa" rispetto all'intersezione, si utilizza il coefficiente di *Dice*. Sebbene *Dice* sia sempre maggiore o uguale al coefficiente di *Jaccard*, e quindi più "generoso", può risultare utile in specifici contesti, come l'*indexing* e l'*Iris Neighbor Search*.

Per quanto riguarda le misure di distanza, la disuguaglianza triangolare è una proprietà fondamentale che definisce una metrica.

Sebbene si usino termini come "misure" o "criteri" in modo generico, il termine "metrica" è riservato a misure, come *Jaccard*, che soddisfano la disuguaglianza triangolare. *Dice*, ad esempio, non è una metrica perché non la soddisfa.

### Esempio di non soddisfacimento della disuguaglianza triangolare con Dice

Prendiamo due documenti:

* Documento 1: AB
* Documento 2: A
* Documento 3: B

Applicando la disuguaglianza triangolare, dovremmo avere:

* $Distanza(1,2) + Distanza(1,3) \geq Distanza(2,3)$

Tuttavia, usando Dice (che misura la similarità, non la distanza), otteniamo:

* $Dice(1,2) = \frac{2}{3}$
* $Dice(1,3) = \frac{2}{3}$
* $Dice(2,3) = 0$

La disuguaglianza non è rispettata, poiché 2/3 + 2/3 non è maggiore o uguale a 0.

### Overlap Coefficient

L'Overlap Coefficient, detto anche Simpson, è definito come l'intersezione divisa per la minima cardinalità tra X e Y.

$$Overlap(X,Y) = \frac{{|X ∩ Y|}}{{min(|X|, |Y|)}}$$

Calcolare il word overlap tra due porzioni di testo può essere utile, ad esempio per confrontare due summary, uno di riferimento e l'altro generato artificialmente.

#### Frequenza di Collezione (Collection Frequency)

La **frequenza di collezione** (collection frequency) rappresenta il numero totale di occorrenze di una parola all'interno di un'intera collezione di documenti. È una proprietà globale, ovvero riguarda l'intera collezione e non un singolo documento. Rappresenta la somma delle Term Frequency su tutti i documenti della collezione
- Indica quante volte un termine appare in un singolo documento.

#### Frequenza di Termine (Term Frequency)

La **frequenza di termine** (term frequency) è la controparte della frequenza di collezione, ma a livello locale.
- Indica quante volte un termine appare nell'intera collezione di documenti

#### Document Frequency

La **Document Frequency** indica quanti documenti contengono un determinato termine..

## Rappresentazione dei testi: calcolo dello score

Ogni singola cella della matrice di rappresentazione dei testi dovrà contenere un valore reale che esprime il peso del termine all'interno di un documento (quanto contribuisce a rappresentare il contenuto del documento stesso).

##### Come calcoliamo lo score da assegnare a un documento?

##### Tabella dei personaggi e delle opere

| Personaggio | Antony and Cleopatra | Julius Caesar | The Tempest | Hamlet | Othello | Macbeth |
| ----------- | -------------------- | ------------- | ----------- | ------ | ------- | ------- |
| Antony | 1 | 1 | 0 | 0 | 0 | 1 |
| Brutus | 1 | 1 | 0 | 1 | 0 | 0 |
| Caesar | 1 | 1 | 0 | 1 | 1 | 1 |
| Calpurnia | 0 | 1 | 0 | 0 | 0 | 0 |
| Cleopatra | 1 | 0 | 0 | 0 | 0 | 0 |
| mercy | 1 | 0 | 0 | 1 | 1 | 0 |
| worser | 1 | 0 | 0 | 1 | 1 | 0 |

| Personaggio | Antony and Cleopatra | Julius Caesar | The Tempest | Hamlet | Othello | Macbeth |
| --- | --- | --- | --- | --- | --- | --- |
| Antony | 157 | 73 | 0 | 0 | 0 | 1 |
| Brutus | 4 | 157 | 0 | 0 | 1 | 0 |
| Caesar | 232 | 227 | 0 | 0 | 1 | 1 |
| Calpurnia | 0 | 10 | 0 | 0 | 0 | 0 |
| Cleopatra | 57 | 0 | 0 | 0 | 0 | 0 |
| mercy | 2 | 0 | 0 | 1 | 1 | 0 |
| worser | 2 | 0 | 0 | 1 | 0 | 0 |

Esistono diverse opzioni per modellare la rappresentazione dei testi:

##### Term Frequency e Collection Frequency:

Una prima opzione consiste nell'utilizzare direttamente la *term frequency* ($\text{tf}$) e la *collection frequency* ($\text{cf}$) per calcolare il peso di un termine in un documento:

$$w_{t,d}=\text{tf}_{t,d} \frac{1}{\text{cf}_{t}}$$

dove:

* $w_{t,d}$ è il peso del termine *t* nel documento *d*
* $\text{tf}_{t,d}$ è la frequenza del termine *t* nel documento *d*
* $\text{cf}_{t}$ è la frequenza del termine *t* nell'intera collezione di documenti

Al momento, non ci preoccupiamo di normalizzare i valori.

##### Funzioni Separate per Term Frequency e Collection Frequency:

Un'altra opzione consiste nell'utilizzare due funzioni separate, *f* e *g*, per modellare l'influenza di tf e cf sul peso del termine:

$$w_{t,d}^{(t)}=f(\text{tf}_{t,d})+g(\text{cf}_{t})=\text{tf}_{t,d}+ly(\text{cf}_{t})$$

Tuttavia, in questo caso, il secondo addendo (relativo alla collection frequency) risulta troppo dominante.

#### Considerazioni sulla Lunghezza dei Documenti

Per semplicità, non facciamo assunzioni sulla lunghezza dei documenti e ci poniamo su una lunghezza media. Potrebbe esserci alternanza tra documenti lunghi e brevi.

### Smorzamento della Term Frequency

Smorzare in maniera lineare inversa la term frequency con la collection frequency appiattisce troppo verso il basso i valori.

## La Rilevanza di un Termine in un Documento

Come possiamo definire la rilevanza di un termine all'interno di un documento?

* **Tempo di Inerzia:** Un'idea potrebbe essere quella di utilizzare il tempo di inerzia del termine nel documento come indicatore di rilevanza. Tuttavia, la *term frequency* da sola non è sufficiente.

* **Importanza di CF e DF:** Anche la *collection frequency* (CF) e la *document frequency* (DF) giocano un ruolo importante. La legge di Zipf ci ricorda che termini eccessivamente popolari tendono ad essere più intensi localmente.

* **Funzione di Scoring:** Per definire la rilevanza, possiamo utilizzare una funzione che tenga conto sia di TF che di CF:

$$Rilevanza(termine) = F(TF) + G(CF)$$

* **Peso Relativo:** Se vogliamo dare maggior peso alla rilevanza locale, F dovrà crescere più rapidamente di G. Viceversa, se vogliamo dare maggior peso alla CF, G dovrà crescere più rapidamente di F.

* **TF vs. CF:** La *term frequency* sarà generalmente minore della _collection frequency_, soprattutto in corpus di grandi dimensioni.

* **Lunghezza dei Documenti:** Non facciamo alcuna assunzione sulla lunghezza dei documenti nel corpus.

* **Esempio Medico:** In un corpus di documenti medici, sia CF che TF avranno un significato rilevante.

* **Problemi con la Moltiplicazione:** Moltiplicare TF e CF risulta troppo aggressivo nello smorzamento della rilevanza.

## Coerenza con la legge di Zipf e analisi delle proposte

Ci stiamo interrogando sulla coerenza di due idee (idea 1 e idea 2) con la legge di Zipf e la sua derivata. In particolare, stiamo analizzando l'importanza dei termini in un intervallo di frequenze medio-alte e medio-basse, escludendo gli estremi.

##### Analisi delle proposte:

##### Caso 1: Termine nella testa della distribuzione

- **Proposta 1:** Il peso del termine è prossimo a 0 a prescindere dalla *term frequency* del documento, dato che la *collection frequency* è molto alta.
- **Proposta 2:** Il logaritmo della *collection frequency* è alto, ma potrebbe essere smorzato. La *term frequency* potrebbe essere alta, ma non nel caso particolare di un unico documento molto lungo. Il fattore dominante è il secondo termine (logaritmo della *collection frequency*).

##### Caso 2: Termine non nella testa della distribuzione

- **Proposta 1:** La *term frequency* è divisa per la *collection frequency*.
- **Proposta 2:** La *term frequency* è sommata al logaritmo della *collection frequency*. La *term frequency* è la stessa, ma nella prima proposta è smorzata, mentre nella seconda è enfatizzata.

##### Problemi:

- Nessuna delle due proposte sembra efficace, in quanto il comportamento varia in base al termine e alle caratteristiche del documento.
- Non è possibile fare calcoli precisi senza conoscere le caratteristiche del documento.

##### Considerazioni aggiuntive:

- È importante considerare la *document frequency* al posto della *collection frequency*?
- La *document frequency* potrebbe essere più discriminante della *collection frequency*, in quanto la sua distribuzione è potenzialmente più piatta.
- L'obiettivo è valorizzare l'importanza dei termini in un documento rispetto a una query.
- La combinazione lineare non è efficace, in quanto un termine potrebbe dominare l'altro.
- La funzione reciproca lineare è troppo aggressiva, anche con la *document frequency*.

##### Soluzione proposta:

Per smorzare in modo *smooth*, si propone di utilizzare la seguente formula:

$$\frac{1}{log(document frequency)}$$

##### Spiegazione:

- Se la *document frequency* è prossima a *n* (numero totale di documenti), significa che il termine appare in quasi tutti i documenti.
- In questo caso, il peso del termine sarà basso, in quanto poco discriminante.

## Smorzamento della Term Frequency e Inverse Document Frequency (IDF)

Lo smorzamento della frequenza del termine (TF) è cruciale nel calcolo del peso di un termine all'interno di un documento.
- Valori di TF superiori a 1 non sono problematici, dato che la frequenza massima di un termine è limitata dal numero totale di documenti (n) nel corpus.
- Lo smorzamento si ottiene dividendo la TF per il logaritmo di n (es. log₂n).
- Questo smorzamento è più accentuato per i termini rari, presenti in pochi documenti.
- Ad esempio, se un termine appare in 3 documenti, la sua TF viene divisa per log₂3.

## Funzione TF-IDF (Term Frequency - Inverse Document Frequency)

La funzione TF-IDF è una misura statistica che valuta l'importanza di un termine all'interno di un documento, in relazione ad una collezione di documenti (corpus). Combina la frequenza di un termine in un documento con la sua rarità nel corpus.

La formula per calcolare il peso TF-IDF di un termine *t* nel documento *d* è:

$$w_{t,d}=\log(1+tf_{t,d}) \times \log_{10}\left( \frac{N}{df_{t}} \right)$$

Dove:

* **tf<b><sub>t,d</sub></b>**: Frequenza del termine *t* nel documento *d*. Misura quanto frequentemente un termine appare in un documento specifico.
* **N**: Numero totale di documenti nel corpus.
* **df<b><sub>t</sub></b>**: Numero di documenti in cui il termine *t* compare. Indica in quanti documenti del corpus appare un determinato termine.

##### Interpretazione della formula:

* **log(1+tf<b><sub>t,d</sub></b>):** Rappresenta la frequenza del termine nel documento. Il logaritmo smorza l'influenza dei termini molto frequenti in un documento.
* **log<b><sub>10</sub></b>(N/df<b><sub>t</sub></b>):** Rappresenta la frequenza inversa del documento (IDF). Un valore di IDF elevato indica che il termine è raro nel corpus, quindi più informativo. Viceversa, un IDF basso indica un termine comune e poco informativo.

##### Vantaggi dell'utilizzo di TF-IDF:

* **Penalizza i termini comuni:** Termini frequenti in molti documenti (articoli, stop words) avranno un peso TF-IDF basso.
* **Evidenzia i termini rari:** Termini che compaiono in pochi documenti avranno un peso TF-IDF alto, evidenziando la loro importanza per quei documenti specifici.
* **Bilancia frequenza locale e globale:** TF-IDF considera sia la frequenza del termine in un documento specifico che la sua rarità nel corpus, fornendo una misura più accurata dell'importanza del termine.

##### Considerazioni importanti nell'utilizzo di TF-IDF:

* **Rimozione delle stop words:** È fondamentale rimuovere le stop words ("il", "la", "che", etc.) prima di calcolare TF-IDF, poiché non forniscono informazioni utili.
* **Stemming e lemmatization:** Applicare tecniche di stemming (riduzione di una parola alla sua radice) e lemmatization (riduzione di una parola al suo lemma) può migliorare la precisione del calcolo TF-IDF, raggruppando parole con lo stesso significato.
* **Soglie di taglio:** È possibile impostare soglie per escludere termini con frequenza troppo alta o troppo bassa, evitando che influenzino eccessivamente il calcolo.

##### Smorzamento e Legge di Zipf:

Lo smorzamento logaritmico nella formula TF-IDF aiuta a gestire la distribuzione dei termini descritta dalla legge di Zipf, che afferma che la frequenza di una parola è inversamente proporzionale al suo rango nella lista di frequenza. Lo smorzamento riduce l'influenza dei termini molto frequenti e aumenta l'importanza di quelli meno frequenti.

##### Vantaggi dello smorzamento:

* Evita di definire soglie di taglio arbitrarie per il vocabolario.
* Permette di lavorare con matrici TF-IDF sparse, gestendo i valori prossimi allo zero.

##### Doppio logaritmo:

In caso di corpus molto grandi, si può utilizzare un doppio logaritmo per smorzare ulteriormente il peso del fattore di frequenza del termine (TF).

##### Normalizzazione e calcolo della similarità:

La normalizzazione dei vettori TF-IDF e il calcolo della similarità tra di essi sono aspetti cruciali per utilizzare questa metrica in compiti come il recupero delle informazioni o la classificazione dei documenti.

## Modello Bag-of-words (BoW)

Il modello Bag-of-words (BoW) si basa sull'ipotesi di indipendenza dei termini.

Per capire se una feature è significativa, dovremmo misurare quantitativamente quanto è il suo potere caratterizzante e discriminante: il modello BoW ignora l'ordine delle parole, considerando solo la frequenza di ogni termine in un documento.

##### Contro:

* Informazioni sintattiche mancanti (ad esempio, struttura frasale, ordine delle parole, informazioni di prossimità).
* Informazioni semantiche mancanti (ad esempio, senso delle parole).
* Manca il controllo di un modello booleano (ad esempio, richiedere che un termine appaia in un documento).
* Data una query a due termini "A B", si potrebbe preferire un documento che contiene A frequentemente ma non B, rispetto a un documento che contiene sia A che B, ma entrambi meno frequentemente.

##### Pro:

* Fornisce una corrispondenza parziale e una misura naturale di punteggi/classifica **->** non più booleana.
* Tende a funzionare abbastanza bene nella pratica nonostante le ipotesi semplificative.
* Consente un'implementazione efficiente per grandi collezioni di documenti.
* La query diventa un vettore nello stesso spazio dei documenti **->** Modello dello spazio vettoriale.

## Tipi di frequenza

Vogliamo usare la frequenza del termine (*tf*) quando calcoliamo i punteggi di corrispondenza query-documento. Ma come?

* *La frequenza grezza del termine non è ciò che vogliamo*: Un documento con 10 occorrenze del termine è più rilevante di un documento con 1 occorrenza del termine, ma non 10 volte più rilevante.
* La rilevanza non aumenta proporzionalmente alla frequenza del termine.

### Peso di frequenza logaritmica del termine

$$w_{t,d}
\begin{cases}
1+\log_{10}\text{tf}_{td} \ \text{ if tf}_{td} \ >0 \\
0,\ \text{otherwise}
\end{cases}$$

Il peso di frequenza logaritmica del termine *t* in *d*:

* $0 → 0, \quad1 → 1,\quad 2 → 1.3,\quad 10 → 2,\quad 1000 → 4,\quad \dots$

Punteggio per una coppia documento-query: somma sui termini *t* sia in *q* che in *d*:
$$\sum_{t\in q \cap d}(1+\log(tf_{t,d}))$$
* Il punteggio è 0 se nessuno dei termini di query è presente nel documento.

## Frequenza inversa del documento (idf)

La frequenza inversa del documento (IDF) considera che i termini rari sono più informativi. Un termine raro in un corpus, presente in un documento, indica alta rilevanza.
Il concetto è illustrato con l'esempio del termine "*arachnocentrico*".
* Un documento che contiene questo termine è molto probabilmente rilevante per la query.

Pertanto, più raro è il termine, maggiore è il suo peso.

##### Frequenza della collezione (cf) vs. Frequenza del documento (df)

Si sceglie la document frequency rispetto alla collection frequency perché quest'ultima rischia di avere correlazione con la maggior parte della tf di quel termine, ma nella pratica l'informazione di document frequency ci aiuta maggiormente nel discriminare tra un documento e un altro.

* Quale è meglio per la ricerca?
* Ad esempio, "assicurazione": cf=10440, df=3997.
* Ad esempio, "prova": cf=10422, df=8760.

##### Frequenza inversa del documento

L' *inverse document frequency* (idf) è una misura inversa dell'informatività di un termine, calcolata come:
$$idf_{t}=\log_{10}\left( \frac{N}{df_{t}} \right)$$

- dove *N* è il numero totale di documenti. Il logaritmo smorza l'effetto dell'idf.

Nota che:

* La df di un termine è unica.
* Influisce sulla classificazione dei documenti solo per le query a *k* termini (*k*<1).

## Term Frequency-Inverse Document Frequency (tf-idf)

Il metodo tf-idf (term frequency-inverse document frequency) assegna un peso ai termini in un documento in base alla loro frequenza nel documento stesso e alla loro rarità nell'intero corpus di documenti.

##### Concretamente:

* **tf (term frequency):** Maggiore è la frequenza di un termine in un documento, maggiore è il suo peso.
* **idf (inverse document frequency):** Maggiore è la rarità di un termine nell'intero corpus, maggiore è il suo peso.

Questo schema di ponderazione è in linea con la legge di distribuzione della probabilità, che prevede una distribuzione di tipo power-law nella frequenza dei termini.
- In altre parole, i termini comuni hanno una bassa probabilità di apparire, mentre i termini rari hanno una probabilità maggiore.

Il metodo tf-idf, quindi, **riduce il peso dei termini comuni e aumenta il peso dei termini rari**, riflettendo la distribuzione di probabilità osservata nella realtà.

Il peso tf-idf di un termine è il prodotto del suo peso tf e del suo peso idf:

$$w_{t,d}=\log(1+tf_{t,d})\times\log_{10}\left( \frac{N}{df_{t}} \right)$$

* **Aumenta con il numero di occorrenze all'interno di un documento.**
* **Aumenta con la rarità del termine nella collezione.**

Il punteggio per una coppia documento-query è la somma sui termini *t* sia in *q* che in *d*.

## Varianti di Tf-Idf

$$
\frac{tf_{i,d}}{\max_{j}tf_{j,d}} ,\
\frac{tf_{id}}{\sqrt{ \sum_{j}(tf_{j,d})^2 }} ,\
\frac{tf_{id} \cdot idf_{i}}{\sqrt{ \sum_{j}(tf_{j,d} \cdot idf_{j})^2 }}
$$

Esistono diverse varianti di questa tecnica, che si differenziano principalmente per il modo in cui viene calcolato il termine "tf" (frequenza del termine) e se i termini nella query sono anche ponderati.

##### Calcolo del termine "tf":

* **Con logaritmi:** Il logaritmo della frequenza del termine viene utilizzato per attenuare l'influenza dei termini che compaiono molto frequentemente in un documento.
* **Senza logaritmi:** La frequenza del termine viene utilizzata direttamente, senza alcuna trasformazione.

##### Ponderazione dei termini nella query:

* **Ponderati:** I termini nella query vengono ponderati in base alla loro importanza, ad esempio utilizzando la loro frequenza nella query stessa.
* **Non ponderati:** Tutti i termini nella query hanno lo stesso peso.

##### Assunzioni:

* La collezione di documenti è omogenea in termini di dominio dei dati, ovvero il dominio è fissato e il lessico è comune a tutti i documenti.
* I pattern di frequenza dei termini sono molto simili tra i documenti.

##### Principi chiave:

* **Peso variabile:** Il peso di uno stesso termine cambia a seconda del documento su cui appare. Un termine che compare frequentemente in un documento avrà un peso elevato, mentre un termine raro avrà un peso basso.
* **Normalizzazione della lunghezza:** I documenti hanno dimensioni diverse. Per compensare le variazioni di lunghezza, si può applicare una normalizzazione che tiene conto della lunghezza del documento.
* **Smoothing:** Per evitare che i termini rari abbiano un peso eccessivo, si può applicare uno smoothing che attenua l'influenza dei termini che compaiono poche volte.

## Normalizzazione della lunghezza

**Obiettivo:** Capire l'impatto della normalizzazione dei vettori di parole sulla rappresentazione dei topic.

**Problema:** La normalizzazione dei vettori di parole, in particolare la divisione per la norma L2, può influenzare la rappresentazione dei topic.

##### Considerazioni:

* **Diluisce il segnale informativo:** La normalizzazione può diluire il segnale informativo dei topic, soprattutto quando si passa da un abstract breve a un testo lungo.
* **Proprietà geometriche:** La normalizzazione L2 ha proprietà geometriche specifiche che possono differire da altri metodi di normalizzazione, come la normalizzazione al massimo.
* **Influenza sulla costruzione della matrice dei dati:** La normalizzazione L2 interviene direttamente nella costruzione della matrice dei dati, modificando la rappresentazione dei vettori di parole.

##### Esempio:

Se si utilizza la matrice di peso Tf-Idf senza normalizzazione e la collezione di documenti contiene un mix di documenti lunghi e brevi, la distanza euclidea tenderà a favorire i documenti più lunghi.

## Matrice di peso Tf.Idf

![[1) Intro-20241014151203327.png]]

## Documenti e query come vettori

Ci viene dato uno spazio vettoriale a |V| dimensioni.

* I termini sono gli assi dello spazio.
* I documenti sono punti o vettori in questo spazio.

Questo spazio è:

* **Ad alta dimensionalità:** Decine di milioni di dimensioni quando si applica questo a un motore di ricerca web.
* **Molto sparso:** La maggior parte delle voci è zero.

Possiamo rappresentare anche le query come vettori nello spazio.

Classifichiamo i documenti in base alla loro prossimità alla query in questo spazio.

* **Prossimità = similarità dei vettori.**
* **Prossimità ≈ inversa della distanza.**

## Prossimità dello spazio vettoriale

![[1) Intro-20241014153843176.png]]

Potremmo decidere di utilizzare una misura di prossimità che sia **scale-invariant**, ovvero indipendente dalla lunghezza dei vettori.
Invece di calcolare la distanza euclidea tra due vettori, possiamo calcolare la prossimità in termini di **angolo**.

Questo approccio comporta una certa **normalizzazione implicita**.

### Perché la Distanza Euclidea non è una Buona Idea

Le distanze di Minkowski soffrono maggiormente l'alta dimensionalità rispetto a misure di correlazione o similarità, come il coseno.
La distanza euclidea, inoltre, ha i seguenti problemi:
* **Sensibilità alla Lunghezza:** La distanza euclidea è grande per vettori di lunghezze diverse. Ad esempio, la distanza euclidea tra una query `q` e un documento `d2` è grande anche se la distribuzione dei termini in `q` e `d2` è molto simile.
* **Controesempio Principale:** La distanza euclidea è molto grande tra un documento e lo stesso concatenato con se stesso.

I documenti lunghi sarebbero più simili tra loro in virtù della lunghezza, non dell'argomento.

### Normalizzazione Implicita con l'Angolo

Possiamo normalizzare implicitamente guardando gli angoli.
* **Idea chiave:** Classificare i documenti in base all'angolo con la query.

Classifichiamo i documenti in ordine decrescente dell'angolo tra query e documento. In alternativa, classifichiamo i documenti in ordine crescente di **coseno(query, documento)**.
* Il coseno è una funzione monotona decrescente per l'intervallo [0°, 180°].

La misura del **coseno** tra due vettori multidimensionali si ottiene dal prodotto scalare tra i due vettori diviso per il prodotto delle loro norme. Se i vettori fossero inclusi in una sfera di raggio unitario, non sarebbe necessario normalizzarli.

### Normalizzazione dei Vettori

La normalizzazione di un vettore consiste nel dividere ciascuna delle sue componenti per la sua lunghezza (norma). Questo processo ha due importanti conseguenze:

* **Vettore Unitario:** Dividendo un vettore per la sua norma L2 (la radice quadrata della somma dei quadrati delle sue componenti), si ottiene un vettore unitario, ovvero un vettore con lunghezza 1. Geometricamente, questo vettore si trova sulla superficie dell'ipersfera unitaria.

* **Pesi Comparabili:** Nei modelli di informazione, la normalizzazione permette di rendere confrontabili documenti di lunghezza diversa. Documenti lunghi e brevi avranno pesi comparabili dopo la normalizzazione, evitando che la lunghezza del documento influenzi in modo sproporzionato il risultato della similarità.

La similarità del coseno è un esempio di metrica che utilizza la normalizzazione. È calcolata come il prodotto interno normalizzato di due vettori:

$$
\text{sim}(d_1, d_2) = \frac{d_1 \cdot d_2}{\|d_1\| \cdot \|d_2\|} = \frac{\sum_{i=1}^{n} w_{i,j} \cdot w_{i,k}}{\sqrt{\sum_{i=1}^{n} w_{i,j}^2} \cdot \sqrt{\sum_{i=1}^{n} w_{i,k}^2}}
$$

![[1) Intro-20241123122258468.png]]

In sintesi, la normalizzazione dei vettori, riducendoli a vettori di lunghezza unitaria, permette di confrontare in modo equo vettori di diversa lunghezza e costituisce la base per il calcolo di metriche di similarità come la similarità del coseno.

### Vector Space Proximity: Cosine Score Calculation

L'algoritmo Cosine Score calcola la similarità tra una query e i documenti in un modello a spazio vettoriale. L'algoritmo procede come segue:
```

1. float Scores[N] = 0
2. float Length[N]
3. for each query term t
4. do calculate w_t,q and fetch postings list for t
	4.1. for each pair(d, tf_(t,d), w_(t,q)) in postings list
	4.2. do Scores[d] += w_(t,d) × w_(t,q)
5. Read the array Length
6. for each d
7. do Scores[d] = Scores[d]/Length[d]
8. return Top K components of Scores
```

## Varianti di ponderazione Tf-Idf

##### Term Frequency

* **n (natural)**: $tf_{r, d}$
* **l (logarithm)**: $1 + \log(tf_{r, d})$
* **a (augmented)**: $0.5 + \frac{0.5 \cdot tf_{r, d}}{\max_{r} (tf_{r, d})}$
* **b (boolean)**: $\begin{cases} 1 & \text{if } tf_{r, d} > 0 \\ 0 & \text{otherwise} \end{cases}$

##### Document Frequency

* **n (no)**: $1$
* **t (idf)**: $\log \frac{N}{df_r}$
* **p (prob idf)**: $\max \{ 0, \log \frac{N - df_r}{df_r} \}$

##### Normalization

* **n (none)**: $1$
* **c (cosine)**: $\frac{1}{\sqrt{w_1^2 + w_2^2 + \dots + w_n^2}}$
* **u (pivoted unique)**: $\frac{1}{u}$
* **b (byte size)**: $\frac{1}{\text{CharLength}^{\alpha}}, \alpha < 1$

- **Soluzioni di default:** Le soluzioni di default in alcuni sistemi di retrieval sono le seconde di ogni tipo
- **Term frequency aumentata:** La versione aumentata della term frequency è interessante in contesti di retrieval puro, dove si confrontano query espanse con documenti.
- **Smoothing:** La term frequency aumentata prevede una sorta di smoothing, simile a quello che si incontra nel retrieval probabilistico.

## Varianti di ponderazione Tf-Idf

Molti motori di ricerca consentono ponderazioni diverse per le query rispetto ai documenti.

**Notazione SMART:** indica la combinazione in uso in un motore, con la notazione ddd.qqq, usando gli acronimi della tabella precedente.

Uno schema di ponderazione molto standard è: lnc.ltc

* **Documento:** tf logaritmico, nessun idf, normalizzazione del coseno.
* **Query:** tf logaritmico, idf, normalizzazione del coseno.

| Termine | Query | Documento | Prod | | | | | | | | |
| --------- | ----- | --------- | ----- | --- | ------ | ------ | ----- | --- | ------ | ---- | ---- |
| tf-raw | tf-wt | df | idf | wt | n'lize | tf-raw | tf-wt | wt | n'lize | | |
| auto | 0 | 0 | 5000 | 2.3 | 0 | 0 | 1 | 1 | 1 | 0.52 | 0 |
| best | 1 | 1 | 50000 | 1.3 | 1.3 | 0.34 | 0 | 0 | 0 | 0 | 0 |
| car | 1 | 1 | 10000 | 2.0 | 2.0 | 0.52 | 1 | 1 | 1 | 0.52 | 0.27 |
| insurance | 1 | 1 | 1000 | 3.0 | 3.0 | 0.78 | 2 | 1.3 | 1.3 | 0.68 | 0.53 |

**Documento:** assicurazione auto assicurazione auto
**Query:** migliore assicurazione auto

- **Punteggio = 0 + 0 + 0.27 + 0.53 = 0.8**

## Classifica dello spazio vettoriale

##### Riepilogo:

* Rappresentare ogni documento e query come un vettore tf-idf ponderato.
* Calcolare il punteggio di similarità del coseno per il vettore di query e ogni vettore di documento.
* Classificare i documenti rispetto alla query in base al punteggio.
* Restituire i primi K all'utente.

##### Pro:

* Fornisce una corrispondenza parziale e una misura naturale di punteggi/classifica.
* Funziona abbastanza bene nella pratica nonostante le ipotesi semplificative.
* Implementazione efficiente.

##### Contro:

* Informazioni sintattiche mancanti.
* Informazioni semantiche mancanti.
* Ipotesi di indipendenza dei termini (BoW).
* Ipotesi che i vettori dei termini siano ortogonali a coppie.
* Manca il controllo di un modello booleano (ad esempio, richiedere che un termine appaia in un documento).

