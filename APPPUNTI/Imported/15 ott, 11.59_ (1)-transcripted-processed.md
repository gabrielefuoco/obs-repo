## Registrazione di un'area: Grafo di computazione, flusso forward e backward

La registrazione di un'area è definita dal grafo di computazione associato alla parte. Successivamente, si definiscono il flusso di forward e di backward. Il flusso di forward è abbastanza standard e viene automatizzato. Il flusso di backward, invece, è definito dalle regole reali dell'indicazione.

### Tensori e operazioni multidimensionali

È importante notare che in questo contesto non ci limiteremo a semplici variabili, ma lavoreremo con tensori, ovvero strutture multidimensionali. Questo significa che dovremo fare un upgrade di tutti i calcoli matriciali, considerando le operazioni su tensori.

Le operazioni su tensori possono essere di due tipi:

* **Operazioni di punto a punto:** Ogni nodo del grafo di computazione rappresenta un'operazione su tensori, dove gli input sono tensori e l'obiettivo è ottimizzare questi tensori.
* **Operazioni matriciali:** Si applicano le regole del calcolo matriciale, considerando che ogni nodo rappresenta un'operazione su tensori.

### Esempi di calcolo della derivata

Consideriamo alcuni esempi di come si calcola la derivata in base al tipo di input e output:

* **Input scalare, output scalare:** La derivata è uno scalare.
* **Input vettore, output scalare:** La derivata è un vettore, ovvero la derivata dell'output rispetto ad ogni dimensione dell'input.
* **Input vettore, output vettore:** La derivata è una matrice, dove per ogni dimensione dell'output si calcolano le derivate rispetto a ogni dimensione dell'input.

Questo concetto può essere esteso a qualsiasi vettore multidimensionale, come matrici, input matrice, output matrice. Per gestire queste operazioni, avremo bisogno di un framework che permetta di eseguire queste operazioni in modo efficiente.

### Propagazione della derivata vettoriale

Consideriamo un esempio con due vettori di input, uno in $\mathbb{R}^M$ e l'altro in $\mathbb{R}^K$, che vengono combinati tramite un'operazione per ottenere un altro vettore in $\mathbb{R}^M$. La propagazione della derivata viene fatta usando il calcolo differenziale, in particolare il calcolo tensoriale.

Le regole per la propagazione della derivata sono abbastanza standard. Se si ha familiarità con il calcolo differenziale su multidimensionalità, si può capire come si propaga la derivata in questo contesto.

### Esempio di calcolo della derivata rispetto a un vettore

Supponiamo di avere una rete neurale con un nodo di output multidimensionale. Vogliamo calcolare la derivata rispetto a un vettore di input. Questo è sostanzialmente un Jacobiano, perché stiamo assumendo che l'output abbia dimensione $M$.

Il Jacobiano si proietta rispetto alle due direzioni:

* **Proiezione verso $y$:** $\frac{\partial z}{\partial y}$
* **Proiezione verso $x$:** $\frac{\partial z}{\partial x}$

La proiezione viene fatta tramite una moltiplicazione matriciale, che è l'estensione della moltiplicazione matriciale al campo tensoriale.

### Conclusione

La registrazione di un'area è un processo complesso che richiede la comprensione del grafo di computazione, del flusso di forward e di backward, e delle operazioni su tensori. Il calcolo della derivata è un passo fondamentale per l'ottimizzazione dei modelli di apprendimento automatico.


## Differenziazione Automatica con PyTorch

Questo documento esplora il concetto di differenziazione automatica utilizzando la libreria PyTorch.

### Introduzione a PyTorch

PyTorch è una libreria di deep learning che offre funzionalità simili a NumPy, ma con l'aggiunta di un potente meccanismo di differenziazione automatica. Questo meccanismo consente di calcolare automaticamente le derivate di funzioni complesse, rendendo più semplice l'ottimizzazione di modelli di apprendimento automatico.

### Esempio di Differenziazione Automatica

Consideriamo l'esempio presentato nelle slide, che coinvolge una serie di matrici e operazioni tra loro.

**Grafo di Computazione:**

Il grafo di computazione rappresenta la sequenza di operazioni che vengono eseguite per calcolare il risultato finale. In questo caso, il grafo è composto dai seguenti nodi:

* **X:** Matrice 3x2 con tutti gli elementi uguali a 1.
* **Y:** Matrice 2x2.
* **Y_tilde:** Matrice 2x2 ottenuta moltiplicando Y per 0.25.
* **Z_tilde:** Matrice 2x2 ottenuta moltiplicando Y per 0.25.
* **Z1:** Matrice 3x2 ottenuta moltiplicando X per Y.
* **Z2:** Matrice 3x2 ottenuta sommando Z1 e Z_tilde.
* **Y1:** Matrice 3x2 ottenuta sommando 0.2 moltiplicato per Z2 e un tensore 1x2 con elementi [0.2, 1].

**Propagazione del Gradiente:**

La differenziazione automatica in PyTorch funziona calcolando il gradiente di ogni nodo rispetto al risultato finale. Questo processo di propagazione del gradiente inizia dal nodo finale e si propaga all'indietro attraverso il grafo, calcolando le derivate parziali di ogni nodo rispetto al nodo precedente.

**Calcolo del Gradiente:**

Per calcolare il gradiente di un nodo, PyTorch utilizza un algoritmo di backpropagation. Questo algoritmo calcola le derivate parziali di ogni nodo rispetto al nodo precedente, utilizzando la regola della catena.

**Utilizzo di PyTorch:**

PyTorch fornisce una serie di funzioni per definire e manipolare tensori, eseguire operazioni matematiche e calcolare i gradienti. Ad esempio, la funzione `torch.autograd.grad` può essere utilizzata per calcolare il gradiente di una funzione rispetto a un input specifico.

**Conclusione:**

La differenziazione automatica è una potente funzionalità di PyTorch che semplifica il processo di ottimizzazione dei modelli di apprendimento automatico. Il meccanismo di propagazione del gradiente consente di calcolare automaticamente le derivate di funzioni complesse, rendendo più semplice l'aggiornamento dei parametri del modello durante l'addestramento.


## Appunti sulla computazione del gradiente

Questo testo descrive il processo di computazione del gradiente in un grafo di computazione, evidenziando le diverse operazioni coinvolte e il loro ruolo nel calcolo del gradiente.

**Grafo di computazione:**

Il grafo di computazione è una rappresentazione visiva delle operazioni che vengono eseguite per calcolare un risultato finale. In questo caso, il risultato finale è un valore scalare chiamato ROS (Reduced Output Scalar).

**Operazioni nel grafo:**

Il grafo di computazione presenta diverse operazioni:

* **Moltiplicazione:** Esistono due tipi di moltiplicazione:
    * **Moltiplicazione di origine:** Questa moltiplicazione è la prima operazione nel grafo.
    * **Moltiplicazione di punto a punto:** Questa moltiplicazione è eseguita dopo la moltiplicazione di origine.
* **Somma:** Questa operazione combina i risultati delle moltiplicazioni.
* **Modificazione tramatrice:** Questa operazione modifica la struttura dei dati.
* **Collassamento:** Questa operazione riduce un tensore in uno scalare.

**Tensori e gradiente:**

Il grafo di computazione coinvolge diversi tensori, tra cui x, y e z. Questi tensori hanno un attributo "requires_grad=True", il che significa che il gradiente deve essere calcolato rispetto a questi tensori.

**Calcolo del gradiente:**

Il calcolo del gradiente inizia dalla radice del grafo (ROS) e procede verso le foglie. Ogni nodo del grafo ha un'operazione associata che determina come il gradiente viene propagato.

**Esempio di calcolo del gradiente:**

Il testo fornisce un esempio di calcolo del gradiente per un nodo specifico. Il nodo è una somma, quindi il gradiente viene propagato come una costante per la dimensione del tensore.

**Conclusione:**

Il testo fornisce una panoramica del processo di computazione del gradiente in un grafo di computazione. Il grafo è composto da diverse operazioni che influenzano il calcolo del gradiente. Il calcolo del gradiente inizia dalla radice del grafo e procede verso le foglie, con ogni nodo che contribuisce al calcolo del gradiente finale.


## Il Gradiente e l'Operazione di Backward

L'operazione di somma nel grafo di computazione ci indica che l'operazione di backward dovrebbe essere un'operazione di somma. Questo perché il gradiente di una somma è la somma dei gradienti.

Ad esempio, se consideriamo Z1, definito come una moltiplicazione di matrici, la funzione sul gradiente è la funzione di moltiplicazione di matrici. Lo stesso vale per y, che è anch'essa una moltiplicazione.

**Il Grafo di Computazione e l'Operazione di Backward**

Il grafo di computazione rappresenta il processo di calcolo dell'output a partire dagli input. In questo caso, il grafo ha due punti di ingresso e un punto di uscita. L'operazione di backward, implementata in PyTorch, esegue il calcolo del gradiente a partire dall'output e propagandolo all'indietro attraverso il grafo.

**Il Gradiente e la Componente Gradiente**

Dopo aver eseguito l'operazione di backward, ogni variabile nel grafo avrà una componente gradiente. Questa componente rappresenta la derivata della variabile rispetto all'output. Ad esempio, il gradiente rispetto a O è la derivata di O rispetto a se stessa.

**Calcolo del Gradiente Rispetto a un Input**

Se vogliamo calcolare il gradiente rispetto a un input, come X, dobbiamo eseguire il backward a partire da O e propagare il gradiente fino a X. In questo caso, X è un tensore 3x2 e il gradiente sarà una matrice 3x2, dove ogni elemento rappresenta la derivata di O rispetto alla corrispondente posizione di X.

**Nodi Foglia e Richiesta di Gradienti**

I nodi foglia sono gli input del grafo. Se un nodo foglia è contrassegnato come "richiede gradienti", significa che il framework calcolerà il gradiente rispetto a quel nodo e lo utilizzerà per l'ottimizzazione dei parametri.

**In sintesi:**

- L'operazione di backward calcola il gradiente di ogni variabile nel grafo rispetto all'output.
- Il gradiente di una somma è la somma dei gradienti.
- I nodi foglia contrassegnati come "richiede gradienti" sono i parametri che vengono ottimizzati.


## Propagazione all'indietro e Rete Neurale

In questa lezione, stiamo esaminando la propagazione all'indietro in un contesto di reti neurali. Il concetto chiave è quello di un **grafo di computazione**, che rappresenta le operazioni eseguite per calcolare il valore finale di una funzione.

**Esempio:**

Immaginiamo di avere una funzione che calcola il valore di `E` in base a `A`, `B`, `C` e `D`. Il grafo di computazione mostrerà le operazioni coinvolte, come ad esempio:

* `C = A + B`
* `D = C * 2`
* `E = D + 1`

**Propagazione all'indietro:**

La propagazione all'indietro è il processo di calcolo delle derivate parziali di `E` rispetto a ogni variabile di input (`A`, `B`, `C`, `D`). Questo processo si basa sulle regole di derivazione e sul grafo di computazione.

**Passaggi:**

1. **Calcolo del valore finale:** Si calcola il valore di `E` utilizzando il grafo di computazione.
2. **Calcolo delle derivate parziali:** Si calcolano le derivate parziali di `E` rispetto a ogni variabile di input, seguendo le regole di derivazione e il grafo di computazione.
3. **Propagazione all'indietro:** Si propagano le derivate parziali lungo il grafo di computazione, utilizzando la regola della catena.

**Esempio:**

Per calcolare la derivata di `E` rispetto ad `A`, si procede come segue:

* `dE/dA = dE/dD * dD/dC * dC/dA`
* `dE/dD = 1` (derivata di `E` rispetto a `D`)
* `dD/dC = 2` (derivata di `D` rispetto a `C`)
* `dC/dA = 1` (derivata di `C` rispetto ad `A`)

Quindi, `dE/dA = 1 * 2 * 1 = 2`.

**Tensori:**

Nella pratica, le reti neurali utilizzano tensori per rappresentare i dati e le operazioni. I tensori sono matrici multidimensionali che permettono di rappresentare dati complessi.

**Rete Neurale:**

Una rete neurale è un modello matematico che apprende da dati. È composta da nodi (neuroni) interconnessi che eseguono operazioni matematiche.

**Grafo di computazione in una rete neurale:**

Il grafo di computazione di una rete neurale rappresenta le operazioni eseguite dai neuroni per calcolare il valore finale. Ogni nodo rappresenta un neurone e ogni arco rappresenta un'operazione.

**Annotazione del grafo:**

Ogni arco del grafo di computazione è annotato con la derivata parziale del nodo di destinazione rispetto al nodo di origine. Ad esempio, l'arco che collega il nodo `C` al nodo `E` è annotato con `dE/dC`.

**Propagazione all'indietro in una rete neurale:**

La propagazione all'indietro in una rete neurale è il processo di calcolo delle derivate parziali del valore finale rispetto a ogni parametro della rete. Questo processo si basa sulle regole di derivazione e sul grafo di computazione.

**Regola del campo matriciale:**

La regola del campo matriciale è una regola di derivazione che si applica ai tensori. Questa regola permette di calcolare le derivate parziali di una funzione rispetto a un tensore.

**MyTorch:**

MyTorch è una libreria di deep learning che implementa la propagazione all'indietro. MyTorch utilizza la regola del campo matriciale per calcolare le derivate parziali e la propagazione all'indietro per aggiornare i parametri della rete.

**Rating Grad:**

Rating Grad è un metodo per annotare il grafo di computazione con i gradienti. Questo metodo è utile per visualizzare e comprendere la propagazione all'indietro.

**Conclusione:**

La propagazione all'indietro è un processo fondamentale per l'apprendimento in reti neurali. Permette di calcolare le derivate parziali del valore finale rispetto a ogni parametro della rete, che vengono poi utilizzate per aggiornare i parametri e migliorare le prestazioni della rete.


## Grafi di computazione e backpropagation

### Nodi intermedi e require_grad

Tutti i nodi intermedi, ovvero i nodi che non sono foglie e non sono indicati con `require_grad`, non vengono memorizzati. Vengono calcolati durante la fase di *forward pass* e poi scartati. 

L'attributo `require_grad` indica al sistema di conservare il valore del nodo. Se non viene specificato, il valore non sarà disponibile. Ad esempio, la variabile `y1` nel codice è un nodo intermedio e non verrà memorizzato. Se invece si specifica `require_grad` per `y1`, il suo valore sarà conservato.

### Calcolo dei gradienti

Il fatto che non memorizziamo i gradienti di tutti i nodi intermedi non impedisce il calcolo dei gradienti durante la fase di *backward pass*. I gradienti vengono calcolati, ma non vengono memorizzati. 

La memorizzazione di tutti i gradienti richiederebbe di memorizzare l'intero grafo di computazione, il che può essere molto costoso in termini di memoria, soprattutto per grafi grandi e complessi.

### Esempio di rete neurale

Consideriamo una rete neurale banale con una sola funzione, la funzione sigmoide (o logistica). La funzione sigmoide è definita come:

```
σ(x) = 1 / (1 + exp(-x))
```

Assumiamo che la funzione sigmoide abbia un'estensione vettoriale, ovvero se `x` è un vettore, il risultato sarà un vettore in cui ogni elemento è il risultato della funzione sigmoide applicata all'elemento corrispondente di `x`.

```
σ(x) = [σ(x1), σ(x2), ..., σ(xn)]
```

Questa estensione vale per qualsiasi forma tensoriale, non solo per i vettori.

### Grafo di computazione per la rete neurale

Consideriamo la seguente struttura:

```
z = σ(Wx + b)
```

Dove:

* `x` è un vettore in `R^2`
* `W` è una matrice `3x2`
* `b` è un vettore in `R^3`
* `z` è un vettore in `R^3`

Il grafo di computazione per questa rete neurale è il seguente:

1. **Somma:** `Wx + b`
2. **Sigmoide:** `σ(Wx + b)`

Quindi, per ottenere `z`, dobbiamo prima calcolare la somma di `Wx` e `b`, e poi applicare la funzione sigmoide al risultato.

### Definizione del grafo di computazione

Per definire il grafo di computazione, dobbiamo identificare le operazioni che vengono eseguite. In questo caso, le operazioni sono:

1. **Somma:** `Wx + b`
2. **Sigmoide:** `σ(Wx + b)`

Il grafo di computazione è quindi composto da due nodi:

1. **Nodo somma:** Questo nodo riceve in input `x`, `W` e `b`, e produce in output `Wx + b`.
2. **Nodo sigmoide:** Questo nodo riceve in input `Wx + b` e produce in output `z`.

### Conclusione

Il grafo di computazione è uno strumento fondamentale per comprendere il funzionamento delle reti neurali e per implementare la backpropagation. La comprensione del grafo di computazione e delle sue proprietà è essenziale per ottimizzare le prestazioni delle reti neurali.


## Rete Morale e Grafo di Computazione

**Introduzione**

Questo documento descrive la relazione tra una rete morale e un grafo di computazione, illustrando come il grafo di computazione può essere utilizzato per rappresentare le operazioni di una rete morale.

**Notazione**

* **x:** Vettore di input
* **z:** Vettore di output intermedio
* **y:** Vettore di output finale
* **v:** Matrice dei pesi che collegano z e y
* **W:** Matrice dei pesi che collegano x e z
* **σ:** Funzione di attivazione (ad esempio, sigmoidale)
* **A1:** Matrice 2x3 che rappresenta la moltiplicazione di x per W
* **A2:** Matrice 3x2 che rappresenta la moltiplicazione di z per v

**Grafo di Computazione**

Il grafo di computazione rappresenta le operazioni eseguite dalla rete morale. In questo caso, il grafo è composto dai seguenti nodi:

1. **x:** Nodo di input
2. **A1:** Nodo che rappresenta la moltiplicazione di x per W
3. **z:** Nodo che rappresenta l'applicazione della funzione di attivazione σ su A1
4. **A2:** Nodo che rappresenta la moltiplicazione di z per v
5. **y:** Nodo che rappresenta l'applicazione della funzione di attivazione σ su A2

**Relazione con la Rete Morale**

Il grafo di computazione rappresenta le operazioni della rete morale in modo esplicito. Ogni nodo del grafo corrisponde a un'operazione specifica nella rete morale. Ad esempio, il nodo A1 rappresenta la moltiplicazione di x per W, che è un'operazione che viene eseguita nella rete morale.

**Calcolo del Gradiente**

Per ottimizzare i pesi della rete morale, è necessario calcolare il gradiente della funzione di perdita rispetto ai pesi. Il grafo di computazione facilita questo calcolo, poiché consente di tracciare il percorso del gradiente attraverso la rete.

**Nodi di Foglia**

I nodi di foglia nel grafo di computazione sono i nodi che non hanno figli. In questo caso, i nodi di foglia sono x e y. I nodi di foglia sono importanti perché sono i nodi da cui inizia il calcolo del gradiente.

**Funzione di Perdita**

La funzione di perdita è una funzione che misura la differenza tra l'output della rete morale e il valore desiderato. Il gradiente della funzione di perdita rispetto ai pesi viene utilizzato per aggiornare i pesi durante l'addestramento.

**Conclusione**

Il grafo di computazione è uno strumento utile per comprendere e analizzare le operazioni di una rete morale. Esso facilita il calcolo del gradiente e consente di visualizzare il flusso di informazioni attraverso la rete.


## Operazioni del Framework

Questo framework ci permette di implementare due operazioni fondamentali:

1. **Passo di Poma:** 
    - Partendo dai valori attuali di W e V, e dai valori di X, calcoliamo Y.
    - Continuiamo a provare fino a raggiungere il problema della rossa.
2. **Passo di Pequa:**
    - Annotiamo tutto il grafo con i gradienti.
    - I gradienti sono rappresentati da due variabili: un doppio e un u.

**Differenze con Nampai:**

Questo framework va oltre Nampai, che si limita al livello di tensione e all'operazione di tensione. 

**Vantaggi:**

- **Automazione del calcolo dei gradienti:** Il framework automatizza il calcolo dei gradienti, fissando un punto di ingresso e un punto di uscita sul pericolente.
- **Gradienti discendenti:** L'automazione del calcolo dei gradienti permette di implementare gradienti discendenti con due linee di logica.

**Conclusione:**

Il vantaggio principale di questo framework è la sua capacità di automatizzare il calcolo dei gradienti, aprendo nuove possibilità per l'implementazione di gradienti discendenti. 


