## Introduzione alle Funzioni Kernel

In questo contesto, consideriamo:

* **Vettori di input:** $\vec{x_{i}}$ nello spazio di input $\mathbb{R}^{d_{1}}$.
* **Trasformazione non lineare:** $\phi(\vec{x_i})$ che mappa i vettori di input in uno spazio di output ad alta dimensione $\mathbb{R}^{d_{2}}$.
* **Relazione tra dimensioni:** $d_{1} \ll d_{2}$, ovvero la dimensione dello spazio di input √® molto minore della dimensione dello spazio di output.

**Prodotto scalare:**

* Nello spazio di input: $<\vec{x_i},\vec{x_{j}}>$.
* Nello spazio di output: $<\phi (\vec{x_i}),\phi(\vec{x_{j})}>$.

**Costo computazionale:**

* $O(d_{1})$: indica un costo computazionale proporzionale alla dimensione dello spazio di input.
* $O(d_{2})$: indica un costo computazionale proporzionale alla dimensione dello spazio di output.

## Funzioni Kernel $K_{\phi}$

Data una trasformazione $\phi: \mathbb{R}^{d_{1}} \to \mathbb{R}^{d_{2}}, (d_{2} \gg d_{1})$, una funzione kernel $K_{\phi}: \mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{1}} \to \mathbb{R}$ √® tale che:

$$\forall \vec{x_{1}},\vec{x_{2}} \in \mathbb{R}^{d_{1}}, K_{\phi}(\vec{x_{1}},\vec{x_{2}})\ = \ <\phi (\vec{x_1}),\phi(\vec{x_{2})}>$$

Le funzioni kernel interessanti hanno costo computazionale $O(d_{1})$. 
Se troviamo una funzione avente questa propriet√†, abbiamo risolto il nostro problema.

**Nota:**

* La notazione $<\cdot, \cdot>$ indica il prodotto scalare.
* La condizione $d_{1} \ll d_{2}$ indica che la dimensione dello spazio di input $d_{1}$ √® molto minore della dimensione dello spazio di output $d_{2}$.
* Il costo computazionale $O(d_{1})$ indica che il tempo necessario per calcolare la funzione kernel √® proporzionale alla dimensione dello spazio di input.

## Kernel Polinomiale

Il kernel polinomiale √® un esempio di funzione kernel che permette di eseguire una trasformazione non lineare dei dati nello spazio di input, proiettandoli in uno spazio di output ad alta dimensione.

**Definizione:**

Dato un vettore di input $x \in \mathbb{R}$, il prodotto scalare tra il vettore dei pesi $\vec{w}$ e la trasformazione non lineare $\phi(x)$ √® definito come:

$<\vec{w},\phi(x)>\  = w_{0}+w_{1}x+w_{2}x^{2}+\dots+w_{n}x^n$

dove il prodotto scalare rappresenta la normale al nostro iperpiano.

**Trasformazione:**

Possiamo riscrivere il prodotto scalare come:

$=\ <w_{0}+w_{1}+\dots+w_{n}>,\ (1,x,x^2,\dots,x^n)>$

Dove $(1,x,x^2,\dots,x^n)= \phi(x)$ √® la trasformazione non lineare che mappa il vettore di input $x$ in uno spazio di output ad alta dimensione.

**Forma del Kernel Polinomiale:**

Il kernel polinomiale ha la seguente forma:

$K_{\phi_{n}}(\vec{x_{1}},\vec{x_{2}})=(1+<\vec{x_{1}},\vec{x_{2}}>)^n$

**Costo computazionale:**

Il costo computazionale del kernel polinomiale √® $O(d_{1})$, a causa del prodotto scalare tra i vettori di input.

**Esempio:**

Consideriamo il caso $d_1=1, \ n=2$:

$$
\begin{align*}
&K_{\phi_{n}}({x_{1}},{x_{2}})=
\\&(1+x_{1}x_{2})^2=x_{1}^2x_{2}^2+a+2x_{1}x_{2}=
\\&(1,\sqrt{ 2 }x_{1},x_{1}^2),(1,\sqrt{ 2 }x_{2},x_{2}^2)=\\
& \ <\phi_{2}(x_{1}),\phi_{2}(x_{2})>
\end{align*}
$$

**Caso multidimensionale:**

Per un vettore di input multidimensionale $\vec{x_{1}} = (x_{11}, x_{12})$ e $\vec{x_{2}} = (x_{21}, x_{22})$, il kernel polinomiale diventa:


$$\begin{aligned} k_{\phi_{2}}(\vec{x_{1}},\vec{x_{2}})&=(1+<\vec{x_{1}},\vec{x_{2}}>)^n=\\ &=(1+<(x_{11},x_{12}),(x_{21},x_{22})>)^2\\ &=1+x_{11}^2x_{21}^2+x_{12}^2,x_{22}^2+2x_{11}x_{21}+2x_{12}x_{22}+2x_{11}x_{12}x_{21}x_{22}\\ &=<(1,\sqrt{ 2 }x_{11},\sqrt{ 2 }x_{12},\sqrt{ 2 }x_{11}x_{12},x_{11}^2,x_{12}^2),(1,\sqrt{ 2 }x_{21},\sqrt{ 2 }x_{22},\sqrt{ 2 }x_{21}x_{22},x_{21}^2,x_{22}^2)>\\ &=<\phi_{2}(\vec{x_{1}}),\phi_{2}(\vec{x_{2}})> \end{aligned}$$


$d_{2}=\exp \ in \ d_{1}\in n$
??????

## Support Vector Machine + Kernel

L'algoritmo pu√≤ essere descritto come segue
$$\begin{aligned}
&\beta^{(1)}=\vec{0} \\
&\text{for } T=1 \text{ to } T-1 \text{ do} \\
&\qquad \vec{\alpha}^{(t)}=\frac{1}{\lambda t}\vec{\beta}^{(t)} \\
&\qquad \text{seleziona casualmente } (\vec{x_{i}},y_{i}) \text{ in } S \\
&\qquad \text{if } y_{1} \sum_{j=1}^m \alpha_{j} <\phi(\vec{x_{i}}),\phi(\vec{x_{j}})> \ <1 \\
&\qquad \beta_{i}^{(t+1)}=\beta_{i}^{(t)}+y_{i} \\
&\text{return } \ \vec{\alpha}
\end{aligned}$$

Se esiste una funzione kernel, possiamo sostituire il prodotto scalare nello spazio di destinazione con il valore della funzione kernel:
$$y_{1} \sum_{j=1}^m \alpha_{j} K_{\phi}(\vec{x_{i}}\vec{x_{j}}) \ <1$$
Questo permette di calcolare il prodotto scalare senza dover esplicitamente calcolare la proiezione dei dati nello spazio di caratteristiche, rendendo l'algoritmo pi√π efficiente.


## Kernel Gaussiano

$$K(\vec{x},\vec{x}')=\exp\left[ -  \frac{\|\vec{x}-\vec{x}'\|^2}{2\sigma^2} \right]$$

dove:

- $\vec{x}$ e $\vec{x}'$ sono due punti nello spazio di input.
- $\sigma$ √® un parametro che controlla la larghezza del kernel.

![[8)-20241024123135373.png|384]]

![[8)-20241024123236760.png|183]]

Teoricamente, il kernel gaussiano effettua un mapping a infinite dimensioni. Per illustrare questo concetto, consideriamo il caso a una dimensione:

$$K(x,x')=\exp\left[ - \frac{(x-x')^2}{2} \right]$$

Utilizzando la propriet√† della funzione esponenziale:

$$\exp(x)=\sum_{n=0}^{\infty} \frac{x^n}{n!}$$

possiamo riscrivere il kernel gaussiano come:

$$\exp\left[ - \frac{x^2+x'^2-2xx'}{2} \right]$$
$$=\exp\left( -\frac{x^2+x'^2}{2} \right) \exp(x x')$$
$$=\exp\left( -\frac{x^2+x'^2}{2} \right)=\sum_{n=0}^{\infty}   \frac{(xx')^2}{n!}$$

Espandendo l'ultima espressione, otteniamo:

$$\sum_{n=0}^{\infty}  \left\{  \left[ {\frac{x^n}{n!}} \exp\left( -  \frac{x^{2}}{2} \right) \right] \cdot \left[ {\frac{x'^n}{n!}} \exp\left( -  \frac{x'^{2}}{2} \right) \right]  \right\}$$

Questa espressione rappresenta un prodotto scalare infinito di due vettori $\phi(x)_{n}$ e $\phi(x')_{n}$.

Questa tecnica permette di estendere l'applicabilit√† di questi algoritmi anche a training set in cui gli esempi non sono vettori.



# Neural Networks

Si ottiene collegando tra di loro pi√π neuroni artificiali secondo diverse modalit√†

Architetture pi√π semplici: Reti neurali alimentate in avanti

## FeedForward Neaural Networks

Il flusso dell'informazione va solo in una direzione (dall'input all'output)
#### Struttura generale:
Queste reti possono essere descritte attraverso un grafo diretto aciclico. La rete pu√≤ essere rappresentata come un grafo $ùê∫ = (ùëâ, ùê∏,\vec{w},\phi)$ dove:

![[8)-20241024130024790.jpg|537]]
Dove:
* **ùëâ (nodi)** rappresentano i neuroni artificiali.
* **ùê∏ (archi)** rappresentano i collegamenti tra i neuroni.
* **ùë§** √® una funzione che assegna un peso ad ogni arco.
* $\phi$ √® la funzione di attivazione.

I neuroni del livello di input non eseguono calcoli, ma trasmettono i valori da elaborare alla rete. Il livello successivo √® il livello 1. Tutti i livelli intermedi tra il livello di input e quello finale sono detti livelli nascosti o "hidden". Ogni livello √® composto da un numero di neuroni, che pu√≤ variare da livello a livello.

Quando ogni neurone del livello i-esimo √® collegato a ogni neurone del livello i+1-esimo, la rete √® detta **densa**. 


![[8)-20241024130054817.png|354]]



## Funzioni di attivazione

**Segno:**
![[8)-20241024130744776.png|402]]
$\phi$ sulle ordinate e $<\vec{w},\vec{x}>$ sulle ascisse.
Quando usiamo questa funzione il neruone corrisponde a un semispazio.

**Gradino:**
![[8)-20241024130804304.png|423]]

Il problema di queste funzioni √® legato al fatto che la loro derivata non √® utile ai fini dell'algoritmo di minimizzazione, poich√® √® sempre prossima allo zero.
Una soluzione √® data dalla funzione Sigmoide:

**Sigmoide**:
![[8)-20241024130821929.png|455]]
Gode della seguente prorpiet√†:
$\sigma'(x)=\sigma(x)(1-\sigma (x))$

**Tangente iperbolica:**
![[8)-20241024131749391.png|445]]

$\text{tanh}(x)=\tau(x)=2\sigma(2x)-1$
$\tau'(x)=1-\tau(x)^2$

Se ci allontaniamo dall'origine, il gradiente tende ad azzerarsi e la funzione non cresce(Regioni saturanti). Per risolvere questo problema vengono in gioco altre funzioni di attivazione:

**ReLU:**
![[8)-20241024131933828.png|446]]
$$\mathrm{ReLU}=\max(0,x)$$
$$ \mathrm{ReLU}'
\begin{cases}
0, \ x<0 \\
1< \ x\geq 0
\end{cases}
$$
**Leaky ReLU**
![[8)-20241024132331064.png|436]]

$$ \mathrm{Leaky \ ReLU}'
\begin{cases}
x, \ x>0 \\
\alpha x, \  x\leq 0
\end{cases}
$$
