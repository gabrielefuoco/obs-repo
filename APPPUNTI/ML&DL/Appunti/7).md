## SVM - Support Vector Machines

L'algoritmo SVM (Support Vector Machines) è un algoritmo di apprendimento che si basa sulla ricerca di semispazi. In altre parole, si tratta di un predittore lineare che cerca di trovare un iperpiano separatore. Questo iperpiano divide lo spazio in due piani, uno positivo e uno negativo.

Se i dati sono linearmente separabili, l'obiettivo è trovare un iperpiano che li divida perfettamente. Tuttavia, non tutti gli iperpiani sono uguali. Per questo motivo, si introducono dei criteri di preferenza: si desidera un iperpiano che massimizzi il margine.

### Margine

Il margine è la minima distanza tra l'iperpiano e uno dei punti del training set.

#### Immagine SVM

![Immagine SVM](immagine_svm.png)

La distanza tra un punto $\vec{x}$ e l'iperpiano definito da $(\vec{w}, b)$ è data dalla seguente formula:

$$Dist(\vec{x},(\vec{w},b))=\frac{|(\vec{w}+\vec{x})+b|}{\|\vec{w}\|}$$

Il margine è quindi definito come:

$\text{Margin}_{s}(\vec{w},b)=\min_{1\leq i \leq m}Dist(\vec{x},(\vec{w},b))=\min_{1\leq i \leq m} \frac{|<\vec{w},\vec{x}>+b|}{\|\vec{w}\|}$

Nel caso di dati linearmente separabili, esiste un iperpiano separatore con errore empirico nullo. Questo significa che l'iperpiano separatore non commette errori nella classificazione dei dati del training set.

In questo caso, il margine può essere espresso come:

$$\text{Margin}_{s}(\vec{w},b)=\min_{1\leq i \leq m}\frac{y_{i}\cdot(<\vec{w},\vec{x_{i}}>+b)}{\|\vec{w}\|} >0$$

dove $y_i$ è l'etichetta del punto $\vec{x_i}$.

## Hard SVM

L'obiettivo dell'Hard SVM è trovare l'iperpiano che massimizza il margine. Questo si traduce nel seguente problema di ottimizzazione:

$$(\vec{w}^*,b^*)=\arg \max_{(\vec{w},b)\in R^{d+1}}\text{Margin}_{S}((\vec{w},b))$$

che può essere riscritto come:

$$(\vec{w}^*,b^*)=\arg \max_{(\vec{w},b)\in R^{d+1}}\min_{1\leq_{1}\leq,m} \frac{y_{i}(<\vec{w},\vec{x}>+b)}{\|\vec{w}\|}$$

Questo problema è soggetto ai seguenti vincoli:

$$
\begin{cases} 
(\vec{w}^*,b^*)=\arg\min_{(\vec{w},b)\in R^{d+1}} \ \|\vec{w}\|^2   \\
\forall_{i}, y_{i}(<\vec{w},\vec{x_{i}}>+b)\geq 1
\end{cases}
$$

In altre parole, si cerca l'iperpiano separatore con norma minima. Questo problema è un problema di ottimizzazione quadratica, per il quale esistono algoritmi specifici ed efficienti.

Questa formulazione è simile a quella introdotta parlando di semispazi:

$$
\begin{cases}
\vec{w}^*=\arg\min_{\vec{w}\in R^{d+1}} <0,\vec{w}> \\
\forall_{i}, y_{i}(<\vec{w},\vec{x_{i}}>+b)\geq_{1}
\end{cases}
$$

Nella formulazione originale, qualsiasi iperpiano è accettabile. Questo è un problema lineare.

Per risolvere l'Hard SVM, è necessario utilizzare algoritmi di ottimizzazione quadratica. Tuttavia, nella pratica questa formulazione non viene utilizzata perché i dati reali non sono linearmente separabili. Di conseguenza, non esiste un iperpiano separatore e l'insieme delle soluzioni è vuoto.

Per affrontare questo problema, è necessario passare a una formulazione più generale che funzioni con dati non linearmente separabili.


## Soft SVM

L'Hard SVM assume che i dati siano linearmente separabili, il che non è sempre vero nella realtà. La Soft SVM introduce un modo per gestire i punti mal classificati, ovvero i punti che si trovano dal lato sbagliato dell'iperpiano.

L'approccio della Soft SVM è quello di rilassare i vincoli dell'Hard SVM, poiché alcuni punti non li rispettano. Per fare ciò, si introduce una variabile aggiuntiva $\xi_i$ per ogni punto $x_i$. Questa variabile misura l'entità della violazione del vincolo da parte del punto.

* Se il punto si trova dal lato corretto dell'iperpiano (oltre il suo margine di competenza), $\xi_i = 0$ (il vincolo non viene violato).
* Per i punti che si trovano oltre il margine di competenza, $\xi_i$ rappresenta la distanza del punto dal margine di competenza. In questo caso, $\xi_i > 0$.

La formulazione della Soft SVM diventa:

$$
\begin{cases} 
(\vec{w}^*,b^*)=\arg\min_{(\vec{w},b)\in R^{d+1}} \ \|\vec{w}\|^2   \\ \\

\forall_{i}, y_{i}<\vec{w},\vec{x_{i}}>\ \geq 1-\xi_{i}, \ \xi_{i}\geq_{0}
\end{cases}
$$

In questo modo, i vincoli sono stati rilassati. Tuttavia, è necessario porre un vincolo sull'assegnamento dei valori a $\xi_i$. Questo si ottiene modificando la funzione obiettivo:

$$
\begin{cases} 
(\vec{w}^*,b^*)=\arg\min_{(\vec{w},b)\in R^{d+1}} \ \lambda\|\vec{w}\|^2 +\frac{1}{m} \sum_{i=1}^m \xi_{i}   \\ \\

\forall_{i}, y_{i}<\vec{w},\vec{x_{i}} > \ \geq 1-\xi_{i}, \ \xi_{i}\geq_{0}
\end{cases}
$$

Nella funzione obiettivo, abbiamo due quantità non omogenee: $\|\vec{w}\|^2$ e $\sum_{i=1}^m \xi_{i}$. Per questo motivo, si introduce un iperparametro $\lambda$ che serve a pesare una delle due grandezze.

Il valore di $\xi_i$ può essere espresso come:

$$\xi_{i}=
\begin{cases}
0, \  se \ y_{i}  <\vec{w},x_{i}> \ \geq 1\\
 \\
1-y_{i}<\vec{w},x_{i}>, \ \text{ altrimenti}
\end{cases}

$$

Questo valore corrisponde al valore della loss surrogata introdotta precedentemente:

$$\xi_{i}=l_{hinge}(\vec{w},(\vec{x_{i},y_{i}}))=\max \{ 0,1-y_{i}<\vec{w},\vec{x_{i}>} \}$$

Otteniamo dunque questa formulazione per la Soft SVM:

$$
\vec{w}^*=\arg\min_{\vec{w}\in R^{d+1}} \ \lambda \|\vec{w}\|^2 +\frac{1}{m}\sum_{i=1}^m \ l_{hinge}(\vec{w,(\vec{x_{i}},y_{i})})
$$

che può essere riscritta come:

$$
\vec{w}^*=\arg\min_{\vec{w}\in R^{d+1}} \ L_{S}^{\text{hinge}}+ \lambda \|\vec{w}\|^2 
$$

Questa formulazione è un esempio di Regularized Linear Model (RLM). $L_S^{hinge}$ è una funzione convessa ed è la surrogata di $L_S^{0-1}$.

La Soft SVM è l'algoritmo che cercavamo per il learning dei semispazi. Tuttavia, abbiamo un errore di generalizzazione perché stiamo usando la funzione surrogata. La funzione hinge è Lipschitz-bounded, con Lipschitzness $\rho$:

$\rho=\| \nabla_{\vec{w}}l_{hinge}(\vec{w,(\vec{x},y)})\|= \| \vec{x_{i}}   \|$

$\rho =\max_{1\leq i\leq m} \|\vec{x_{i}}\|$


