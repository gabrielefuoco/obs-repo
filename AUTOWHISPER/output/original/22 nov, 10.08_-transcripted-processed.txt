Questa lezione riguarda il Reinforcement Learning.

Iniziamo con il concetto di *reward* (ricompensa). L'obiettivo dell'agente è massimizzare la ricompensa cumulativa. Questa ricompensa, indicata con G, dipende dalla politica π, dalla funzione di transizione P e dalla funzione di ricompensa R, nel caso di un singolo agente, o anche dagli altri agenti nel caso di giochi multi-agente, dove si introduce la competizione o la cooperazione.  L'obiettivo, ad esempio, potrebbe essere raggiungere un obiettivo specifico, come raccogliere del cibo o conquistare il centro di una scacchiera, evitando comportamenti che portano a un guadagno immediato ma a una perdita a lungo termine.

Formalizziamo il concetto di massimizzazione della ricompensa cumulativa.  Per formalizzare l'indipendenza temporale, introduciamo il concetto di *return* (guadagno). Il *return* al tempo *t* è la somma delle ricompense ottenute dall'agente dal tempo *t* in poi.

Nei *task* episodici, dove esiste uno stato finale, il *return* è ben definito.  In questi casi, l'interazione termina quando si raggiunge lo stato finale.  Tipicamente si considera il *return* per ogni episodio.

Nei *task* continui, senza uno stato finale, la somma delle ricompense potrebbe divergere.  Quindi, per i *task* continui, questa definizione di *return* non è adatta.  Inoltre, una ricompensa ottenuta ora ha più valore di una ricompensa futura.

Per formalizzare questo concetto di valore temporale, si introduce il *discounted return*.  Si introduce un fattore di sconto γ (gamma) compreso tra 0 e 1. Il *discounted return* è la somma pesata delle ricompense future, dove il peso diminuisce esponenzialmente con il tempo.  γ sconta le ricompense future, rappresentando il fatto che una ricompensa immediata è preferibile a una ricompensa futura dello stesso valore.  Con il *discounted return*, anche nei *task* continui, il *return* converge se le ricompense sono limitate.  Possiamo quindi calcolare il *return* sia per *task* episodici che continui.  Nei *task* episodici, γ può essere anche uguale a 1.

Parliamo ora del concetto di stato.  L'agente interagisce con l'ambiente.  L'ambiente fornisce all'agente delle osservazioni tramite dei sensori. Lo stato non è l'osservazione grezza, ma una sua elaborazione.  Lo stato dovrebbe contenere tutte le informazioni rilevanti per l'agente per prendere decisioni.  Lo stato deve essere una rappresentazione succinta delle informazioni necessarie.  Si assume che lo stato segua la proprietà di Markov.

La proprietà di Markov afferma che lo stato attuale riassume tutta la storia passata rilevante.  In altre parole, la probabilità di transizione al prossimo stato, dato lo stato attuale e l'azione intrapresa, è indipendente dagli stati precedenti.  Questo semplifica notevolmente il problema.  Ad esempio, la posizione e la velocità di un'automobile sono sufficienti a predire il suo stato futuro, senza bisogno di conoscere tutta la sua traiettoria passata.  Anche se in realtà ci potrebbero essere altri fattori, come l'usura dei componenti, in prima approssimazione possiamo ignorarli.

Formalmente, la proprietà di Markov implica che la probabilità di transizione da uno stato S ad uno stato S' tramite un'azione A è data da una funzione P(S'|S, A). Questa funzione rappresenta la probabilità di trovarsi nello stato S' dopo aver eseguito l'azione A nello stato S.


