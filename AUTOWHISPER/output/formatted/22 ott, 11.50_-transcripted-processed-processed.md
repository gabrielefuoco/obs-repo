## Adattamento di Architetture di Rete per la Classificazione di Immagini

In questa lezione, analizzeremo come adattare le architetture di rete per risolvere problemi di classificazione di immagini. Prendiamo come esempio il caso in cui dobbiamo identificare un animale presente in un'immagine. Questo tipo di applicazione può essere risolta tramite un'architettura di rete neurale. Abbiamo già visto esempi di architetture come LeNet, piuttosto semplice, e GoogLeNet (o Inception), più sofisticata e probabilmente più interessante per noi. 

La domanda è: come adatteremmo l'architettura per risolvere questo specifico problema? Qual è la differenza tra l'identificare un solo animale e identificarne diversi nella stessa immagine? 

In entrambi i casi, l'output desiderato è un vettore che associa valori a diverse categorie. Ad esempio, se avessimo solo due categorie, "cane" e "gatto", il vettore di output potrebbe essere [1, 0] per un cane e [0, 1] per un gatto. Ma cosa succede se abbiamo 10 possibili animali? Non possiamo avere 1024 risposte diverse per ogni immagine. Come possiamo quindi risolvere questo problema?

### Soluzione con Rete Fully Connected e Softmax

La soluzione consiste nell'utilizzare una rete neurale con uno strato fully connected e una funzione di attivazione Softmax. Vediamo i passaggi:

1. **Estrazione delle feature:** L'immagine di input viene passata al blocco di estrazione delle feature.
2. **Linearizzazione:** Le feature estratte vengono linearizzate.
3. **Strato Fully Connected:** Viene applicato uno strato fully connected alle feature linearizzate.
4. **Output:** Otteniamo un vettore di output con la stessa dimensione del numero di categorie (ad esempio, 5 categorie: cane, gatto, gorilla, orso, toro). Ogni elemento del vettore di output rappresenta la probabilità che l'immagine appartenga a quella specifica categoria. La somma di tutte le probabilità deve essere uguale a 1.

### Funzione Softmax e Stabilità Numerica

Per ottenere le probabilità, utilizziamo la funzione Softmax. La formula della Softmax è la seguente:

```
P(classe i | x) = exp(f(x)_i) / Σ(j=1 a N) exp(f(x)_j)
```

Dove:

* P(classe i | x) è la probabilità che l'input x appartenga alla classe i.
* f(x)_i è l'output dello strato fully connected per la classe i.
* N è il numero di classi.

Tuttavia, la formula della Softmax può portare a problemi di stabilità numerica a causa degli esponenziali. Per evitarli, si utilizza il logaritmo della Softmax (log-softmax). La formula della log-softmax è:

```
log(P(classe i | x)) = f(x)_i - log(Σ(j=1 a N) exp(f(x)_j))
```

Il secondo termine della formula è il logaritmo della somma degli esponenziali (log-sum-exp). Per calcolare il log-sum-exp in modo stabile, si può utilizzare il seguente trucco matematico:

```
log(Σ(i=1 a N) exp(y_i)) = m + log(Σ(i=1 a N) exp(y_i - m))
```

Dove m è il valore massimo di y. 


## Lezione Universitaria di Computer Vision: Classificazione di Immagini

### Softmax e Log-Softmax

La funzione **Softmax** è una funzione di attivazione utilizzata in reti neurali per problemi di classificazione. Essa trasforma un vettore di valori reali in una distribuzione di probabilità, dove ogni elemento rappresenta la probabilità che l'input appartenga a una specifica classe.

La formula per la Softmax è:

```
softmax(x)_i = exp(x_i) / sum(exp(x_j))
```

dove:

* `x` è il vettore di input
* `i` è l'indice della classe
* `j` è l'indice che itera su tutte le classi

**Problematiche di Stabilità Numerica:**

La funzione Softmax può soffrire di problemi di stabilità numerica quando i valori di input sono molto grandi. Questo perché l'esponenziale può crescere rapidamente, portando a overflow numerici.

**Log-Softmax:**

Per risolvere questo problema, si utilizza la **Log-Softmax**, che calcola il logaritmo della Softmax:

```
log_softmax(x)_i = x_i - log(sum(exp(x_j)))
```

La Log-Softmax è numericamente più stabile della Softmax, in quanto evita l'overflow numerico.

### Implementazione in PyTorch

In PyTorch, la funzione log-softmax è già implementata e viene utilizzata per calcolare la perdita durante l'addestramento.

### Considerazioni sul Batch Size

È importante ricordare che le reti neurali elaborano i dati in batch. Quindi, l'input della rete non sarà una singola immagine, ma un batch di immagini. Di conseguenza, la dimensione dell'input dovrà essere modificata per includere la dimensione del batch. Ad esempio, se il batch size è 64, la dimensione dell'input sarà [64, 1, 28, 28] per immagini in scala di grigio di dimensione 28x28.

### Conclusione

In questa lezione, abbiamo visto come adattare le architetture di rete per risolvere problemi di classificazione di immagini. Abbiamo analizzato l'utilizzo della funzione Softmax e le problematiche di stabilità numerica, introducendo la log-softmax come soluzione. Infine, abbiamo considerato l'importanza del batch size e come influisce sulla dimensione dell'input della rete.

## Analisi di un Modello di Classificazione

Consideriamo un modello di classificazione che utilizza un layer convoluzionale seguito da un layer fully connected. Il layer convoluzionale estrae feature dall'immagine, mentre il layer fully connected classifica le feature estratte.

**Esempio:**

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.fc = nn.Linear(256, 10)

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)  # Appiattimento
        x = self.fc(x)
        return x

model = Model()
x = torch.randn(1, 1, 28, 28)  # Input di dimensione [batch_size, canali, altezza, larghezza]
y = model(x)
```

In questa linea di codice, stiamo appiattendo tutto tranne la prima dimensione. Il risultato finale sarà una matrice con la dimensione del batch come prima componente e 256 come seconda dimensione, ottenuta appiattendo il blocco.

**Spiegazione:**

* **`x`**: rappresenta le feature dell'immagine.
* **`x.shape`**: restituisce la forma di `x`.
* **`x.view`**: modifica la forma di `x`.
* **`model.fc(x)`**: applica il layer fully connected a `x`.

**Analisi del Bacino:**

Consideriamo un blocco di un edificio nell'immagine. Il layer convoluzionale estrae un vettore di dimensione 1x16x4x4 da questo blocco. Moltiplicando questi quattro elementi, otteniamo un blocco di 256 elementi. Appiattendo questo blocco, otteniamo una matrice 1x256. Ricordiamo che "1" rappresenta la dimensione del batch.

**Processo di Classificazione:**

1. **Input:** L'immagine di dimensione 1x1x28x28 viene passata al modello.
2. **Estrazione Feature:** Il layer convoluzionale estrae feature, ottenendo un vettore 1x16x4x4.
3. **Appiattimento:** Il blocco di feature viene appiattito, ottenendo una matrice di dimensione "dimensione del batch" x 256.
4. **Classificazione:** Il layer fully connected classifica le feature appiattite, fornendo la risposta sulla quale calcolare la logica.

**Softmax:**

Potremmo anche calcolare direttamente la softmax sul risultato del layer fully connected, ottenendo le probabilità per ogni classe.

**Conclusione:**

Questo esempio illustra come un modello di classificazione utilizza i layer convoluzionali e fully connected per estrarre feature e classificare le immagini. La funzione Softmax o Log-Softmax viene utilizzata per ottenere una distribuzione di probabilità sulle classi. La dimensione del batch influenza la dimensione dell'input e la forma delle feature estratte.


## Computer Vision: Classificazione Multipla

Questo documento descrive il processo di classificazione in Computer Vision, con particolare attenzione alla differenza tra classificazione mutuamente esclusiva e classificazione multipla.

### Classificazione Mutuamente Esclusiva

Il processo di classificazione in Computer Vision, come descritto nel testo, prevede l'utilizzo di una rete neurale per classificare un'immagine in una delle possibili classi. Questo processo può essere suddiviso in due fasi:

1. **Estrazione delle feature:** La rete neurale elabora l'immagine di input ("x") attraverso una serie di layer convoluzionali e pooling, estraendo le feature significative.
2. **Classificazione:** Le feature estratte vengono appiattite e passate a layer fully connected, che producono un vettore di probabilità per ogni classe. La funzione di attivazione softmax viene applicata a questo vettore, normalizzando le probabilità in modo che la loro somma sia pari a 1.

In questo scenario, la rete prevede una sola classe per ogni immagine, ovvero la classe con la probabilità più alta. Questo tipo di classificazione è definita **mutuamente esclusiva**, poiché un'immagine può appartenere solo a una classe alla volta.

### Classificazione Multipla

Nel caso della classificazione multipla, un'immagine può appartenere a più classi contemporaneamente. Ad esempio, un'immagine potrebbe contenere sia un cane che un gatto. Per gestire questo tipo di classificazione, è necessario modificare la rete neurale in due modi:

1. **Funzione di attivazione:** La funzione di attivazione softmax viene sostituita con la funzione sigmoid. La sigmoid opera elemento per elemento, restituendo un vettore con la stessa dimensione dell'input, dove ogni elemento rappresenta la probabilità associata a una specifica classe.
2. **Funzione di loss:** La cross-entropia su tutte le classi viene sostituita con una somma di cross-entropie binarie. Le etichette reali sono rappresentate da un vettore binario con "1" in corrispondenza degli oggetti presenti nell'immagine.

### Architettura della Rete

È importante notare che l'architettura della rete neurale rimane invariata sia per la classificazione mutuamente esclusiva che per la classificazione multipla. La differenza risiede nella funzione di attivazione e nella funzione di loss. La prima parte della rete, che estrae le feature, rimane invariata perché il suo obiettivo è estrarre informazioni significative dall'immagine, indipendentemente dal tipo di classificazione. La seconda parte della rete, che si occupa della classificazione, utilizza le informazioni estratte dalla prima parte per generare le predizioni.

### Conclusione

La classificazione multipla è un'estensione della classificazione mutuamente esclusiva che consente di gestire immagini con più oggetti. La modifica della funzione di attivazione e della funzione di loss permette di ottenere predizioni accurate per questo tipo di classificazione. L'architettura della rete neurale rimane invariata, dimostrando la flessibilità e l'adattabilità di questo tipo di modello.


## Architettura di una Rete Neurale per la Computer Vision

L'architettura di una rete neurale per la computer vision può essere suddivisa in due parti principali:

1. **Estrazione delle Caratteristiche:** Questa parte "vede" l'immagine e ne estrae le caratteristiche salienti.
2. **Interpretazione delle Caratteristiche:** Questa parte "interpreta" le caratteristiche estratte per classificare l'immagine.

Questa architettura è in grado di risolvere sia problemi di classificazione esclusiva che multipla, adattando semplicemente la funzione di attivazione e la funzione di loss.

### Adattamento dell'Output per Classificazione Multipla

Nel caso della classificazione multipla, l'output della rete neurale deve essere adattato per gestire più di una classe. Ad esempio, se l'immagine può appartenere a una delle 1024 classi possibili, l'output dovrebbe essere un vettore di 1024 elementi, dove ogni elemento rappresenta la probabilità che l'immagine appartenga a una specifica classe.

**Esempio:**

Se l'ultimo layer della rete è 84x10, per adattarlo alla classificazione multipla con 1024 classi, dovremmo modificarlo in 84x2^10. Questo significa che il layer dovrebbe avere 2^10 (1024) neuroni in output, uno per ogni classe.

**Soluzione alternativa:**

Una soluzione alternativa, più efficiente, è quella di mantenere l'ultimo layer con 84x10 neuroni e utilizzare una funzione di attivazione che permetta di ottenere un output multi-classe. Ad esempio, si potrebbe utilizzare una funzione di attivazione sigmoidale per ogni neurone in output, ottenendo così una probabilità di appartenenza a ciascuna delle 10 classi.

**Conclusione:**

L'architettura di una rete neurale per la computer vision può essere adattata per risolvere diversi tipi di problemi, come la classificazione esclusiva e multipla. L'adattamento principale riguarda l'output della rete, che può essere modificato per gestire un numero maggiore di classi. 


